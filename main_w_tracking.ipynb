{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust workflow\n",
    "*change load classes to the utils\n",
    "* add in the youtube download to utils XX\n",
    "* add changes for above two changes the main.py XX\n",
    "* change main.py to train.py so you can full automate to CLI easiest\n",
    "* add in the classes to the labels visualizations XX\n",
    "* add uses to it \n",
    "* add to hugging face/Snap AR\n",
    "* attempt to use cv2 to see when basketballs intersect with rims XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images outside of youtube nba footage come from: [text](https://stock.adobe.com/search?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bcontent_type%3Aillustration%5D=1&filters%5Bcontent_type%3Azip_vector%5D=1&filters%5Bcontent_type%3Avideo%5D=1&filters%5Bcontent_type%3Atemplate%5D=1&filters%5Bcontent_type%3A3d%5D=1&filters%5Bcontent_type%3Aaudio%5D=0&filters%5Binclude_stock_enterprise%5D=0&filters%5Bis_editorial%5D=0&filters%5Bfree_collection%5D=0&filters%5Bcontent_type%3Aimage%5D=1&k=basketball+hoop&order=relevance&safe_search=1&limit=100&search_page=21&search_type=pagination&get_facets=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\ByteTrack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ByteTrack' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.0)\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.1)\n",
      "Collecting opencv_python (from -r requirements.txt (line 4))\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting loguru (from -r requirements.txt (line 5))\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting scikit-image (from -r requirements.txt (line 6))\n",
      "  Using cached scikit_image-0.22.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting tqdm (from -r requirements.txt (line 7))\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in c:\\users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages (from -r requirements.txt (line 8)) (0.16.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages (from -r requirements.txt (line 9)) (10.0.1)\n",
      "Collecting thop (from -r requirements.txt (line 10))\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Collecting ninja (from -r requirements.txt (line 11))\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting tabulate (from -r requirements.txt (line 12))\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting tensorboard (from -r requirements.txt (line 13))\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting lap (from -r requirements.txt (line 15))\n",
      "  Using cached lap-0.4.0.tar.gz (1.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting motmetrics (from -r requirements.txt (line 16))\n",
      "  Using cached motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
      "Collecting filterpy (from -r requirements.txt (line 17))\n",
      "  Using cached filterpy-1.4.5.zip (177 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting h5py (from -r requirements.txt (line 18))\n",
      "  Using cached h5py-3.10.0-cp39-cp39-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting onnx==1.8.1 (from -r requirements.txt (line 21))\n",
      "  Using cached onnx-1.8.1.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [18 lines of output]\n",
      "      fatal: not a git repository (or any of the parent directories): .git\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "        File \"C:\\Users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "        File \"C:\\Users\\ghadf\\AppData\\Local\\Temp\\pip-build-env-vhumkdpi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "        File \"C:\\Users\\ghadf\\AppData\\Local\\Temp\\pip-build-env-vhumkdpi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\ghadf\\AppData\\Local\\Temp\\pip-build-env-vhumkdpi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 480, in run_setup\n",
      "          super(_BuildMetaLegacyBackend, self).run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\ghadf\\AppData\\Local\\Temp\\pip-build-env-vhumkdpi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 75, in <module>\n",
      "      AssertionError: Could not find \"cmake\" executable!\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running develop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3'\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\setuptools\\command\\easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "error: can't create or remove files in install directory\n",
      "\n",
      "The following error occurred while trying to add or remove files in the\n",
      "installation directory:\n",
      "\n",
      "    [Errno 13] Permission denied: 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\\\Lib\\\\site-packages\\\\test-easy-install-10140.write-test'\n",
      "\n",
      "The installation directory you specified (via --install-dir, --prefix, or\n",
      "the distutils default setting) was:\n",
      "\n",
      "    C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\\n",
      "\n",
      "Perhaps your account does not have write access to this directory?  If the\n",
      "installation directory is a system-owned directory, you may need to sign in\n",
      "as the administrator or \"root\" account.  If you do not have administrative\n",
      "access to this machine, you may wish to choose a different installation\n",
      "directory, preferably one that is listed in your PYTHONPATH environment\n",
      "variable.\n",
      "\n",
      "For information on other options, you may wish to consult the\n",
      "documentation at:\n",
      "\n",
      "  https://setuptools.pypa.io/en/latest/deprecated/easy_install.html\n",
      "\n",
      "Please make the appropriate changes for your system and try again.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython\n",
      "  Downloading Cython-3.0.8-cp39-cp39-win_amd64.whl.metadata (3.2 kB)\n",
      "Downloading Cython-3.0.8-cp39-cp39-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.8 MB 2.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.2/2.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.2/2.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.2/2.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.5/2.8 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.8 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.3/2.8 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.3/2.8 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.5/2.8 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: cython\n",
      "Successfully installed cython-3.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\"\n",
      "Hint: = is not a valid operator. Did you mean == ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython_bbox\n",
      "  Downloading cython_bbox-0.1.5.tar.gz (4.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Cython in c:\\users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages (from cython_bbox) (3.0.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\ghadf\\anaconda3\\envs\\ml\\lib\\site-packages (from cython_bbox) (1.26.0)\n",
      "Building wheels for collected packages: cython_bbox\n",
      "  Building wheel for cython_bbox (setup.py): started\n",
      "  Building wheel for cython_bbox (setup.py): finished with status 'done'\n",
      "  Created wheel for cython_bbox: filename=cython_bbox-0.1.5-cp39-cp39-win_amd64.whl size=30117 sha256=0849b089384a5bc67c99582115d0d33724340672e821b5d5b234be572b9e5608\n",
      "  Stored in directory: c:\\users\\ghadf\\appdata\\local\\pip\\cache\\wheels\\61\\9b\\a5\\1c5e3f1441f19936a756e9b4d697413b173f1710631cbcd8c7\n",
      "Successfully built cython_bbox\n",
      "Installing collected packages: cython_bbox\n",
      "Successfully installed cython_bbox-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ifzhang/ByteTrack.git\n",
    "%cd ByteTrack\n",
    "!pip3 install -r requirements.txt\n",
    "!python3 setup.py develop\n",
    "!pip3 install cython\n",
    "!pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "!pip3 install cython_bbox\n",
    "#https://github.com/gatagat/lap shows lap requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get and print the current working directory before changing it\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory before change:\", current_directory)\n",
    "\n",
    "# Define the new directory\n",
    "new_directory = r\"C:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\"\n",
    "parent_directory = os.path.dirname(new_directory)\n",
    "\n",
    "# Change the current working directory to the new directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Get and print the current working directory after changing it\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory after change:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu118\n",
      "CUDA available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "#create going_modular repository\n",
    "create_directory(\"going_modular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required files from torchvision\n",
    "import requests\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "]\n",
    "download_files(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "#!pip install roboflow\n",
    "\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"ai-79z1a\").project(\"basketball_child\")\n",
    "#dataset = project.version(6).download(\"coco-segmentation\")\n",
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "import requests\n",
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "def download_videos_from_youtube(video_urls, output_path):\n",
    "    \"\"\"\n",
    "    Downloads videos from YouTube.\n",
    "\n",
    "    Args:\n",
    "    video_urls (list): List of YouTube video URLs.\n",
    "    output_path (str): Directory where videos will be saved.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of successful and failed downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'best',\n",
    "        'outtmpl': output_path + '/%(title)s.%(ext)s',\n",
    "        'quiet': True\n",
    "    }\n",
    "\n",
    "    failed_downloads = []\n",
    "    successful_downloads = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        for url in video_urls:\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "                print(f\"Successfully downloaded {url}\")\n",
    "                successful_downloads.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "                failed_downloads.append(url)\n",
    "\n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_project(api_key, workspace, project_name, version):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(\"coco-segmentation\")\n",
    "    return dataset\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def construct_dataset_paths(project_name, version):\n",
    "    base_path = f\"{project_name}-{version}\"\n",
    "    train_annotation_path = f\"{base_path}/train/_annotations.coco.json\"\n",
    "    valid_annotation_path = f\"{base_path}/valid/_annotations.coco.json\"\n",
    "    test_annotation_path = f\"{base_path}/test/_annotations.coco.json\"\n",
    "\n",
    "    train_root_dir = f\"{base_path}/train\"\n",
    "    valid_root_dir = f\"{base_path}/valid\"\n",
    "    test_root_dir = f\"{base_path}/test\"\n",
    "\n",
    "    return train_annotation_path, valid_annotation_path, test_annotation_path, train_root_dir, valid_root_dir, test_root_dir\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/coco_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/coco_dataset.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_path, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter out images without annotations\n",
    "        annotated_images = []\n",
    "        for img in self.annotations['images']:\n",
    "            image_id = img['id']\n",
    "            anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "            if len(anns) > 0:\n",
    "                annotated_images.append(img)\n",
    "\n",
    "        self.image_ids = [img['id'] for img in annotated_images]\n",
    "\n",
    "        # Update the self.annotations['images'] to include only annotated images\n",
    "        self.annotations['images'] = annotated_images\n",
    "        \n",
    "        #print(\"Number of images:\", len(self.annotations['images']))\n",
    "        #print(\"Sample image entry:\", self.annotations['images'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.annotations['images'][idx]\n",
    "        image_id = img_info['id']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "        #print(\"Image size (PIL):\", img.size)\n",
    "        #print(\"Image shape (tensor):\", img_tensor.shape)\n",
    "\n",
    "        anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "        #print(\"Number of annotations for this image:\", len(anns))\n",
    "\n",
    "        boxes = [ann['bbox'] for ann in anns]  # bbox format: [x_min, y_min, width, height]\n",
    "        # Convert from XYWH to XYXY format\n",
    "        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = [ann['category_id'] for ann in anns]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        #print(\"Boxes shape:\", boxes.shape)\n",
    "        #print(\"Labels:\", labels)\n",
    "        # Debug print\n",
    "        #print(f\"Boxes shape for image {idx}: {boxes.shape}\")\n",
    "\n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            if 'segmentation' in ann and isinstance(ann['segmentation'], list):\n",
    "                for seg in ann['segmentation']:\n",
    "                    mask_img = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "                    ImageDraw.Draw(mask_img).polygon(seg, outline=1, fill=1)\n",
    "                    mask = np.array(mask_img)\n",
    "                    masks.append(mask)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "        #print(\"Masks shape:\", masks.shape)\n",
    "\n",
    "        areas = [ann['area'] for ann in anns]\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = [ann['iscrowd'] for ann in anns]\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Convert masks to Mask format\n",
    "        masks = tv_tensors.Mask(masks)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id  # Changed to integer\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        #print(\"Target:\", target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/visualization_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/visualization_utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F  # Add this import\n",
    "\n",
    "# New function to visualize transformations\n",
    "def visualize_transformation(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    transformed_img, transformed_target = dataset.transforms(img, target)\n",
    "    original_img = F.to_pil_image(img)\n",
    "    transformed_img = F.to_pil_image(transformed_img)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    for box in target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Original Image - ID: {idx}\")\n",
    "\n",
    "    # Transformed Image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(transformed_img)\n",
    "    for box in transformed_target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='b', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Transformed Image - ID: {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bbox(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    original_img = F.to_pil_image(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(original_img)\n",
    "\n",
    "    for box in target[\"boxes\"]:  # Access the boxes directly\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        # Debug print\n",
    "        print(f\"Visualizing BBox - xmin: {x_min}, ymin: {y_min}, xmax: {x_max}, ymax: {y_max}\")\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.title(f\"Image with Bounding Boxes - ID: {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_utils.py\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extracting classes and their IDs\n",
    "    classes = {category['id']: category['name'] for category in data['categories']}\n",
    "    return classes\n",
    "\n",
    "# Usage example:\n",
    "#classes = load_classes_from_json('basketball_child-6/test/_annotations.coco.json')\n",
    "#print(classes)\n",
    "\n",
    "# model_utils.py\n",
    "def get_model_instance_segmentation(num_classes, hidden_layer=256):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/transforms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/transforms.py\n",
    "import torch  # Add this import statement\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, ConvertImageDtype\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    #if train:\n",
    "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "# train.py\n",
    "import torch\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "\n",
    "def train_model(model, data_loader, data_loader_valid, device, num_epochs,\n",
    "                lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_valid, device=device)\n",
    "\n",
    "    #torch.save(model.state_dict(), 'results/models/model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/process_video_check_track.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/process_video_check_track.py\n",
    "\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from ByteTrack.yolox.tracker.byte_tracker import BYTETracker\n",
    "\n",
    "# Define ByteTrack arguments\n",
    "class ByteTrackArgument:\n",
    "    track_thresh = 0.5\n",
    "    track_buffer = 30\n",
    "    match_thresh = 0.8\n",
    "    aspect_ratio_thresh = 1.6\n",
    "    min_box_area = 10\n",
    "    mot20 = False\n",
    "\n",
    "def intersects(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
    "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
    "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
    "\n",
    "def process_video_check_track(video_path, model, device, classes, classes_to_track, threshold=0.5, output_video_path='output_video.mp4'):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    tracker = BYTETracker(ByteTrackArgument)\n",
    "    score_counter = 0  # Score counter\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc('F','M','P','4')\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading video file\")\n",
    "        return\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_tensor = T.Compose([\n",
    "            T.ToImage(),\n",
    "            T.ToDtype(torch.float32, scale=True)\n",
    "        ])(frame)\n",
    "\n",
    "        frame_tensor = frame_tensor.unsqueeze_(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)[0]\n",
    "\n",
    "        pred_scores = prediction['scores']\n",
    "        pred_boxes = prediction['boxes']\n",
    "        pred_labels = prediction['labels']\n",
    "        pred_masks = prediction['masks']\n",
    "\n",
    "        keep = pred_scores > threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        pred_masks = pred_masks[keep]\n",
    "\n",
    "        if not keep.any():\n",
    "            continue\n",
    "\n",
    "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
    "\n",
    "        for class_pair in classes_to_track:\n",
    "            class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
    "            class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
    "\n",
    "            for box1 in class1_boxes:\n",
    "                for box2 in class2_boxes:\n",
    "                    if intersects(box1, box2):\n",
    "                        score_counter += 1\n",
    "                        print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score_counter)\n",
    "\n",
    "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
    "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
    "\n",
    "        # Draw bounding boxes and segmentation masks\n",
    "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
    "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
    "        \n",
    "        # Convert output image for displaying and writing\n",
    "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        output_image = np.clip(output_image, 0, 255)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2_imshow(output_image)\n",
    "\n",
    "        # Write the frame to video\n",
    "        out.write(output_image)\n",
    "\n",
    "        dets = []\n",
    "        for box, det_score in zip(pred_boxes, pred_scores):\n",
    "            if det_score > threshold:\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                current_det = [x1, y1, x2, y2, det_score.item()]\n",
    "                dets.append(current_det)\n",
    "\n",
    "        dets_np = np.array(dets)\n",
    "        img_info_list = [frame.shape[0], frame.shape[1]]\n",
    "        online_targets = tracker.update(dets_np, img_info_list, [frame.shape[1], frame.shape[0]])\n",
    "\n",
    "        for t in online_targets:\n",
    "            tlwh = t.tlwh\n",
    "            tid = t.track_id\n",
    "            cv2.rectangle(frame, (int(tlwh[0]), int(tlwh[1])), (int(tlwh[2]), int(tlwh[3])), (0,255,0), 2)\n",
    "            cv2.putText(frame, str(tid), (int(tlwh[0]), int(tlwh[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# process_video_check_track(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os\n",
    "from going_modular.utils import (get_device, create_directory, get_project,\n",
    "                                 download_files, construct_dataset_paths,\n",
    "                                 download_videos_from_youtube)\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import (get_model_instance_segmentation,\n",
    "                                       load_classes_from_json)\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.transforms import get_transform\n",
    "from going_modular.process_video_check_track import process_video_check_track\n",
    "import utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def main(args):\n",
    "    data_path = Path(args.data_path)\n",
    "    model_path = Path(args.model_path)\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dataset = get_project(args.api_key, args.workspace, args.project_name, args.version)\n",
    "\n",
    "    classes = load_classes_from_json(f'{args.project_folder_name}-{args.version}/test/_annotations.coco.json')\n",
    "    print(\"Classes loaded:\", classes)\n",
    "\n",
    "    num_classes = len(classes) + 1\n",
    "    device = get_device()\n",
    "\n",
    "    train_annotation_path, valid_annotation_path, test_annotation_path, train_image_dir, valid_image_dir, test_image_dir = construct_dataset_paths(args.project_folder_name, args.version)\n",
    "\n",
    "    train_dataset = CustomCocoDataset(train_annotation_path, train_image_dir, transforms=get_transform(train=True))\n",
    "    valid_dataset = CustomCocoDataset(valid_annotation_path, valid_image_dir, transforms=get_transform(train=False))\n",
    "    test_dataset = CustomCocoDataset(test_annotation_path, test_image_dir)\n",
    "\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=args.hidden_layer)\n",
    "    model.to(device)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        train_model(model, train_data_loader, valid_data_loader, device, args.num_epochs, lr=args.lr)\n",
    "        model_file_path = model_path / 'model_weights.pth'\n",
    "        try:\n",
    "            torch.save(model.state_dict(), str(model_file_path))\n",
    "            print(\"Model saved at:\", model_file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "\n",
    "    elif args.mode == 'process_video':\n",
    "        model_file_path = model_path / 'model_weights.pth'\n",
    "        if not model_file_path.exists():\n",
    "            print(\"Model weights file not found. Please train the model first.\")\n",
    "            return\n",
    "\n",
    "        # Download the video\n",
    "        successful_downloads, failed_downloads = download_videos_from_youtube([args.video_url], str(data_path))\n",
    "        print(f\"Successfully downloaded {len(successful_downloads)} videos.\")\n",
    "        print(f\"Failed to download {len(failed_downloads)} videos.\")\n",
    "\n",
    "        if successful_downloads:\n",
    "            # Use the video filename from the command-line argument\n",
    "            video_filename = args.video_name\n",
    "            if not video_filename.endswith('.mp4'):\n",
    "                video_filename += '.mp4'\n",
    "            video_path = data_path / video_filename\n",
    "            print(f\"Video path: {video_path}\")\n",
    "\n",
    "            # Load model state for video processing\n",
    "            model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
    "            model.to(device)\n",
    "\n",
    "            # Process video\n",
    "            process_video_check_track(video_path, model, device, classes, [('Basketball', 'Hoop')], threshold=args.threshold)\n",
    "        else:\n",
    "            print(\"No videos were downloaded.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a model for object detection or process video\")\n",
    "    parser.add_argument('--api_key', type=str, default=\"your_roboflow_api_key\", help='API key for Roboflow')\n",
    "    parser.add_argument('--workspace', type=str, default=\"your_roboflow_workspace\", help='Workspace name in Roboflow')\n",
    "    parser.add_argument('--project_name', type=str, default=\"your_roboflow_project\", help='Project name in Roboflow')\n",
    "    parser.add_argument('--project_folder_name', type=str, default=\"your_roboflow_project_folder_name\", help='Project folder name in Roboflow')\n",
    "    parser.add_argument('--version', type=int, default=1, help='Version of the dataset in Roboflow')\n",
    "    parser.add_argument('--hidden_layer', type=int, default=256, help='Hidden layer size for the MaskRCNN predictor')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs to train the model')\n",
    "    parser.add_argument('--video_url', type=str, default=\"https://www.youtube.com/watch?v=example_video_id\", help='URL of the video to process')\n",
    "    parser.add_argument('--video_name', type=str, default=\"your_youtube_video_name\", help='Name of the video file (with extension) to process')\n",
    "    parser.add_argument('--threshold', type=float, default=0.6, help='Detection threshold for process_video')\n",
    "    parser.add_argument('--display_video', type=bool, default=True, help='Whether to display the video during processing')\n",
    "    parser.add_argument('--data_path', type=str, default='results/data', help='Path to save downloaded data')\n",
    "    parser.add_argument('--model_path', type=str, default='results/models', help='Path to save model weights')\n",
    "    parser.add_argument('--mode', type=str, choices=['train', 'process_video'], default='train', help='Mode of operation: train or process_video')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\n",
      "Python Path: ['c:\\\\Users\\\\ghadf\\\\vscode_projects\\\\venv_projects\\\\Pytorch\\\\bball_instanceseg', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0', 'c:\\\\Users\\\\ghadf\\\\vscode_projects\\\\venv_projects\\\\Pytorch\\\\bball_instanceseg\\\\.venv', '', 'c:\\\\Users\\\\ghadf\\\\vscode_projects\\\\venv_projects\\\\Pytorch\\\\bball_instanceseg\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\ghadf\\\\vscode_projects\\\\venv_projects\\\\Pytorch\\\\bball_instanceseg\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ghadf\\\\vscode_projects\\\\venv_projects\\\\Pytorch\\\\bball_instanceseg\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ghadf\\\\vscode_projects\\\\venv_projects\\\\Pytorch\\\\bball_instanceseg\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n",
      "Failed to import ByteTrack: No module named 'yolox'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Print Python path\n",
    "print(\"Python Path:\", sys.path)\n",
    "\n",
    "# Attempt to import ByteTrack\n",
    "try:\n",
    "    from ByteTrack.yolox.tracker.byte_tracker import BYTETracker\n",
    "    print(\"ByteTrack imported successfully.\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"Failed to import ByteTrack:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\train.py\", line 11, in <module>\n",
      "    from going_modular.process_video_check_track import process_video_check_track\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\going_modular\\process_video_check_track.py\", line 7, in <module>\n",
      "    from ByteTrack.yolox.tracker.byte_tracker import BYTETracker\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\ByteTrack\\yolox\\tracker\\byte_tracker.py\", line 10, in <module>\n",
      "    from yolox.tracker import matching\n",
      "ModuleNotFoundError: No module named 'yolox'\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
    "                --workspace basketball-formations \\\n",
    "                --project_name basketball-and-hoop-7xk0h \\\n",
    "                --project_folder_name basketball-and-hoop \\\n",
    "                --version 10 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 1 \\\n",
    "                --threshold 0.6 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" \\\n",
    "                --video_name \"The Best NBA 3 Point Contest Performances\" \\\n",
    "                --mode process_video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
