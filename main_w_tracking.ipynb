{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust workflow\n",
    "*change load classes to the utils\n",
    "* add in the youtube download to utils XX\n",
    "* add changes for above two changes the main.py XX\n",
    "* change main.py to train.py so you can full automate to CLI easiest\n",
    "* add in the classes to the labels visualizations XX\n",
    "* add uses to it \n",
    "* add to hugging face/Snap AR\n",
    "* attempt to use cv2 to see when basketballs intersect with rims XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images outside of youtube nba footage come from: [text](https://stock.adobe.com/search?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bcontent_type%3Aillustration%5D=1&filters%5Bcontent_type%3Azip_vector%5D=1&filters%5Bcontent_type%3Avideo%5D=1&filters%5Bcontent_type%3Atemplate%5D=1&filters%5Bcontent_type%3A3d%5D=1&filters%5Bcontent_type%3Aaudio%5D=0&filters%5Binclude_stock_enterprise%5D=0&filters%5Bis_editorial%5D=0&filters%5Bfree_collection%5D=0&filters%5Bcontent_type%3Aimage%5D=1&k=basketball+hoop&order=relevance&safe_search=1&limit=100&search_page=21&search_type=pagination&get_facets=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'deep_sort' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nwojke/deep_sort.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory before change: C:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\n",
      "Current working directory after change: C:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get and print the current working directory before changing it\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory before change:\", current_directory)\n",
    "\n",
    "# Define the new directory\n",
    "new_directory = r\"C:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\"\n",
    "parent_directory = os.path.dirname(new_directory)\n",
    "\n",
    "# Change the current working directory to the new directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Get and print the current working directory after changing it\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory after change:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu118\n",
      "CUDA available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "#create going_modular repository\n",
    "create_directory(\"going_modular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required files from torchvision\n",
    "import requests\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "]\n",
    "download_files(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "#!pip install roboflow\n",
    "\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"ai-79z1a\").project(\"basketball_child\")\n",
    "#dataset = project.version(6).download(\"coco-segmentation\")\n",
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "import requests\n",
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "def download_videos_from_youtube(video_urls, output_path):\n",
    "    \"\"\"\n",
    "    Downloads videos from YouTube.\n",
    "\n",
    "    Args:\n",
    "    video_urls (list): List of YouTube video URLs.\n",
    "    output_path (str): Directory where videos will be saved.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of successful and failed downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'best',\n",
    "        'outtmpl': output_path + '/%(title)s.%(ext)s',\n",
    "        'quiet': True\n",
    "    }\n",
    "\n",
    "    failed_downloads = []\n",
    "    successful_downloads = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        for url in video_urls:\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "                print(f\"Successfully downloaded {url}\")\n",
    "                successful_downloads.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "                failed_downloads.append(url)\n",
    "\n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_project(api_key, workspace, project_name, version):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(\"coco-segmentation\")\n",
    "    return dataset\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def construct_dataset_paths(project_name, version):\n",
    "    base_path = f\"{project_name}-{version}\"\n",
    "    train_annotation_path = f\"{base_path}/train/_annotations.coco.json\"\n",
    "    valid_annotation_path = f\"{base_path}/valid/_annotations.coco.json\"\n",
    "    test_annotation_path = f\"{base_path}/test/_annotations.coco.json\"\n",
    "\n",
    "    train_root_dir = f\"{base_path}/train\"\n",
    "    valid_root_dir = f\"{base_path}/valid\"\n",
    "    test_root_dir = f\"{base_path}/test\"\n",
    "\n",
    "    return train_annotation_path, valid_annotation_path, test_annotation_path, train_root_dir, valid_root_dir, test_root_dir\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/coco_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/coco_dataset.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_path, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter out images without annotations\n",
    "        annotated_images = []\n",
    "        for img in self.annotations['images']:\n",
    "            image_id = img['id']\n",
    "            anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "            if len(anns) > 0:\n",
    "                annotated_images.append(img)\n",
    "\n",
    "        self.image_ids = [img['id'] for img in annotated_images]\n",
    "\n",
    "        # Update the self.annotations['images'] to include only annotated images\n",
    "        self.annotations['images'] = annotated_images\n",
    "        \n",
    "        #print(\"Number of images:\", len(self.annotations['images']))\n",
    "        #print(\"Sample image entry:\", self.annotations['images'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.annotations['images'][idx]\n",
    "        image_id = img_info['id']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "        #print(\"Image size (PIL):\", img.size)\n",
    "        #print(\"Image shape (tensor):\", img_tensor.shape)\n",
    "\n",
    "        anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "        #print(\"Number of annotations for this image:\", len(anns))\n",
    "\n",
    "        boxes = [ann['bbox'] for ann in anns]  # bbox format: [x_min, y_min, width, height]\n",
    "        # Convert from XYWH to XYXY format\n",
    "        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = [ann['category_id'] for ann in anns]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        #print(\"Boxes shape:\", boxes.shape)\n",
    "        #print(\"Labels:\", labels)\n",
    "        # Debug print\n",
    "        #print(f\"Boxes shape for image {idx}: {boxes.shape}\")\n",
    "\n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            if 'segmentation' in ann and isinstance(ann['segmentation'], list):\n",
    "                for seg in ann['segmentation']:\n",
    "                    mask_img = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "                    ImageDraw.Draw(mask_img).polygon(seg, outline=1, fill=1)\n",
    "                    mask = np.array(mask_img)\n",
    "                    masks.append(mask)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "        #print(\"Masks shape:\", masks.shape)\n",
    "\n",
    "        areas = [ann['area'] for ann in anns]\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = [ann['iscrowd'] for ann in anns]\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Convert masks to Mask format\n",
    "        masks = tv_tensors.Mask(masks)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id  # Changed to integer\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        #print(\"Target:\", target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/visualization_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/visualization_utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F  # Add this import\n",
    "\n",
    "# New function to visualize transformations\n",
    "def visualize_transformation(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    transformed_img, transformed_target = dataset.transforms(img, target)\n",
    "    original_img = F.to_pil_image(img)\n",
    "    transformed_img = F.to_pil_image(transformed_img)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    for box in target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Original Image - ID: {idx}\")\n",
    "\n",
    "    # Transformed Image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(transformed_img)\n",
    "    for box in transformed_target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='b', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Transformed Image - ID: {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bbox(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    original_img = F.to_pil_image(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(original_img)\n",
    "\n",
    "    for box in target[\"boxes\"]:  # Access the boxes directly\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        # Debug print\n",
    "        print(f\"Visualizing BBox - xmin: {x_min}, ymin: {y_min}, xmax: {x_max}, ymax: {y_max}\")\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.title(f\"Image with Bounding Boxes - ID: {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_utils.py\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extracting classes and their IDs\n",
    "    classes = {category['id']: category['name'] for category in data['categories']}\n",
    "    return classes\n",
    "\n",
    "# Usage example:\n",
    "#classes = load_classes_from_json('basketball_child-6/test/_annotations.coco.json')\n",
    "#print(classes)\n",
    "\n",
    "# model_utils.py\n",
    "def get_model_instance_segmentation(num_classes, hidden_layer=256):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/transforms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/transforms.py\n",
    "import torch  # Add this import statement\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, ConvertImageDtype\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    #if train:\n",
    "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "# train.py\n",
    "import torch\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "\n",
    "def train_model(model, data_loader, data_loader_valid, device, num_epochs,\n",
    "                lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_valid, device=device)\n",
    "\n",
    "    #torch.save(model.state_dict(), 'results/models/model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/process_video_check_track.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/process_video_check.py\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "def intersects(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
    "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
    "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
    "\n",
    "def process_video_check(video_path, model, device, classes, classes_to_track, threshold=0.5):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    score = 0  # Initialize the score counter\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_tensor = T.ToTensor()(frame).unsqueeze_(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)[0]\n",
    "\n",
    "        pred_scores = prediction['scores']\n",
    "        pred_boxes = prediction['boxes']\n",
    "        pred_labels = prediction['labels']\n",
    "        pred_masks = prediction['masks']\n",
    "\n",
    "        keep = pred_scores > threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        pred_masks = pred_masks[keep]\n",
    "\n",
    "        print(f\"Original Frame Size: {frame.shape}\")\n",
    "        print(f\"Pred Boxes before drawing: {pred_boxes}\")\n",
    "\n",
    "        if not keep.any():\n",
    "            continue  # Skip this frame if no detections are kept\n",
    "\n",
    "        # Convert numeric labels to class names\n",
    "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
    "\n",
    "        # Iterate through each pair of classes to track\n",
    "        for class_pair in classes_to_track:\n",
    "            class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
    "            class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
    "\n",
    "            # Check for intersections and update score\n",
    "            for box1 in class1_boxes:\n",
    "                for box2 in class2_boxes:\n",
    "                    if intersects(box1, box2):\n",
    "                        score += 1\n",
    "                        print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score)\n",
    "\n",
    "        # Frame Tensor Conversion for Drawing\n",
    "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
    "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
    "\n",
    "        # Draw bounding boxes and segmentation masks\n",
    "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
    "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
    "\n",
    "        # Convert output image for displaying\n",
    "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        output_image = np.clip(output_image, 0, 255)  # Ensure values are within 0-255\n",
    "\n",
    "        # Draw score text\n",
    "        cv2.putText(output_image, f'Score: {score}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Frame', output_image)\n",
    "        \n",
    "        # Just before cv2.imshow\n",
    "        print(f\"Output Image Size: {output_image.shape}\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# process_video_check(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/process_video_check_track.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/process_video_check_track.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor, convert_image_dtype\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "from bytetracker.kalman_filter import KalmanFilter\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Utility Functions\n",
    "def tlwh_to_xyah(tlwh):\n",
    "    x, y, w, h = tlwh\n",
    "    center_x = x + w / 2\n",
    "    center_y = y + h / 2\n",
    "    aspect_ratio = w / h\n",
    "    return np.array([center_x, center_y, aspect_ratio, h])\n",
    "\n",
    "def xyah_to_tlwh(xyah):\n",
    "    \"\"\"\n",
    "    Convert the state vector (including velocity or other components)\n",
    "    back to the top-left x, y, width, height (tlwh) bounding box format.\n",
    "    \"\"\"\n",
    "    # Ensure xyah only contains [center_x, center_y, aspect_ratio, height]\n",
    "    if len(xyah) > 4:\n",
    "        xyah = xyah[:4]  # Consider only the first 4 components for conversion\n",
    "    center_x, center_y, aspect_ratio, h = xyah\n",
    "    w = aspect_ratio * h\n",
    "    tlwh = np.array([center_x - w / 2, center_y - h / 2, w, h])\n",
    "    return tlwh\n",
    "\n",
    "# Track Class\n",
    "class Track:\n",
    "    _id_count = 1\n",
    "    \n",
    "    def __init__(self, tlwh, score):\n",
    "        self.kalman_filter = KalmanFilter()\n",
    "        self.xyah = tlwh_to_xyah(tlwh)\n",
    "        self.state, self.covariance = self.kalman_filter.initiate(self.xyah)\n",
    "        self.track_id = Track._id_count\n",
    "        Track._id_count += 1\n",
    "        self.score = score\n",
    "        self.hits = 1\n",
    "        self.age = 1\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def predict(self):\n",
    "        self.state, self.covariance = self.kalman_filter.predict(self.state, self.covariance)\n",
    "        self.age += 1\n",
    "        self.time_since_update += 1\n",
    "\n",
    "    def update(self, tlwh):\n",
    "        self.xyah = tlwh_to_xyah(tlwh)\n",
    "        self.state, self.covariance = self.kalman_filter.update(self.state, self.covariance, self.xyah)\n",
    "        self.hits += 1\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def get_tlwh(self):\n",
    "        return xyah_to_tlwh(self.state[:4])\n",
    "\n",
    "def iou(bbox1, bbox2):\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    xi1, yi1 = max(x1, x2), max(y1, y2)\n",
    "    xi2, yi2 = min(x1+w1, x2+w2), min(y1+h1, y2+h2)\n",
    "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "    box1_area, box2_area = w1*h1, w2*h2\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def adjust_boxes(boxes, scale_factor=(1.0, 1.0)):\n",
    "    \"\"\"Adjust bounding boxes by a scale factor.\"\"\"\n",
    "    adjusted_boxes = []\n",
    "    for box in boxes:\n",
    "        # Assuming box is in tlwh format\n",
    "        x, y, w, h = box\n",
    "        adjusted_box = [x * scale_factor[0], y * scale_factor[1], (x + w) * scale_factor[0], (y + h) * scale_factor[1]]\n",
    "        adjusted_boxes.append(adjusted_box)\n",
    "    return adjusted_boxes\n",
    "\n",
    "def process_video_check_track(video_path, model, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    tracks = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        original_width, original_height = frame.shape[1], frame.shape[0]\n",
    "        frame_tensor = to_tensor(frame).unsqueeze_(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)[0]\n",
    "\n",
    "        keep = prediction['scores'] > threshold\n",
    "        boxes = prediction['boxes'][keep].cpu()\n",
    "        scores = prediction['scores'][keep].cpu()\n",
    "\n",
    "        # Ensure the loop for processing detections and adjusting boxes is correctly scoped\n",
    "        for i, box in enumerate(boxes):\n",
    "            det = box.numpy()  # Convert to numpy array\n",
    "            score = scores[i].item()\n",
    "\n",
    "            # Scale factor for adjusting boxes to the original frame size\n",
    "            scale_factor = (original_width / frame_tensor.shape[3], original_height / frame_tensor.shape[2])\n",
    "            adjusted_box = adjust_boxes([det], scale_factor)[0]\n",
    "            adjusted_box_tensor = torch.tensor([adjusted_box], dtype=torch.float32)\n",
    "            adjusted_tlbr = box_convert(adjusted_box_tensor, in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "            matched = False\n",
    "            for track in tracks:\n",
    "                if iou(adjusted_tlbr.squeeze(0).numpy(), torch.tensor(track.get_tlwh()).unsqueeze(0).numpy()) > 0.5:\n",
    "                    track.update(det)\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if not matched:\n",
    "                tracks.append(Track(det, score))\n",
    "\n",
    "        # Prepare the frame for drawing\n",
    "        frame_draw = convert_image_dtype(frame_tensor.squeeze(0), dtype=torch.uint8)\n",
    "\n",
    "        # Correctly use adjusted boxes within this loop\n",
    "        for track in tracks:\n",
    "            tlwh = track.get_tlwh()\n",
    "            track_box_adjusted = adjust_boxes([tlwh], scale_factor)[0]\n",
    "            track_box_tensor = torch.tensor([track_box_adjusted], dtype=torch.float32)\n",
    "            track_tlbr = box_convert(track_box_tensor, in_fmt='xywh', out_fmt='xyxy')\n",
    "            frame_draw = draw_bounding_boxes(frame_draw, track_tlbr, colors=\"blue\", width=3)\n",
    "\n",
    "        # Convert tensor to numpy array for display\n",
    "        output_image = frame_draw.permute(1, 2, 0).cpu().numpy()\n",
    "        cv2.imshow('Frame with Tracks', output_image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from going_modular.utils import get_device, create_directory, get_project, download_videos_from_youtube\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import get_model_instance_segmentation\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.transforms import get_transform\n",
    "from going_modular.process_video_check_track import process_video_check_track\n",
    "import utils\n",
    "\n",
    "def load_classes_from_json(annotation_path):\n",
    "    with open(annotation_path) as f:\n",
    "        data = json.load(f)\n",
    "    categories = data['categories']\n",
    "    classes = {category['id']: category['name'] for category in categories}\n",
    "    return classes\n",
    "\n",
    "def split_dataset(dataset, split_ratio=0.8):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * split_ratio)\n",
    "    valid_size = total_size - train_size\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(42))\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def main(args):\n",
    "    # Create model path directory\n",
    "    model_path = Path(args.model_path)\n",
    "    create_directory(model_path)\n",
    "\n",
    "    # Check if dataset is already downloaded\n",
    "    project_folder = Path.cwd() / f'{args.project_folder_name}-{args.version}'\n",
    "    print(f\"Checking dataset at: {project_folder.absolute()}\")\n",
    "\n",
    "    if not project_folder.exists():\n",
    "        print(\"Downloading dataset...\")\n",
    "        dataset = get_project(args.api_key, args.workspace, args.project_name, args.version)\n",
    "    else:\n",
    "        print(\"Project data already downloaded. Skipping download...\")\n",
    "\n",
    "    # Load classes from annotation file\n",
    "    classes_path = project_folder / 'train' / '_annotations.coco.json'\n",
    "    if not classes_path.exists():\n",
    "        print(f\"Error: Annotation file for the train dataset not found at {classes_path.absolute()}.\")\n",
    "        return\n",
    "    classes = load_classes_from_json(classes_path)\n",
    "    print(\"Classes loaded:\", classes)\n",
    "\n",
    "    # Initialize model\n",
    "    num_classes = len(classes) + 1\n",
    "    device = get_device()\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=args.hidden_layer)\n",
    "    model.to(device)\n",
    "\n",
    "    # Load datasets\n",
    "    datasets = {}\n",
    "    data_loaders = {}\n",
    "    for dtype in ['train', 'valid', 'test']:\n",
    "        ann_path = project_folder / dtype / '_annotations.coco.json'\n",
    "        img_dir = project_folder / dtype\n",
    "        if ann_path.exists() and img_dir.exists():\n",
    "            datasets[dtype] = CustomCocoDataset(str(ann_path), str(img_dir), transforms=get_transform(train=dtype=='train'))\n",
    "            batch_size = 2 if dtype == 'train' else 1\n",
    "            data_loaders[dtype] = DataLoader(datasets[dtype], batch_size=batch_size, shuffle=dtype=='train', num_workers=0, collate_fn=utils.collate_fn)\n",
    "            print(f\"{dtype.capitalize()} dataset loaded.\")\n",
    "        else:\n",
    "            print(f\"{dtype.capitalize()} dataset not found or incomplete. Skipping.\")\n",
    "\n",
    "    # Split dataset if valid is not loaded\n",
    "    if args.mode == 'train' and 'train' in datasets and 'valid' not in datasets:\n",
    "        train_dataset, valid_dataset = split_dataset(datasets['train'])\n",
    "        data_loaders['train'] = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "        data_loaders['valid'] = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "\n",
    "        # Execute training\n",
    "        if 'train' in data_loaders:\n",
    "            print(\"Starting training process...\")\n",
    "            train_model(model, data_loaders['train'], data_loaders.get('valid'), device, args.num_epochs, lr=args.lr)\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(f\"Model saved at: {model_file_path}\")\n",
    "        else:\n",
    "            print(\"Training failed. No data loaders available.\")\n",
    "\n",
    "    # Process video if mode is 'process_video'\n",
    "    if args.mode == 'process_video':\n",
    "        video_filename = args.video_name if args.video_name.endswith('.mp4') else f\"{args.video_name}.mp4\"\n",
    "        video_path = Path(args.data_path) / video_filename\n",
    "        if not video_path.exists():\n",
    "            print(\"Downloading video...\")\n",
    "            download_videos_from_youtube([args.video_url], str(Path(args.data_path)))\n",
    "        \n",
    "        if video_path.exists():\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            if model_file_path.exists():\n",
    "                model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
    "                #process_video_check_track(video_path, model, device, classes, [('Basketball', 'Hoop')], threshold=args.threshold)\n",
    "                process_video_check_track(video_path, model, device, threshold=0.5)\n",
    "            else:\n",
    "                print(\"Model weights file not found. Please train the model first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a model for object detection or process video\")\n",
    "    parser.add_argument('--api_key', type=str, default=\"your_roboflow_api_key\", help='API key for Roboflow')\n",
    "    parser.add_argument('--workspace', type=str, default=\"your_roboflow_workspace\", help='Workspace name in Roboflow')\n",
    "    parser.add_argument('--project_name', type=str, default=\"your_roboflow_project\", help='Project name in Roboflow')\n",
    "    parser.add_argument('--project_folder_name', type=str, default=\"your_roboflow_project_folder_name\", help='Project folder name in Roboflow')\n",
    "    parser.add_argument('--version', type=int, default=1, help='Version of the dataset in Roboflow')\n",
    "    parser.add_argument('--hidden_layer', type=int, default=256, help='Hidden layer size for the MaskRCNN predictor')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs to train the model')\n",
    "    parser.add_argument('--video_url', type=str, default=\"https://www.youtube.com/watch?v=example_video_id\", help='URL of the video to process')\n",
    "    parser.add_argument('--video_name', type=str, default=\"your_youtube_video_name\", help='Name of the video file (with extension) to process')\n",
    "    parser.add_argument('--threshold', type=float, default=0.6, help='Detection threshold for process_video')\n",
    "    parser.add_argument('--display_video', type=bool, default=True, help='Whether to display the video during processing')\n",
    "    parser.add_argument('--data_path', type=str, default='results/data', help='Path to save downloaded data')\n",
    "    parser.add_argument('--model_path', type=str, default='results/models', help='Path to save model weights')\n",
    "    parser.add_argument('--mode', type=str, choices=['train', 'process_video'], default='train', help='Mode of operation: train or process_video')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset at: c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\basketball-and-hoop-11\n",
      "Project data already downloaded. Skipping download...\n",
      "Classes loaded: {0: 'Basketball-and-Hoop', 1: 'Backboard', 2: 'Basketball', 3: 'Hoop', 4: 'Player'}\n",
      "Train dataset loaded.\n",
      "Valid dataset loaded.\n",
      "Test dataset loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\train.py\", line 123, in <module>\n",
      "    main(args)\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\train.py\", line 100, in main\n",
      "    process_video_check_track(video_path, model, device, threshold=0.5)\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\going_modular\\process_video_check_track.py\", line 112, in process_video_check_track\n",
      "    if iou(adjusted_tlbr.squeeze(0).numpy(), torch.tensor(track.get_tlwh()).unsqueeze(0).numpy()) > 0.5:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\going_modular\\process_video_check_track.py\", line 62, in iou\n",
      "    x2, y2, w2, h2 = bbox2\n",
      "    ^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 4, got 1)\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
    "                --workspace basketball-formations \\\n",
    "                --project_name basketball-and-hoop-7xk0h \\\n",
    "                --project_folder_name basketball-and-hoop \\\n",
    "                --version 11 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 10 \\\n",
    "                --threshold 0.6 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=kh7s2tGvswc&t=1s\" \\\n",
    "                --video_name \"Devin Booker Sets Record, Wins Three-Point Contest\" \\\n",
    "                --mode process_video\n",
    "\n",
    "#ex command line usage\n",
    "#python train.py --api_key htpcxp3XQh7SsgMfjJns --workspace basketball-formations --project_name basketball-and-hoop-7xk0h --project_folder_name basketball-and-hoop --version 10 --hidden_layer 256 --lr 0.005 --num_epochs 1 --threshold 0.6 --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" --video_name \"The Best NBA 3 Point Contest Performances\" --mode process_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
