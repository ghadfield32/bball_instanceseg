{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQKPGLnjQ05hWOWcq320eT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghadfield32/bball_instanceseg/blob/main/roboflow_r_cnn_auto_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ghadfield32/bball_instanceseg\n",
        "%cd bball_instanceseg/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC4mhi3fYzX3",
        "outputId": "7f345ee0-5e0d-4b50-96ed-f96ea09b8d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bball_instanceseg'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 47 (delta 5), reused 43 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (47/47), 1.72 MiB | 6.62 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "/content/bball_instanceseg\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.1.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.2.2)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (1.0.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (4.8.0.76)\n",
            "Collecting yt_dlp (from -r requirements.txt (line 19))\n",
            "  Downloading yt_dlp-2023.12.30-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg (from -r requirements.txt (line 20))\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (4.66.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (3.27.9)\n",
            "Collecting onnx (from -r requirements.txt (line 23))\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (5.5.6)\n",
            "Collecting torchinfo (from -r requirements.txt (line 25))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (2.15.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.0.7)\n",
            "Collecting roboflow (from -r requirements.txt (line 28))\n",
            "  Downloading roboflow-1.1.17-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ultralytics (from -r requirements.txt (line 29))\n",
            "  Downloading ultralytics-8.1.7-py3-none-any.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 7)) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 8)) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 8)) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 13)) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 16)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 16)) (3.2.0)\n",
            "Collecting mutagen (from yt_dlp->-r requirements.txt (line 19))\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6TiyWHfDYwy"
      },
      "outputs": [],
      "source": [
        "\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "%cd ByteTrack\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 setup.py develop\n",
        "!pip3 install cython\n",
        "!pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!pip3 install cython_bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI"
      ],
      "metadata": {
        "id": "H8WztQPDGq2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the current directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current directory:\", current_directory)\n",
        "\n",
        "# Navigate to the parent directory (ByteTrack's parent)\n",
        "%cd /content/bball_instanceseg\n",
        "\n",
        "print(\"After directory:\", current_directory)"
      ],
      "metadata": {
        "id": "ZceSC1C42HZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required files from torchvision\n",
        "import requests\n",
        "def download_files(urls):\n",
        "    for url in urls:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
        "                file.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
        "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
        "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
        "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
        "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
        "]\n",
        "#download_files(urls) #included in git clone bball_instanceseg"
      ],
      "metadata": {
        "id": "DAtVv3r7TtwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note: this notebook requires torch >= 1.10.0\n",
        "print(torch.__version__)\n",
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "\n",
        "\n",
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "B_f0dLL_D46Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def create_directory(dir_path):\n",
        "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "#create going_modular repository\n",
        "create_directory(\"going_modular\")"
      ],
      "metadata": {
        "id": "RlhFu_K5EuhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "#!pip install roboflow\n",
        "\n",
        "\n",
        "#from roboflow import Roboflow\n",
        "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
        "#project = rf.workspace(\"ai-79z1a\").project(\"basketball_child\")\n",
        "#dataset = project.version(6).download(\"coco-segmentation\")\n",
        "\n",
        "\n",
        "from roboflow import Roboflow\n",
        "import torch\n",
        "import requests\n",
        "import yt_dlp\n",
        "import os\n",
        "\n",
        "def download_videos_from_youtube(video_urls, output_path):\n",
        "    \"\"\"\n",
        "    Downloads videos from YouTube.\n",
        "\n",
        "    Args:\n",
        "    video_urls (list): List of YouTube video URLs.\n",
        "    output_path (str): Directory where videos will be saved.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing lists of successful and failed downloads.\n",
        "    \"\"\"\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'best',\n",
        "        'outtmpl': output_path + '/%(title)s.%(ext)s',\n",
        "        'quiet': True\n",
        "    }\n",
        "\n",
        "    failed_downloads = []\n",
        "    successful_downloads = []\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        for url in video_urls:\n",
        "            try:\n",
        "                ydl.download([url])\n",
        "                print(f\"Successfully downloaded {url}\")\n",
        "                successful_downloads.append(url)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download {url}: {e}\")\n",
        "                failed_downloads.append(url)\n",
        "\n",
        "    return successful_downloads, failed_downloads\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def get_project(api_key, workspace, project_name, version):\n",
        "    rf = Roboflow(api_key=api_key)\n",
        "    project = rf.workspace(workspace).project(project_name)\n",
        "    dataset = project.version(version).download(\"coco-segmentation\")\n",
        "    return dataset\n",
        "\n",
        "def download_files(urls):\n",
        "    for url in urls:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
        "                file.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "def construct_dataset_paths(project_name, version):\n",
        "    base_path = f\"{project_name}-{version}\"\n",
        "    train_annotation_path = f\"{base_path}/train/_annotations.coco.json\"\n",
        "    valid_annotation_path = f\"{base_path}/valid/_annotations.coco.json\"\n",
        "    test_annotation_path = f\"{base_path}/test/_annotations.coco.json\"\n",
        "\n",
        "    train_root_dir = f\"{base_path}/train\"\n",
        "    valid_root_dir = f\"{base_path}/valid\"\n",
        "    test_root_dir = f\"{base_path}/test\"\n",
        "\n",
        "    return train_annotation_path, valid_annotation_path, test_annotation_path, train_root_dir, valid_root_dir, test_root_dir\n",
        "\n",
        "def create_directory(dir_path):\n",
        "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n"
      ],
      "metadata": {
        "id": "DSTP3BeZEqVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/coco_dataset.py\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision import tv_tensors\n",
        "\n",
        "class CustomCocoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotation_path, root_dir, transforms=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        with open(annotation_path) as f:\n",
        "            self.annotations = json.load(f)\n",
        "\n",
        "        # Filter out images without annotations\n",
        "        annotated_images = []\n",
        "        for img in self.annotations['images']:\n",
        "            image_id = img['id']\n",
        "            anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
        "            if len(anns) > 0:\n",
        "                annotated_images.append(img)\n",
        "\n",
        "        self.image_ids = [img['id'] for img in annotated_images]\n",
        "\n",
        "        # Update the self.annotations['images'] to include only annotated images\n",
        "        self.annotations['images'] = annotated_images\n",
        "\n",
        "        #print(\"Number of images:\", len(self.annotations['images']))\n",
        "        #print(\"Sample image entry:\", self.annotations['images'][0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.annotations['images'][idx]\n",
        "        image_id = img_info['id']\n",
        "\n",
        "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_tensor = F.to_tensor(img)\n",
        "        #print(\"Image size (PIL):\", img.size)\n",
        "        #print(\"Image shape (tensor):\", img_tensor.shape)\n",
        "\n",
        "        anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
        "        #print(\"Number of annotations for this image:\", len(anns))\n",
        "\n",
        "        boxes = [ann['bbox'] for ann in anns]  # bbox format: [x_min, y_min, width, height]\n",
        "        # Convert from XYWH to XYXY format\n",
        "        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = [ann['category_id'] for ann in anns]\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        #print(\"Boxes shape:\", boxes.shape)\n",
        "        #print(\"Labels:\", labels)\n",
        "        # Debug print\n",
        "        #print(f\"Boxes shape for image {idx}: {boxes.shape}\")\n",
        "\n",
        "        masks = []\n",
        "        for ann in anns:\n",
        "            if 'segmentation' in ann and isinstance(ann['segmentation'], list):\n",
        "                for seg in ann['segmentation']:\n",
        "                    mask_img = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
        "                    ImageDraw.Draw(mask_img).polygon(seg, outline=1, fill=1)\n",
        "                    mask = np.array(mask_img)\n",
        "                    masks.append(mask)\n",
        "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
        "        #print(\"Masks shape:\", masks.shape)\n",
        "\n",
        "        areas = [ann['area'] for ann in anns]\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        iscrowd = [ann['iscrowd'] for ann in anns]\n",
        "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "        # Convert masks to Mask format\n",
        "        masks = tv_tensors.Mask(masks)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id  # Changed to integer\n",
        "        target[\"area\"] = areas\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        #print(\"Target:\", target)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img_tensor, target = self.transforms(img_tensor, target)\n",
        "\n",
        "        return img_tensor, target\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9rKX4lCNEsBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/visualization_utils.py\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F  # Add this import\n",
        "\n",
        "# New function to visualize transformations\n",
        "def visualize_transformation(dataset, idx):\n",
        "    img, target = dataset[idx]\n",
        "    transformed_img, transformed_target = dataset.transforms(img, target)\n",
        "    original_img = F.to_pil_image(img)\n",
        "    transformed_img = F.to_pil_image(transformed_img)\n",
        "\n",
        "    plt.figure(figsize=(24, 6))\n",
        "    # Original Image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original_img)\n",
        "    for box in target[\"boxes\"]:\n",
        "        x_min, y_min, x_max, y_max = box.tolist()\n",
        "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        plt.gca().add_patch(rect)\n",
        "        #print(x_min, y_min, x_max, y_max)\n",
        "    plt.title(f\"Original Image - ID: {idx}\")\n",
        "\n",
        "    # Transformed Image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(transformed_img)\n",
        "    for box in transformed_target[\"boxes\"]:\n",
        "        x_min, y_min, x_max, y_max = box.tolist()\n",
        "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='b', facecolor='none')\n",
        "        plt.gca().add_patch(rect)\n",
        "        #print(x_min, y_min, x_max, y_max)\n",
        "    plt.title(f\"Transformed Image - ID: {idx}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def visualize_bbox(dataset, idx):\n",
        "    img, target = dataset[idx]\n",
        "    original_img = F.to_pil_image(img)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(original_img)\n",
        "\n",
        "    for box in target[\"boxes\"]:  # Access the boxes directly\n",
        "        x_min, y_min, x_max, y_max = box.tolist()\n",
        "        # Debug print\n",
        "        print(f\"Visualizing BBox - xmin: {x_min}, ymin: {y_min}, xmax: {x_max}, ymax: {y_max}\")\n",
        "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        plt.gca().add_patch(rect)\n",
        "\n",
        "    plt.title(f\"Image with Bounding Boxes - ID: {idx}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ijXbdNRJFA_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_utils.py\n",
        "import json\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def load_classes_from_json(file_path):\n",
        "    \"\"\"\n",
        "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where keys are class IDs and values are class names.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Extracting classes and their IDs\n",
        "    classes = {category['id']: category['name'] for category in data['categories']}\n",
        "    return classes\n",
        "\n",
        "# Usage example:\n",
        "#classes = load_classes_from_json('basketball_child-6/test/_annotations.coco.json')\n",
        "#print(classes)\n",
        "\n",
        "# model_utils.py\n",
        "def get_model_instance_segmentation(num_classes, hidden_layer=256):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "hXwBfXJlFGrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/transforms.py\n",
        "import torch  # Add this import statement\n",
        "from torchvision.transforms import v2 as T\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, ConvertImageDtype\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    #if train:\n",
        "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n"
      ],
      "metadata": {
        "id": "2vZGpgpyFIT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "# train.py\n",
        "import torch\n",
        "import torchvision\n",
        "from engine import train_one_epoch, evaluate\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "from coco_eval import CocoEvaluator\n",
        "\n",
        "def train_model(model, data_loader, data_loader_valid, device, num_epochs,\n",
        "                lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        lr_scheduler.step()\n",
        "        evaluate(model, data_loader_valid, device=device)\n",
        "\n",
        "    torch.save(model.state_dict(), 'models/model_weights.pth')\n"
      ],
      "metadata": {
        "id": "ixX3QBTfGCTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/process_video_check.py\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.transforms import v2 as T\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "\n",
        "def intersects(box1, box2):\n",
        "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
        "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
        "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
        "\n",
        "def process_video_check(video_path, model, device, classes, classes_to_track, threshold=0.5):\n",
        "    model.eval()\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "\n",
        "    score = 0  # Initialize the score counter\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_tensor = T.ToTensor()(frame).unsqueeze_(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model(frame_tensor)[0]\n",
        "\n",
        "        pred_scores = prediction['scores']\n",
        "        pred_boxes = prediction['boxes']\n",
        "        pred_labels = prediction['labels']\n",
        "        pred_masks = prediction['masks']\n",
        "\n",
        "        keep = pred_scores > threshold\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "        pred_labels = pred_labels[keep]\n",
        "        pred_masks = pred_masks[keep]\n",
        "\n",
        "        if not keep.any():\n",
        "            continue  # Skip this frame if no detections are kept\n",
        "\n",
        "        # Convert numeric labels to class names\n",
        "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
        "\n",
        "        # Iterate through each pair of classes to track\n",
        "        for class_pair in classes_to_track:\n",
        "            class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
        "            class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
        "\n",
        "            # Check for intersections and update score\n",
        "            for box1 in class1_boxes:\n",
        "                for box2 in class2_boxes:\n",
        "                    if intersects(box1, box2):\n",
        "                        score += 1\n",
        "                        print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score)\n",
        "\n",
        "        # Frame Tensor Conversion for Drawing\n",
        "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
        "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
        "\n",
        "        # Draw bounding boxes and segmentation masks\n",
        "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
        "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
        "\n",
        "        # Convert output image for displaying\n",
        "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
        "        output_image = np.clip(output_image, 0, 255)  # Ensure values are within 0-255\n",
        "\n",
        "        # Draw score text\n",
        "        cv2.putText(output_image, f'Score: {score}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        cv2.imshow('Frame', output_image)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Example usage\n",
        "# process_video_check(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
      ],
      "metadata": {
        "id": "5tQeRmH_GD-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/process_video_check_track.py\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.transforms import v2 as T\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "from ByteTrack.yolox.tracker.byte_tracker import BYTETracker\n",
        "\n",
        "# Define ByteTrack arguments (customize as needed)\n",
        "class ByteTrackArgument:\n",
        "    track_thresh = 0.5\n",
        "    track_buffer = 30\n",
        "    match_thresh = 0.8\n",
        "    aspect_ratio_thresh = 1.6\n",
        "    min_box_area = 10\n",
        "\n",
        "def intersects(box1, box2):\n",
        "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
        "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
        "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
        "\n",
        "def process_video_check_track(video_path, model, device, classes, classes_to_track, threshold=0.5):\n",
        "    model.eval()\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "\n",
        "    # Initialize ByteTrack\n",
        "    tracker = BYTETracker(ByteTrackArgument)\n",
        "\n",
        "    score_counter = 0  # Initialize a separate score counter\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Debug: Check the type and shape of the frame\n",
        "        print(f\"Frame type: {type(frame)}, Frame shape: {frame.shape}\")\n",
        "\n",
        "        frame_tensor = T.ToTensor()(frame).unsqueeze_(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model(frame_tensor)[0]\n",
        "\n",
        "        pred_scores = prediction['scores']\n",
        "        pred_boxes = prediction['boxes']\n",
        "        pred_labels = prediction['labels']\n",
        "        pred_masks = prediction['masks']\n",
        "\n",
        "        keep = pred_scores > threshold\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "        pred_labels = pred_labels[keep]\n",
        "        pred_masks = pred_masks[keep]\n",
        "\n",
        "        if not keep.any():\n",
        "            continue  # Skip this frame if no detections are kept\n",
        "\n",
        "        # Convert numeric labels to class names\n",
        "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
        "\n",
        "        # Iterate through each pair of classes to track\n",
        "        for class_pair in classes_to_track:\n",
        "            class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
        "            class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
        "\n",
        "            # Check for intersections and update score counter\n",
        "            for box1 in class1_boxes:\n",
        "                for box2 in class2_boxes:\n",
        "                    if intersects(box1, box2):\n",
        "                        score_counter += 1\n",
        "                        print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score_counter)\n",
        "\n",
        "        # Frame Tensor Conversion for Drawing\n",
        "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
        "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
        "\n",
        "        # Draw bounding boxes and segmentation masks\n",
        "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
        "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
        "\n",
        "        # Convert output image for displaying\n",
        "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
        "        output_image = np.clip(output_image, 0, 255)  # Ensure values are within 0-255\n",
        "\n",
        "        # Draw score text\n",
        "        cv2.putText(output_image, f'Score: {score_counter}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Prepare detections for ByteTrack\n",
        "        dets = []\n",
        "        for box, det_score in zip(pred_boxes, pred_scores):\n",
        "            if det_score > threshold:\n",
        "                x1, y1, x2, y2 = box.tolist()\n",
        "                current_det = [x1, y1, x2, y2, det_score.item()]\n",
        "                dets.append(current_det)\n",
        "\n",
        "        # Frame information for BYTETracker\n",
        "        img_info = {\"height\": frame.shape[0], \"width\": frame.shape[1]}\n",
        "        img_size = [frame.shape[1], frame.shape[0]]\n",
        "\n",
        "        # Update ByteTrack\n",
        "        online_targets = tracker.update(dets, img_info, img_size)\n",
        "\n",
        "        # Process tracking results and draw on frame\n",
        "        for t in online_targets:\n",
        "            tlwh = t.tlwh\n",
        "            tid = t.track_id\n",
        "            # Draw tracking ID and box (customize as needed)\n",
        "            cv2.rectangle(frame, (int(tlwh[0]), int(tlwh[1])), (int(tlwh[0]+tlwh[2]), int(tlwh[1]+tlwh[3])), (0,255,0), 2)\n",
        "            cv2.putText(frame, str(tid), (int(tlwh[0]), int(tlwh[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "        cv2.imshow('Frame', frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# process_video_check_track(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
      ],
      "metadata": {
        "id": "mCeEC_gd1qBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import argparse\n",
        "import os\n",
        "from going_modular.utils import (get_device, create_directory, get_project,\n",
        "                                 download_files, construct_dataset_paths,\n",
        "                                 download_videos_from_youtube)\n",
        "from going_modular.coco_dataset import CustomCocoDataset\n",
        "from going_modular.model_utils import (get_model_instance_segmentation,\n",
        "                                       load_classes_from_json)\n",
        "from going_modular.engine import train_model\n",
        "from going_modular.transforms import get_transform\n",
        "from going_modular.process_video_check_track import process_video_check_track\n",
        "import utils\n",
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "\n",
        "# Note: this notebook requires torch >= 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "\n",
        "def main(args):\n",
        "    # Create directories for data and models\n",
        "    data_path = Path(args.data_path)\n",
        "    model_path = Path(args.model_path)\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    model_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Get the project dataset\n",
        "    dataset = get_project(args.api_key, args.workspace, args.project_name, args.version)\n",
        "\n",
        "    # Load classes from json\n",
        "    classes = load_classes_from_json(f'{args.project_folder_name}-{args.version}/test/_annotations.coco.json')\n",
        "    print(classes)\n",
        "\n",
        "    # Set the number of classes\n",
        "    num_classes = len(classes) + 1\n",
        "\n",
        "    # Initialize device\n",
        "    device = get_device()\n",
        "\n",
        "    # Construct dataset paths\n",
        "    train_annotation_path, valid_annotation_path, test_annotation_path, train_image_dir, valid_image_dir, test_image_dir = construct_dataset_paths(args.project_folder_name, args.version)\n",
        "\n",
        "    # Create datasets with transforms\n",
        "    train_dataset = CustomCocoDataset(train_annotation_path, train_image_dir, transforms=get_transform(train=True))\n",
        "    valid_dataset = CustomCocoDataset(valid_annotation_path, valid_image_dir, transforms=get_transform(train=False))\n",
        "    test_dataset = CustomCocoDataset(test_annotation_path, test_image_dir)\n",
        "\n",
        "    # Load model\n",
        "    model = get_model_instance_segmentation(num_classes,\n",
        "                                            hidden_layer=args.hidden_layer)\n",
        "    model.to(device)\n",
        "\n",
        "    # Data Loaders\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
        "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
        "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_data_loader, valid_data_loader, device, args.num_epochs, lr=args.lr)\n",
        "\n",
        "    # Save model after training\n",
        "    model_file_path = model_path / 'model_weights.pth'\n",
        "    try:\n",
        "        torch.save(model.state_dict(), str(model_file_path))\n",
        "        print(\"Model saved at:\", model_file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "    # Download and process video\n",
        "    try:\n",
        "        successful_downloads, failed_downloads = download_videos_from_youtube([args.video_url], args.data_path)\n",
        "        print(f\"Successfully downloaded {len(successful_downloads)} videos.\")\n",
        "        print(f\"Failed to download {len(failed_downloads)} videos.\")\n",
        "\n",
        "        if successful_downloads:\n",
        "            # Construct video path\n",
        "            video_filename = args.video_name\n",
        "            if not video_filename.endswith('.mp4'):\n",
        "                video_filename += '.mp4'\n",
        "            video_path = data_path / video_filename\n",
        "            print(f\"Video path: {video_path}\")\n",
        "\n",
        "            # Load model state for video processing\n",
        "            model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
        "            model.to(device)\n",
        "\n",
        "            # Process video\n",
        "            process_video_check_track(video_path, model, device, classes, [('Basketball', 'Hoop')], threshold=args.threshold)\n",
        "        else:\n",
        "            print(\"No videos were downloaded.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in video downloading or processing: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train a model for object detection\")\n",
        "    # Define default values\n",
        "    default_api_key = \"your_roboflow_api_key\"\n",
        "    default_workspace = \"your_roboflow_workspace\"\n",
        "    default_project_name = \"your_roboflow_project\"\n",
        "    default_project_folder_name = \"your_roboflow_project_folder_name\"\n",
        "    default_version = 1\n",
        "    default_hidden_layer = 256\n",
        "    default_lr = 0.005\n",
        "    default_num_epochs = 10\n",
        "    default_video_url = \"https://www.youtube.com/watch?v=example_video_id\"\n",
        "    default_video_name = \"your_youtube_video_name\"\n",
        "    default_confidence_threshold = 0.6\n",
        "    default_display_video = True\n",
        "    default_data_path = 'results/data'\n",
        "    default_model_path = 'results/models'\n",
        "\n",
        "    # Setup argparse with defaults\n",
        "    parser.add_argument('--api_key', type=str, default=default_api_key, help='API key for Roboflow')\n",
        "    parser.add_argument('--workspace', type=str, default=default_workspace, help='Workspace name in Roboflow')\n",
        "    parser.add_argument('--project_name', type=str, default=default_project_name, help='Project name in Roboflow')\n",
        "    parser.add_argument('--project_folder_name', type=str, default=default_project_folder_name, help='Project folder name in Roboflow')\n",
        "    parser.add_argument('--version', type=int, default=default_version, help='Version of the dataset in Roboflow')\n",
        "    parser.add_argument('--hidden_layer', type=int, default=default_hidden_layer, help='Hidden layer size for the MaskRCNN predictor')\n",
        "    parser.add_argument('--lr', type=float, default=default_lr, help='Learning rate')\n",
        "    parser.add_argument('--num_epochs', type=int, default=default_num_epochs, help='Number of epochs to train the model')\n",
        "    parser.add_argument('--video_url', type=str, default=default_video_url, help='URL of the video to process')\n",
        "    parser.add_argument('--video_name', type=str, default=default_video_name, help='Name of the video file (with extension) to process')\n",
        "    parser.add_argument('--threshold', type=float, default=default_confidence_threshold, help='Detection threshold for process_video')\n",
        "    parser.add_argument('--display_video', type=bool, default=default_display_video, help='Whether to display the video during processing')\n",
        "    parser.add_argument('--data_path', type=str, default=default_data_path, help='Path to save downloaded data')\n",
        "    parser.add_argument('--model_path', type=str, default=default_model_path, help='Path to save model weights')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "id": "t7LvD4NxGFmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example usage\n",
        "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
        "                --workspace basketball-formations \\\n",
        "                --project_name basketball-and-hoop-7xk0h \\\n",
        "                --project_folder_name basketball-and-hoop \\\n",
        "                --version 10 \\\n",
        "                --hidden_layer 256 \\\n",
        "                --lr 0.005 \\\n",
        "                --num_epochs 1 \\\n",
        "                --threshold 0.6 \\\n",
        "                --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" \\\n",
        "                --video_name \"The Best NBA 3 Point Contest Performances\"\n",
        "\n"
      ],
      "metadata": {
        "id": "X87HNmVqGJag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEq1gNIrV3qQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}