{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust workflow\n",
    "*change load classes to the utils\n",
    "* add in the youtube download to utils XX\n",
    "* add changes for above two changes the main.py XX\n",
    "* change main.py to train.py so you can full automate to CLI easiest\n",
    "* add in the classes to the labels visualizations XX\n",
    "* add uses to it \n",
    "* add to hugging face/Snap AR\n",
    "* attempt to use cv2 to see when basketballs intersect with rims XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images outside of youtube nba footage come from: [text](https://stock.adobe.com/search?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bcontent_type%3Aillustration%5D=1&filters%5Bcontent_type%3Azip_vector%5D=1&filters%5Bcontent_type%3Avideo%5D=1&filters%5Bcontent_type%3Atemplate%5D=1&filters%5Bcontent_type%3A3d%5D=1&filters%5Bcontent_type%3Aaudio%5D=0&filters%5Binclude_stock_enterprise%5D=0&filters%5Bis_editorial%5D=0&filters%5Bfree_collection%5D=0&filters%5Bcontent_type%3Aimage%5D=1&k=basketball+hoop&order=relevance&safe_search=1&limit=100&search_page=21&search_type=pagination&get_facets=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu118\n",
      "CUDA available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "#create going_modular repository\n",
    "create_directory(\"going_modular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required files from torchvision\n",
    "import requests\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "]\n",
    "download_files(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "#!pip install roboflow\n",
    "\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"ai-79z1a\").project(\"basketball_child\")\n",
    "#dataset = project.version(6).download(\"coco-segmentation\")\n",
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "import requests\n",
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "def download_videos_from_youtube(video_urls, output_path):\n",
    "    \"\"\"\n",
    "    Downloads videos from YouTube.\n",
    "\n",
    "    Args:\n",
    "    video_urls (list): List of YouTube video URLs.\n",
    "    output_path (str): Directory where videos will be saved.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of successful and failed downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'best',\n",
    "        'outtmpl': output_path + '/%(title)s.%(ext)s',\n",
    "        'quiet': True\n",
    "    }\n",
    "\n",
    "    failed_downloads = []\n",
    "    successful_downloads = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        for url in video_urls:\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "                print(f\"Successfully downloaded {url}\")\n",
    "                successful_downloads.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "                failed_downloads.append(url)\n",
    "\n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_project(api_key, workspace, project_name, version):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(\"coco-segmentation\")\n",
    "    return dataset\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def construct_dataset_paths(project_name, version):\n",
    "    base_path = f\"{project_name}-{version}\"\n",
    "    train_annotation_path = f\"{base_path}/train/_annotations.coco.json\"\n",
    "    valid_annotation_path = f\"{base_path}/valid/_annotations.coco.json\"\n",
    "    test_annotation_path = f\"{base_path}/test/_annotations.coco.json\"\n",
    "\n",
    "    train_root_dir = f\"{base_path}/train\"\n",
    "    valid_root_dir = f\"{base_path}/valid\"\n",
    "    test_root_dir = f\"{base_path}/test\"\n",
    "\n",
    "    return train_annotation_path, valid_annotation_path, test_annotation_path, train_root_dir, valid_root_dir, test_root_dir\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/coco_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/coco_dataset.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_path, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter out images without annotations\n",
    "        annotated_images = []\n",
    "        for img in self.annotations['images']:\n",
    "            image_id = img['id']\n",
    "            anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "            if len(anns) > 0:\n",
    "                annotated_images.append(img)\n",
    "\n",
    "        self.image_ids = [img['id'] for img in annotated_images]\n",
    "\n",
    "        # Update the self.annotations['images'] to include only annotated images\n",
    "        self.annotations['images'] = annotated_images\n",
    "        \n",
    "        #print(\"Number of images:\", len(self.annotations['images']))\n",
    "        #print(\"Sample image entry:\", self.annotations['images'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.annotations['images'][idx]\n",
    "        image_id = img_info['id']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "        #print(\"Image size (PIL):\", img.size)\n",
    "        #print(\"Image shape (tensor):\", img_tensor.shape)\n",
    "\n",
    "        anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "        #print(\"Number of annotations for this image:\", len(anns))\n",
    "\n",
    "        boxes = [ann['bbox'] for ann in anns]  # bbox format: [x_min, y_min, width, height]\n",
    "        # Convert from XYWH to XYXY format\n",
    "        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = [ann['category_id'] for ann in anns]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        #print(\"Boxes shape:\", boxes.shape)\n",
    "        #print(\"Labels:\", labels)\n",
    "        # Debug print\n",
    "        #print(f\"Boxes shape for image {idx}: {boxes.shape}\")\n",
    "\n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            if 'segmentation' in ann and isinstance(ann['segmentation'], list):\n",
    "                for seg in ann['segmentation']:\n",
    "                    mask_img = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "                    ImageDraw.Draw(mask_img).polygon(seg, outline=1, fill=1)\n",
    "                    mask = np.array(mask_img)\n",
    "                    masks.append(mask)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "        #print(\"Masks shape:\", masks.shape)\n",
    "\n",
    "        areas = [ann['area'] for ann in anns]\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = [ann['iscrowd'] for ann in anns]\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Convert masks to Mask format\n",
    "        masks = tv_tensors.Mask(masks)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id  # Changed to integer\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        #print(\"Target:\", target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/visualization_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/visualization_utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F  # Add this import\n",
    "\n",
    "# New function to visualize transformations\n",
    "def visualize_transformation(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    transformed_img, transformed_target = dataset.transforms(img, target)\n",
    "    original_img = F.to_pil_image(img)\n",
    "    transformed_img = F.to_pil_image(transformed_img)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    for box in target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Original Image - ID: {idx}\")\n",
    "\n",
    "    # Transformed Image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(transformed_img)\n",
    "    for box in transformed_target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='b', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Transformed Image - ID: {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bbox(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    original_img = F.to_pil_image(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(original_img)\n",
    "\n",
    "    for box in target[\"boxes\"]:  # Access the boxes directly\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        # Debug print\n",
    "        print(f\"Visualizing BBox - xmin: {x_min}, ymin: {y_min}, xmax: {x_max}, ymax: {y_max}\")\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.title(f\"Image with Bounding Boxes - ID: {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_utils.py\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extracting classes and their IDs\n",
    "    classes = {category['id']: category['name'] for category in data['categories']}\n",
    "    return classes\n",
    "\n",
    "# Usage example:\n",
    "#classes = load_classes_from_json('basketball_child-6/test/_annotations.coco.json')\n",
    "#print(classes)\n",
    "\n",
    "# model_utils.py\n",
    "def get_model_instance_segmentation(num_classes, hidden_layer=256):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/transforms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/transforms.py\n",
    "import torch  # Add this import statement\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, ConvertImageDtype\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    #if train:\n",
    "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "# train.py\n",
    "import torch\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "\n",
    "def train_model(model, data_loader, data_loader_valid, device, num_epochs,\n",
    "                lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_valid, device=device)\n",
    "\n",
    "    #torch.save(model.state_dict(), 'results/models/model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/process_video_check.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/process_video_check.py\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "def intersects(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
    "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
    "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
    "\n",
    "def process_video_check(video_path, model, device, classes, classes_to_track, threshold=0.5):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    score = 0  # Initialize the score counter\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_tensor = T.ToTensor()(frame).unsqueeze_(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)[0]\n",
    "\n",
    "        pred_scores = prediction['scores']\n",
    "        pred_boxes = prediction['boxes']\n",
    "        pred_labels = prediction['labels']\n",
    "        pred_masks = prediction['masks']\n",
    "\n",
    "        keep = pred_scores > threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        pred_masks = pred_masks[keep]\n",
    "\n",
    "        if not keep.any():\n",
    "            continue  # Skip this frame if no detections are kept\n",
    "\n",
    "        # Convert numeric labels to class names\n",
    "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
    "\n",
    "        # Iterate through each pair of classes to track\n",
    "        for class_pair in classes_to_track:\n",
    "            class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
    "            class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
    "\n",
    "            # Check for intersections and update score\n",
    "            for box1 in class1_boxes:\n",
    "                for box2 in class2_boxes:\n",
    "                    if intersects(box1, box2):\n",
    "                        score += 1\n",
    "                        print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score)\n",
    "\n",
    "        # Frame Tensor Conversion for Drawing\n",
    "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
    "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
    "\n",
    "        # Draw bounding boxes and segmentation masks\n",
    "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
    "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
    "\n",
    "        # Convert output image for displaying\n",
    "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        output_image = np.clip(output_image, 0, 255)  # Ensure values are within 0-255\n",
    "\n",
    "        # Draw score text\n",
    "        cv2.putText(output_image, f'Score: {score}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Frame', output_image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# process_video_check(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu118\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "{0: 'Basketball-and-Hoop', 1: 'Basketball', 2: 'Hoop', 3: 'Player', 4: 'backboard'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The function `to_tensor(...)` is deprecated and will be removed in a future release. Instead, please use `to_image(...)` followed by `to_dtype(..., dtype=torch.float32, scale=True)`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 97\u001b[0m\n\u001b[0;32m     93\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#visualize_transformation(train_dataset, sample_idx)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m  \n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use default hyperparameters\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m#train_model(model, train_data_loader, valid_data_loader, device, num_epochs, lr=lr, momentum=momentum, weight_decay=weight_decay, step_size=step_size, gamma=gamma)\u001b[39;00m\n\u001b[0;32m    101\u001b[0m model_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\going_modular\\engine.py:15\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, data_loader, data_loader_valid, device, num_epochs, lr, momentum, weight_decay, step_size, gamma)\u001b[0m\n\u001b[0;32m     12\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39mstep_size, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m     evaluate(model, data_loader_valid, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\engine.py:31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     29\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 31\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:83\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting the last two dimensions of the Tensor to be H and W instead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m     original_image_sizes\u001b[38;5;241m.\u001b[39mappend((val[\u001b[38;5;241m0\u001b[39m], val[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m---> 83\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Check for degenerate boxes\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# TODO: Move this to a function\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py:140\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages is expected to be a list of 3d tensors of shape [C, H, W], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image)\n\u001b[1;32m--> 140\u001b[0m image, target_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m images[i] \u001b[38;5;241m=\u001b[39m image\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py:195\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.resize\u001b[1;34m(self, image, target)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n\u001b[0;32m    194\u001b[0m bbox \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 195\u001b[0m bbox \u001b[38;5;241m=\u001b[39m \u001b[43mresize_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bbox\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m target:\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py:317\u001b[0m, in \u001b[0;36mresize_boxes\u001b[1;34m(boxes, original_size, new_size)\u001b[0m\n\u001b[0;32m    315\u001b[0m ymin \u001b[38;5;241m=\u001b[39m ymin \u001b[38;5;241m*\u001b[39m ratio_height\n\u001b[0;32m    316\u001b[0m ymax \u001b[38;5;241m=\u001b[39m ymax \u001b[38;5;241m*\u001b[39m ratio_height\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#testing the train.py before command line commitment\n",
    "\n",
    "from going_modular.utils import get_device, get_project, download_files, construct_dataset_paths, download_videos_from_youtube\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.visualization_utils import visualize_transformation, visualize_bbox\n",
    "from going_modular.model_utils import get_model_instance_segmentation, load_classes_from_json\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.transforms import get_transform\n",
    "#from going_modular.video_processing import process_video\n",
    "import torchvision.transforms as T\n",
    "import utils\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "print(torch.__version__)\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "data_path = 'results/data'\n",
    "model_path = 'results/models'\n",
    "create_directory(data_path)\n",
    "create_directory(model_path)\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "]\n",
    "\n",
    "download_files(urls)\n",
    "\n",
    "\n",
    "# roboflow details\n",
    "api_key = \"htpcxp3XQh7SsgMfjJns\"\n",
    "workspace = \"basketball-formations\"#\"ai-79z1a\"\n",
    "project_name = \"basketball-and-hoop-7xk0h\"#\"basketball_child\"\n",
    "project_folder_name = \"basketball-and-hoop\"#\"basketball_child\"\n",
    "version = 10#6\n",
    "\n",
    "\n",
    "# Get the project dataset\n",
    "dataset = get_project(api_key, workspace, project_name, version)\n",
    "\n",
    "#use load_classes_from_json to load the classes and get length of classes\n",
    "classes = load_classes_from_json(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "print(classes)\n",
    "\n",
    "#automate classes volume from roboflow dataset\n",
    "num_classes = len(classes)+1  # Update with the actual number of classes\n",
    "\n",
    "\n",
    "# Hyperparameters and settings\n",
    "lr = 0.01  # Learning rate\n",
    "momentum = 0.8\n",
    "weight_decay = 0.0003\n",
    "step_size = 5\n",
    "gamma = 0.1\n",
    "hidden_layer = 512\n",
    "threshold = 0.6\n",
    "display_video = True  # Set to True to display the video\n",
    "num_workers = 0\n",
    "num_epochs = 1  # Define the number of epochs for training\n",
    "\n",
    "# Initialize device\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "\n",
    "# Construct dataset paths\n",
    "train_annotation_path, valid_annotation_path, test_annotation_path, train_image_dir, valid_image_dir, test_image_dir = construct_dataset_paths(project_folder_name, version)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomCocoDataset(train_annotation_path, train_image_dir, transforms=get_transform(train=True))\n",
    "valid_dataset = CustomCocoDataset(valid_annotation_path, valid_image_dir, transforms=get_transform(train=False))\n",
    "test_dataset = CustomCocoDataset(test_annotation_path, test_image_dir)\n",
    "\n",
    "# Load model\n",
    "model = get_model_instance_segmentation(num_classes, hidden_layer=hidden_layer)\n",
    "model.to(device)\n",
    "\n",
    "# Data Loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=num_workers, collate_fn=utils.collate_fn)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=num_workers, collate_fn=utils.collate_fn)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers, collate_fn=utils.collate_fn)\n",
    "\n",
    "# Optionally, visualize a sample from the dataset\n",
    "sample_idx = 0  # Adjust as needed\n",
    "#visualize_transformation(train_dataset, sample_idx)\n",
    " \n",
    "# Train model\n",
    "train_model(model, train_data_loader, valid_data_loader, \n",
    "            device, num_epochs) # Use default hyperparameters\n",
    "#train_model(model, train_data_loader, valid_data_loader, device, num_epochs, lr=lr, momentum=momentum, weight_decay=weight_decay, step_size=step_size, gamma=gamma)\n",
    "\n",
    "model_file_path = os.path.join(model_path, 'model_weights.pth')\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "\n",
    "# Process video with custom settings\n",
    "video_urls = [\"https://www.youtube.com/watch?v=flm-cZ7r6eE&t=8s\"]\n",
    "output_path = data_path\n",
    "successful_downloads, failed_downloads = download_videos_from_youtube(video_urls, output_path)\n",
    "#print(f\"Successfully downloaded {len(successful_downloads)} videos.\")\n",
    "print(successful_downloads)\n",
    "#print(f\"Failed to download {len(failed_downloads)} videos.\")\n",
    "video_path = data_path + '\\Stephen Curryâ€™s Top 30 Career 3-Pointers ðŸ’¦.mp4'\n",
    "\n",
    "model_path = model_path + '\\model_weights.pth'  # Replace with your model's path\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Process video\n",
    "#process_video(video_path, model, device, classes, threshold=threshold, ball_class_idx=1, rim_class_idx=3, display=display_video)\n",
    "process_video_check(video_path, model, device, threshold=threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os\n",
    "from going_modular.utils import (get_device, create_directory, get_project,\n",
    "                                 download_files, construct_dataset_paths,\n",
    "                                 download_videos_from_youtube)\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import (get_model_instance_segmentation,\n",
    "                                       load_classes_from_json)\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.transforms import get_transform\n",
    "from going_modular.process_video_check import process_video_check\n",
    "import utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def main(args):\n",
    "    data_path = Path(args.data_path)\n",
    "    model_path = Path(args.model_path)\n",
    "    data_path.mkdir(parents=True, exist_ok=True) #redundant\n",
    "    model_path.mkdir(parents=True, exist_ok=True) #redundant\n",
    "\n",
    "    dataset = get_project(args.api_key, args.workspace, args.project_name, args.version)\n",
    "\n",
    "    classes = load_classes_from_json(f'{args.project_folder_name}-{args.version}/test/_annotations.coco.json')\n",
    "    print(\"Classes loaded:\", classes)\n",
    "\n",
    "    num_classes = len(classes) + 1\n",
    "    device = get_device()\n",
    "\n",
    "    train_annotation_path, valid_annotation_path, test_annotation_path, train_image_dir, valid_image_dir, test_image_dir = construct_dataset_paths(args.project_folder_name, args.version)\n",
    "\n",
    "    train_dataset = CustomCocoDataset(train_annotation_path, train_image_dir, transforms=get_transform(train=True))\n",
    "    valid_dataset = CustomCocoDataset(valid_annotation_path, valid_image_dir, transforms=get_transform(train=False))\n",
    "    test_dataset = CustomCocoDataset(test_annotation_path, test_image_dir)\n",
    "\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=args.hidden_layer)\n",
    "    model.to(device)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        train_model(model, train_data_loader, valid_data_loader, device, args.num_epochs, lr=args.lr)\n",
    "        model_file_path = model_path / 'model_weights.pth'\n",
    "        try:\n",
    "            torch.save(model.state_dict(), str(model_file_path))\n",
    "            print(\"Model saved at:\", model_file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "\n",
    "    elif args.mode == 'process_video':\n",
    "        model_file_path = model_path / 'model_weights.pth'\n",
    "        if not model_file_path.exists():\n",
    "            print(\"Model weights file not found. Please train the model first.\")\n",
    "            return\n",
    "\n",
    "        # Download the video\n",
    "        successful_downloads, failed_downloads = download_videos_from_youtube([args.video_url], str(data_path))\n",
    "        print(f\"Successfully downloaded {len(successful_downloads)} videos.\")\n",
    "        print(f\"Failed to download {len(failed_downloads)} videos.\")\n",
    "\n",
    "        if successful_downloads:\n",
    "            # Use the video filename from the command-line argument\n",
    "            video_filename = args.video_name\n",
    "            if not video_filename.endswith('.mp4'):\n",
    "                video_filename += '.mp4'\n",
    "            video_path = data_path / video_filename\n",
    "            print(f\"Video path: {video_path}\")\n",
    "\n",
    "            # Load model state for video processing\n",
    "            model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
    "            model.to(device)\n",
    "\n",
    "            # Process video\n",
    "            process_video_check(video_path, model, device, classes, [('Basketball', 'Hoop')], threshold=args.threshold)\n",
    "        else:\n",
    "            print(\"No videos were downloaded.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a model for object detection or process video\")\n",
    "    parser.add_argument('--api_key', type=str, default=\"your_roboflow_api_key\", help='API key for Roboflow')\n",
    "    parser.add_argument('--workspace', type=str, default=\"your_roboflow_workspace\", help='Workspace name in Roboflow')\n",
    "    parser.add_argument('--project_name', type=str, default=\"your_roboflow_project\", help='Project name in Roboflow')\n",
    "    parser.add_argument('--project_folder_name', type=str, default=\"your_roboflow_project_folder_name\", help='Project folder name in Roboflow')\n",
    "    parser.add_argument('--version', type=int, default=1, help='Version of the dataset in Roboflow')\n",
    "    parser.add_argument('--hidden_layer', type=int, default=256, help='Hidden layer size for the MaskRCNN predictor')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs to train the model')\n",
    "    parser.add_argument('--video_url', type=str, default=\"https://www.youtube.com/watch?v=example_video_id\", help='URL of the video to process')\n",
    "    parser.add_argument('--video_name', type=str, default=\"your_youtube_video_name\", help='Name of the video file (with extension) to process')\n",
    "    parser.add_argument('--threshold', type=float, default=0.6, help='Detection threshold for process_video')\n",
    "    parser.add_argument('--display_video', type=bool, default=True, help='Whether to display the video during processing')\n",
    "    parser.add_argument('--data_path', type=str, default='results/data', help='Path to save downloaded data')\n",
    "    parser.add_argument('--model_path', type=str, default='results/models', help='Path to save model weights')\n",
    "    parser.add_argument('--mode', type=str, choices=['train', 'process_video'], default='train', help='Mode of operation: train or process_video')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last todos:\n",
    "* add in more descriptive args like roboflow_api and youtube_video_url\n",
    "* add in args.parse for the classes to check for intersection\n",
    "* add in huggingface save and reload option so we don't save the model here\n",
    "* add streamlit for this so it can be visualized by anyone\n",
    "* update google collab version the same^ but also so that it can work as they walk into it, so just the clones and then train.py\n",
    "*move to different repo and name instanceseg_track_roboflow or something more rounded than bball_instanceseg (so people don't see your api in previous versions)\n",
    "*^clean up to only the bare essentials\n",
    "*add readme and docstrings to all code to make easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu118\n",
      "Using device: cuda\n",
      "\n",
      "loading Roboflow workspace...\n",
      "\n",
      "loading Roboflow project...\n",
      "Classes loaded: {0: 'Basketball-and-Hoop', 1: 'Basketball', 2: 'Hoop', 3: 'Player', 4: 'backboard'}\n",
      "\n",
      "[download]   0.0% of   36.51MiB at  Unknown B/s ETA Unknown\n",
      "[download]   0.0% of   36.51MiB at  Unknown B/s ETA Unknown\n",
      "[download]   0.0% of   36.51MiB at    2.67MiB/s ETA 00:13  \n",
      "[download]   0.0% of   36.51MiB at    4.22MiB/s ETA 00:08\n",
      "[download]   0.1% of   36.51MiB at    5.07MiB/s ETA 00:07\n",
      "[download]   0.2% of   36.51MiB at    2.19MiB/s ETA 00:16\n",
      "[download]   0.3% of   36.51MiB at    1.86MiB/s ETA 00:19\n",
      "[download]   0.7% of   36.51MiB at    2.70MiB/s ETA 00:13\n",
      "[download]   1.4% of   36.51MiB at    3.97MiB/s ETA 00:09\n",
      "[download]   2.7% of   36.51MiB at    6.19MiB/s ETA 00:05\n",
      "[download]   5.5% of   36.51MiB at   10.34MiB/s ETA 00:03\n",
      "[download]  11.0% of   36.51MiB at   17.06MiB/s ETA 00:01\n",
      "[download]  21.9% of   36.51MiB at   23.94MiB/s ETA 00:01\n",
      "[download]  26.5% of   36.51MiB at   22.89MiB/s ETA 00:01\n",
      "[download]  26.5% of   36.51MiB at  548.99KiB/s ETA 00:50\n",
      "[download]  26.5% of   36.51MiB at    1.26MiB/s ETA 00:21\n",
      "[download]  26.5% of   36.51MiB at    2.94MiB/s ETA 00:09\n",
      "[download]  26.5% of   36.51MiB at    5.18MiB/s ETA 00:05\n",
      "[download]  26.5% of   36.51MiB at    5.21MiB/s ETA 00:05\n",
      "[download]  26.6% of   36.51MiB at    1.96MiB/s ETA 00:13\n",
      "[download]  26.8% of   36.51MiB at    2.38MiB/s ETA 00:11\n",
      "[download]  27.1% of   36.51MiB at    3.45MiB/s ETA 00:07\n",
      "[download]  27.8% of   36.51MiB at    5.05MiB/s ETA 00:05\n",
      "[download]  29.2% of   36.51MiB at    7.74MiB/s ETA 00:03\n",
      "[download]  31.9% of   36.51MiB at   11.07MiB/s ETA 00:02\n",
      "[download]  37.4% of   36.51MiB at   17.37MiB/s ETA 00:01\n",
      "[download]  48.4% of   36.51MiB at   24.84MiB/s ETA 00:00\n",
      "[download]  52.5% of   36.51MiB at   27.90MiB/s ETA 00:00\n",
      "[download]  52.5% of   36.51MiB at  Unknown B/s ETA Unknown\n",
      "[download]  52.5% of   36.51MiB at    1.89MiB/s ETA 00:09  \n",
      "[download]  52.5% of   36.51MiB at    3.34MiB/s ETA 00:05\n",
      "[download]  52.5% of   36.51MiB at    5.76MiB/s ETA 00:03\n",
      "[download]  52.6% of   36.51MiB at    7.49MiB/s ETA 00:02\n",
      "[download]  52.7% of   36.51MiB at    2.92MiB/s ETA 00:05\n",
      "[download]  52.8% of   36.51MiB at    3.36MiB/s ETA 00:05\n",
      "[download]  53.2% of   36.51MiB at    4.29MiB/s ETA 00:03\n",
      "[download]  53.9% of   36.51MiB at    5.87MiB/s ETA 00:02\n",
      "[download]  55.2% of   36.51MiB at    8.39MiB/s ETA 00:01\n",
      "[download]  58.0% of   36.51MiB at   12.98MiB/s ETA 00:01\n",
      "[download]  63.4% of   36.51MiB at   19.60MiB/s ETA 00:00\n",
      "[download]  74.4% of   36.51MiB at   29.53MiB/s ETA 00:00\n",
      "[download]  79.1% of   36.51MiB at   33.34MiB/s ETA 00:00\n",
      "[download]  79.1% of   36.51MiB at  654.85KiB/s ETA 00:11\n",
      "[download]  79.1% of   36.51MiB at    1.16MiB/s ETA 00:06\n",
      "[download]  79.1% of   36.51MiB at    2.26MiB/s ETA 00:03\n",
      "[download]  79.1% of   36.51MiB at    3.64MiB/s ETA 00:02\n",
      "[download]  79.1% of   36.51MiB at    5.48MiB/s ETA 00:01\n",
      "[download]  79.2% of   36.51MiB at    2.39MiB/s ETA 00:03\n",
      "[download]  79.4% of   36.51MiB at    3.02MiB/s ETA 00:02\n",
      "[download]  79.7% of   36.51MiB at    4.05MiB/s ETA 00:01\n",
      "[download]  80.4% of   36.51MiB at    5.98MiB/s ETA 00:01\n",
      "[download]  81.8% of   36.51MiB at    9.24MiB/s ETA 00:00\n",
      "[download]  84.5% of   36.51MiB at   14.23MiB/s ETA 00:00\n",
      "[download]  90.0% of   36.51MiB at   21.89MiB/s ETA 00:00\n",
      "[download] 100.0% of   36.51MiB at   16.84MiB/s ETA 00:00\n",
      "[download] 100% of   36.51MiB in 00:00:01 at 18.32MiB/s  \n",
      "                                                       \n",
      "Successfully downloaded https://www.youtube.com/watch?v=y8i6fsAXDZE\n",
      "Successfully downloaded 1 videos.\n",
      "Failed to download 0 videos.\n",
      "Video path: results\\data\\The Best NBA 3 Point Contest Performances.mp4\n",
      "Intersection detected between Basketball and Hoop, Score: 1\n",
      "Intersection detected between Basketball and Hoop, Score: 2\n",
      "Intersection detected between Basketball and Hoop, Score: 3\n",
      "Intersection detected between Basketball and Hoop, Score: 4\n",
      "Intersection detected between Basketball and Hoop, Score: 5\n",
      "Intersection detected between Basketball and Hoop, Score: 6\n",
      "Intersection detected between Basketball and Hoop, Score: 7\n",
      "Intersection detected between Basketball and Hoop, Score: 8\n",
      "Intersection detected between Basketball and Hoop, Score: 9\n",
      "Intersection detected between Basketball and Hoop, Score: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
    "                --workspace basketball-formations \\\n",
    "                --project_name basketball-and-hoop-7xk0h \\\n",
    "                --project_folder_name basketball-and-hoop \\\n",
    "                --version 10 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 1 \\\n",
    "                --threshold 0.6 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" \\\n",
    "                --video_name \"The Best NBA 3 Point Contest Performances\" \\\n",
    "                --mode process_video\n",
    "\n",
    "#ex command line usage\n",
    "#python train.py --api_key htpcxp3XQh7SsgMfjJns --workspace basketball-formations --project_name basketball-and-hoop-7xk0h --project_folder_name basketball-and-hoop --version 10 --hidden_layer 256 --lr 0.005 --num_epochs 1 --threshold 0.6 --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" --video_name \"The Best NBA 3 Point Contest Performances\" --mode process_video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
