{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust workflow\n",
    "*change load classes to the utils\n",
    "* add in the youtube download to utils XX\n",
    "* add changes for above two changes the main.py XX\n",
    "* change main.py to train.py so you can full automate to CLI easiest\n",
    "* add in the classes to the labels visualizations XX\n",
    "* add uses to it \n",
    "* add to hugging face/Snap AR\n",
    "* attempt to use cv2 to see when basketballs intersect with rims XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images outside of youtube nba footage come from: [text](https://stock.adobe.com/search?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bcontent_type%3Aillustration%5D=1&filters%5Bcontent_type%3Azip_vector%5D=1&filters%5Bcontent_type%3Avideo%5D=1&filters%5Bcontent_type%3Atemplate%5D=1&filters%5Bcontent_type%3A3d%5D=1&filters%5Bcontent_type%3Aaudio%5D=0&filters%5Binclude_stock_enterprise%5D=0&filters%5Bis_editorial%5D=0&filters%5Bfree_collection%5D=0&filters%5Bcontent_type%3Aimage%5D=1&k=basketball+hoop&order=relevance&safe_search=1&limit=100&search_page=21&search_type=pagination&get_facets=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "* must have roboflow dataset\n",
    "* must use train/valid/test split  <add in test split in case of other scenarios\n",
    "* must use coco_segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg-1\\.venv\\Lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Note: this notebook requires torch >= 1.10.0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg-1\\.venv\\Lib\\site-packages\\torch\\__init__.py:141\u001b[0m\n\u001b[0;32m    139\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    140\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 141\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    143\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg-1\\.venv\\Lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "#create going_modular repository\n",
    "create_directory(\"going_modular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required files from torchvision\n",
    "import requests\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "]\n",
    "download_files(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "#!pip install roboflow\n",
    "\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"ai-79z1a\").project(\"basketball_child\")\n",
    "#dataset = project.version(6).download(\"coco-segmentation\")\n",
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "import requests\n",
    "import yt_dlp\n",
    "import utils\n",
    "import shutil\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def download_videos_from_youtube(video_urls, output_path):\n",
    "    \"\"\"\n",
    "    Downloads videos from YouTube.\n",
    "\n",
    "    Args:\n",
    "    video_urls (list): List of YouTube video URLs.\n",
    "    output_path (str): Directory where videos will be saved.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of successful and failed downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'best',\n",
    "        'outtmpl': output_path + '/%(title)s.%(ext)s',\n",
    "        'quiet': True\n",
    "    }\n",
    "\n",
    "    failed_downloads = []\n",
    "    successful_downloads = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        for url in video_urls:\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "                print(f\"Successfully downloaded {url}\")\n",
    "                successful_downloads.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "                failed_downloads.append(url)\n",
    "\n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_project(api_key, workspace, project_name, version):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(\"coco-segmentation\")\n",
    "    return dataset\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def construct_dataset_paths(project_name, version):\n",
    "    base_path = f\"{project_name}-{version}\"\n",
    "    train_annotation_path = f\"{base_path}/train/_annotations.coco.json\"\n",
    "    valid_annotation_path = f\"{base_path}/valid/_annotations.coco.json\"\n",
    "    test_annotation_path = f\"{base_path}/test/_annotations.coco.json\"\n",
    "\n",
    "    train_root_dir = f\"{base_path}/train\"\n",
    "    valid_root_dir = f\"{base_path}/valid\"\n",
    "    test_root_dir = f\"{base_path}/test\"\n",
    "\n",
    "    return train_annotation_path, valid_annotation_path, test_annotation_path, train_root_dir, valid_root_dir, test_root_dir\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(dataset, split_ratio=0.8):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * split_ratio)\n",
    "    valid_size = total_size - train_size\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(42))\n",
    "    return train_dataset, valid_dataset\n",
    "        \n",
    "def delete_folder_and_video(folder_path, video_path):\n",
    "    \"\"\"\n",
    "    Delete the specified folder and video.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path: Path object or str, the path to the folder to delete.\n",
    "    - video_path: Path object or str, the path to the video file to delete.\n",
    "    \"\"\"\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    if video_path.exists():\n",
    "        os.remove(video_path)\n",
    "        print(f\"Deleted video: {video_path}\")\n",
    "    else:\n",
    "        print(f\"Video not found: {video_path}\")\n",
    "        \n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    categories = data['categories']\n",
    "    classes = {category['id']: category['name'] for category in categories}\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/coco_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/coco_dataset.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_path, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter out images without annotations\n",
    "        annotated_images = []\n",
    "        for img in self.annotations['images']:\n",
    "            image_id = img['id']\n",
    "            anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "            if len(anns) > 0:\n",
    "                annotated_images.append(img)\n",
    "\n",
    "        self.image_ids = [img['id'] for img in annotated_images]\n",
    "\n",
    "        # Update the self.annotations['images'] to include only annotated images\n",
    "        self.annotations['images'] = annotated_images\n",
    "        \n",
    "        #print(\"Number of images:\", len(self.annotations['images']))\n",
    "        #print(\"Sample image entry:\", self.annotations['images'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.annotations['images'][idx]\n",
    "        image_id = img_info['id']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "        #print(\"Image size (PIL):\", img.size)\n",
    "        #print(\"Image shape (tensor):\", img_tensor.shape)\n",
    "\n",
    "        anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "        #print(\"Number of annotations for this image:\", len(anns))\n",
    "\n",
    "        boxes = [ann['bbox'] for ann in anns]  # bbox format: [x_min, y_min, width, height]\n",
    "        # Convert from XYWH to XYXY format\n",
    "        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = [ann['category_id'] for ann in anns]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        #print(\"Boxes shape:\", boxes.shape)\n",
    "        #print(\"Labels:\", labels)\n",
    "        # Debug print\n",
    "        #print(f\"Boxes shape for image {idx}: {boxes.shape}\")\n",
    "\n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            if 'segmentation' in ann and isinstance(ann['segmentation'], list):\n",
    "                for seg in ann['segmentation']:\n",
    "                    mask_img = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "                    ImageDraw.Draw(mask_img).polygon(seg, outline=1, fill=1)\n",
    "                    mask = np.array(mask_img)\n",
    "                    masks.append(mask)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "        #print(\"Masks shape:\", masks.shape)\n",
    "\n",
    "        areas = [ann['area'] for ann in anns]\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = [ann['iscrowd'] for ann in anns]\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Convert masks to Mask format\n",
    "        masks = tv_tensors.Mask(masks)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id  # Changed to integer\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        #print(\"Target:\", target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/visualization_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/visualization_utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F  # Add this import\n",
    "\n",
    "# New function to visualize transformations\n",
    "def visualize_transformation(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    transformed_img, transformed_target = dataset.transforms(img, target)\n",
    "    original_img = F.to_pil_image(img)\n",
    "    transformed_img = F.to_pil_image(transformed_img)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    for box in target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Original Image - ID: {idx}\")\n",
    "\n",
    "    # Transformed Image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(transformed_img)\n",
    "    for box in transformed_target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='b', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Transformed Image - ID: {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bbox(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    original_img = F.to_pil_image(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(original_img)\n",
    "\n",
    "    for box in target[\"boxes\"]:  # Access the boxes directly\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        # Debug print\n",
    "        print(f\"Visualizing BBox - xmin: {x_min}, ymin: {y_min}, xmax: {x_max}, ymax: {y_max}\")\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.title(f\"Image with Bounding Boxes - ID: {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_utils.py\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import utils\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    categories = data['categories']\n",
    "    classes = {category['id']: category['name'] for category in categories}\n",
    "    return classes\n",
    "\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "#classes = load_classes_from_json('basketball_child-6/test/_annotations.coco.json')\n",
    "#print(classes)\n",
    "\n",
    "# model_utils.py\n",
    "def get_model_instance_segmentation(num_classes, hidden_layer=256):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_to_huggingface(model_directory, model_id):\n",
    "    \"\"\"Upload the model to Hugging Face Hub.\"\"\"\n",
    "    hf_api = HfApi()\n",
    "    username = hf_api.whoami()['name']\n",
    "    repo_name = f\"{username}/{model_id}\"\n",
    "    repo_url = hf_api.create_repo(repo_name, exist_ok=True, private=False)\n",
    "\n",
    "    repo = Repository(local_dir=model_directory, clone_from=repo_url, use_auth_token=True)\n",
    "    repo.lfs_track([\"*.bin\", \"*.pth\", \"*.ckpt\"])  # Track large model files with Git LFS\n",
    "    repo.git_add()\n",
    "    repo.git_commit(\"Initial commit of the model\")\n",
    "    try:\n",
    "        repo.git_push()\n",
    "        print(f\"Model successfully uploaded to: {repo_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload model to Hugging Face: {e}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/transforms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/transforms.py\n",
    "import torch  # Add this import statement\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, ConvertImageDtype\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    #if train:\n",
    "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def transform_image(image_bytes):\n",
    "    \"\"\"\n",
    "    Transforms image bytes into a tensor with the correct format for the model.\n",
    "    \n",
    "    Args:\n",
    "    image_bytes (bytes): The image in bytes format, as uploaded by the user.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The transformed image as a tensor.\n",
    "    \"\"\"\n",
    "    # Define the transformations\n",
    "    my_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to the size required by your model\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard normalization for pre-trained models\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load the image from bytes and apply transformations\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    return my_transforms(image).unsqueeze(0)  # Add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "# train.py\n",
    "import torch\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "\n",
    "def train_model(model, data_loader, data_loader_valid, device, num_epochs,\n",
    "                lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_valid, device=device)\n",
    "\n",
    "    #torch.save(model.state_dict(), 'results/models/model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/process_video_check.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/process_video_check.py\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "def intersects(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
    "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
    "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
    "\n",
    "def process_video_check(video_path, model, device, classes, classes_to_track=None, threshold=0.5, check_intersections=False):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    score = 0  # Initialize the score counter\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_tensor = T.ToTensor()(frame).unsqueeze_(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)[0]\n",
    "\n",
    "        pred_scores = prediction['scores']\n",
    "        pred_boxes = prediction['boxes']\n",
    "        pred_labels = prediction['labels']\n",
    "        pred_masks = prediction['masks']\n",
    "\n",
    "        keep = pred_scores > threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        pred_masks = pred_masks[keep]\n",
    "\n",
    "        #print(f\"Original Frame Size: {frame.shape}\")\n",
    "        #print(f\"Pred Boxes before drawing: {pred_boxes}\")\n",
    "\n",
    "        if not keep.any():\n",
    "            continue  # Skip this frame if no detections are kept\n",
    "\n",
    "        # Convert numeric labels to class names\n",
    "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
    "\n",
    "        if check_intersections == True and classes_to_track:\n",
    "            # Perform intersection checks only if enabled and classes_to_track is specified\n",
    "            for class_pair in classes_to_track:\n",
    "                class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
    "                class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
    "\n",
    "                for box1 in class1_boxes:\n",
    "                    for box2 in class2_boxes:\n",
    "                        if intersects(box1, box2):\n",
    "                            score += 1\n",
    "                            print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score)\n",
    "\n",
    "        # Frame Tensor Conversion for Drawing\n",
    "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
    "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
    "\n",
    "        # Draw bounding boxes and segmentation masks\n",
    "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
    "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
    "\n",
    "        # Convert output image for displaying\n",
    "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        output_image = np.clip(output_image, 0, 255)  # Ensure values are within 0-255\n",
    "        if check_intersections == True and classes_to_track:\n",
    "            # Draw score text\n",
    "            cv2.putText(output_image, f'Score: {score}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Frame', output_image)\n",
    "        \n",
    "        # Just before cv2.imshow\n",
    "        #print(f\"Output Image Size: {output_image.shape}\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# process_video_check(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last todos:\n",
    "* add in more descriptive args like roboflow_api and youtube_video_url\n",
    "* add in args.parse for the classes to check for intersection\n",
    "* add in huggingface save and reload option so we don't save the model here\n",
    "* add streamlit for this so it can be visualized by anyone\n",
    "* update google collab version the same^ but also so that it can work as they walk into it, so just the clones and then train.py\n",
    "*move to different repo and name instanceseg_track_roboflow or something more rounded than bball_instanceseg (so people don't see your api in previous versions)\n",
    "*^clean up to only the bare essentials\n",
    "*add readme and docstrings to all code to make easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from going_modular.utils import (get_device, create_directory \n",
    "                                ,get_project, download_videos_from_youtube\n",
    "                                , delete_folder_and_video, load_classes_from_json\n",
    "                                ,split_dataset)\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import (get_model_instance_segmentation\n",
    "                                        , upload_to_huggingface)\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.transforms import get_transform\n",
    "from going_modular.process_video_check import process_video_check\n",
    "import utils\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    data_path = Path(args.data_path)\n",
    "    model_path = Path(args.model_path)\n",
    "    create_directory(data_path)\n",
    "    create_directory(model_path)\n",
    "\n",
    "    # Check if the project data is already downloaded\n",
    "    project_folder = Path(f'{args.project_folder_name}-{args.version}')\n",
    "    classes_path = project_folder / 'train' / '_annotations.coco.json'\n",
    "\n",
    "    if not project_folder.exists() or not classes_path.exists():\n",
    "        print(\"Downloading project data...\")\n",
    "        get_project(args.api_key, args.workspace, args.project_name, args.version)\n",
    "\n",
    "    # Load classes from JSON\n",
    "    classes = load_classes_from_json(classes_path)\n",
    "    print(\"Classes loaded:\", classes)\n",
    "\n",
    "    num_classes = len(classes) + 1\n",
    "    device = get_device()\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=args.hidden_layer)\n",
    "    model.to(device)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        video_filename = args.video_name if args.video_name.endswith('.mp4') else f\"{args.video_name}.mp4\"\n",
    "        video_path = Path(args.data_path) / video_filename\n",
    "        # Load datasets\n",
    "        datasets = {}\n",
    "        data_loaders = {}\n",
    "        for dtype in ['train', 'valid', 'test']:\n",
    "            ann_path = project_folder / dtype / '_annotations.coco.json'\n",
    "            img_dir = project_folder / dtype\n",
    "            if ann_path.exists() and img_dir.exists():\n",
    "                datasets[dtype] = CustomCocoDataset(str(ann_path), str(img_dir), transforms=get_transform(train=dtype=='train'))\n",
    "                batch_size = 2 if dtype == 'train' else 1\n",
    "                data_loaders[dtype] = DataLoader(datasets[dtype], batch_size=batch_size, shuffle=dtype=='train', num_workers=0, collate_fn=utils.collate_fn)\n",
    "                print(f\"{dtype.capitalize()} dataset loaded.\")\n",
    "            else:\n",
    "                print(f\"{dtype.capitalize()} dataset not found or incomplete. Skipping.\")\n",
    "\n",
    "        if 'train' in datasets and 'valid' not in datasets:\n",
    "            print(\"Splitting dataset into train and valid...\")\n",
    "            train_dataset, valid_dataset = split_dataset(datasets['train'])\n",
    "            data_loaders['train'] = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "            data_loaders['valid'] = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "\n",
    "        if 'train' in data_loaders and 'valid' in data_loaders:\n",
    "            print(\"Starting training process...\")\n",
    "            train_model(model, data_loaders['train'], data_loaders.get('valid'), device, args.num_epochs, lr=args.lr)\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(f\"Model saved at: {model_file_path}\")\n",
    "        else:\n",
    "            print(\"Training failed. No valid data loaders available.\")\n",
    "            \n",
    "        if args.delete_folder_and_video == True:\n",
    "            delete_folder_and_video(project_folder, video_path)\n",
    "\n",
    "    # Process video if mode is 'process_video'\n",
    "    elif args.mode == 'process_video':\n",
    "        video_filename = args.video_name if args.video_name.endswith('.mp4') else f\"{args.video_name}.mp4\"\n",
    "        video_path = Path(args.data_path) / video_filename\n",
    "        if not video_path.exists():\n",
    "            print(\"Downloading video...\")\n",
    "            download_videos_from_youtube([args.video_url], str(Path(args.data_path)))\n",
    "\n",
    "        if video_path.exists():\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            if model_file_path.exists():\n",
    "                model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
    "                classes_to_track = list(zip(args.classes_to_track[::2], args.classes_to_track[1::2]))  # Convert flat list to list of tuples\n",
    "                process_video_check(video_path, model, device, classes, classes_to_track, args.threshold, args.check_intersections)\n",
    "            else:\n",
    "                print(\"Model weights file not found. Please train the model first.\")\n",
    "        if args.delete_folder_and_video == True:\n",
    "            delete_folder_and_video(project_folder, video_path)\n",
    "            \n",
    "    elif args.mode == 'hf_upload':\n",
    "        hf_login()  # Ensure user is logged in\n",
    "\n",
    "        # Automatically determine model directory (or you can still ask the user)\n",
    "        model_directory = args.model_path  # Assuming this is where your model is saved\n",
    "        if not Path(model_directory).exists():\n",
    "            print(f\"Model directory {model_directory} does not exist. Please specify a valid model directory.\")\n",
    "            return\n",
    "\n",
    "        model_id = input(\"Enter a name for your model on Hugging Face (e.g., my-cool-model): \")\n",
    "        try:\n",
    "            upload_to_huggingface(model_directory, model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during model upload: {e}\")\n",
    "            \n",
    "        if args.delete_folder_and_video == True:\n",
    "            delete_folder_and_video(project_folder, video_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a model for object detection or process video\")\n",
    "    parser.add_argument('--api_key', type=str, default=\"your_roboflow_api_key\", help='API key for Roboflow')\n",
    "    parser.add_argument('--workspace', type=str, default=\"your_roboflow_workspace\", help='Workspace name in Roboflow')\n",
    "    parser.add_argument('--project_name', type=str, default=\"your_roboflow_project\", help='Project name in Roboflow')\n",
    "    parser.add_argument('--project_folder_name', type=str, default=\"your_roboflow_project_folder_name\", help='Project folder name in Roboflow')\n",
    "    parser.add_argument('--version', type=int, default=1, help='Version of the dataset in Roboflow')\n",
    "    parser.add_argument('--hidden_layer', type=int, default=256, help='Hidden layer size for the MaskRCNN predictor')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs to train the model')\n",
    "    parser.add_argument('--video_url', type=str, default=\"https://www.youtube.com/watch?v=example_video_id\", help='URL of the video to process')\n",
    "    parser.add_argument('--video_name', type=str, default=\"your_youtube_video_name\", help='Name of the video file (with extension) to process')\n",
    "    parser.add_argument('--threshold', type=float, default=0.6, help='Detection threshold for process_video')\n",
    "    parser.add_argument('--display_video', type=bool, default=True, help='Whether to display the video during processing')\n",
    "    parser.add_argument('--data_path', type=str, default='results/data', help='Path to save downloaded data')\n",
    "    parser.add_argument('--model_path', type=str, default='results/models', help='Path to save model weights')\n",
    "    parser.add_argument('--mode', type=str, choices=['train', 'process_video', 'hf_upload'], default='train', help='Mode of operation: train or process_video')\n",
    "    parser.add_argument('--delete_folder_and_video', type=bool, default=False, help='Whether to delete the image folder and download video after use')\n",
    "    parser.add_argument('--check_intersections', type=bool, default=False , help='Enable intersection checks in video processing')\n",
    "    parser.add_argument('--classes_to_track', nargs='+', help='Classes to track for intersections, specified as pairs', default=[])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add in a delete for the existing image dataset and youtube video's\n",
    "add in hugging face upload if they like the model and name of model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes loaded: {0: 'Basketball-and-Hoop', 1: 'Backboard', 2: 'Basketball', 3: 'Hoop', 4: 'Player'}\n",
      "Train dataset loaded.\n",
      "Valid dataset loaded.\n",
      "Test dataset loaded.\n",
      "Starting training process...\n",
      "Epoch: [0]  [  0/975]  eta: 0:16:39  lr: 0.000010  loss: 3.7534 (3.7534)  loss_classifier: 1.9249 (1.9249)  loss_box_reg: 0.3781 (0.3781)  loss_mask: 1.3841 (1.3841)  loss_objectness: 0.0492 (0.0492)  loss_rpn_box_reg: 0.0171 (0.0171)  time: 1.0249  data: 0.0365  max mem: 1663\n",
      "Epoch: [0]  [ 10/975]  eta: 0:03:09  lr: 0.000061  loss: 3.9083 (4.0019)  loss_classifier: 1.8273 (1.8263)  loss_box_reg: 0.3022 (0.3513)  loss_mask: 1.3314 (1.4186)  loss_objectness: 0.2002 (0.3088)  loss_rpn_box_reg: 0.0764 (0.0969)  time: 0.1965  data: 0.0208  max mem: 1965\n",
      "Epoch: [0]  [ 20/975]  eta: 0:02:26  lr: 0.000113  loss: 3.2336 (3.3274)  loss_classifier: 1.5392 (1.5187)  loss_box_reg: 0.2926 (0.3293)  loss_mask: 1.0291 (1.1925)  loss_objectness: 0.1022 (0.2095)  loss_rpn_box_reg: 0.0691 (0.0775)  time: 0.1101  data: 0.0183  max mem: 2110\n",
      "Epoch: [0]  [ 30/975]  eta: 0:02:11  lr: 0.000164  loss: 2.0859 (2.8419)  loss_classifier: 0.6816 (1.2020)  loss_box_reg: 0.2921 (0.3252)  loss_mask: 0.7914 (1.0243)  loss_objectness: 0.1022 (0.2013)  loss_rpn_box_reg: 0.0531 (0.0891)  time: 0.1076  data: 0.0173  max mem: 2110\n",
      "Epoch: [0]  [ 40/975]  eta: 0:02:02  lr: 0.000215  loss: 1.5874 (2.5130)  loss_classifier: 0.4685 (1.0178)  loss_box_reg: 0.2953 (0.3333)  loss_mask: 0.6066 (0.9073)  loss_objectness: 0.1092 (0.1753)  loss_rpn_box_reg: 0.0531 (0.0793)  time: 0.1079  data: 0.0173  max mem: 2114\n",
      "Epoch: [0]  [ 50/975]  eta: 0:01:57  lr: 0.000267  loss: 1.4132 (2.2951)  loss_classifier: 0.4685 (0.9044)  loss_box_reg: 0.3920 (0.3452)  loss_mask: 0.4378 (0.8186)  loss_objectness: 0.0630 (0.1506)  loss_rpn_box_reg: 0.0354 (0.0764)  time: 0.1088  data: 0.0169  max mem: 2114\n",
      "Epoch: [0]  [ 60/975]  eta: 0:01:53  lr: 0.000318  loss: 1.3149 (2.1382)  loss_classifier: 0.3790 (0.8216)  loss_box_reg: 0.3598 (0.3507)  loss_mask: 0.3901 (0.7462)  loss_objectness: 0.0557 (0.1442)  loss_rpn_box_reg: 0.0439 (0.0755)  time: 0.1082  data: 0.0171  max mem: 2114\n",
      "Epoch: [0]  [ 70/975]  eta: 0:01:50  lr: 0.000369  loss: 1.3628 (2.0509)  loss_classifier: 0.3790 (0.7619)  loss_box_reg: 0.4072 (0.3659)  loss_mask: 0.3653 (0.6930)  loss_objectness: 0.1246 (0.1452)  loss_rpn_box_reg: 0.0704 (0.0848)  time: 0.1082  data: 0.0179  max mem: 2142\n",
      "Epoch: [0]  [ 80/975]  eta: 0:01:47  lr: 0.000420  loss: 1.3812 (1.9598)  loss_classifier: 0.3664 (0.7066)  loss_box_reg: 0.4241 (0.3648)  loss_mask: 0.3549 (0.6528)  loss_objectness: 0.1084 (0.1411)  loss_rpn_box_reg: 0.0895 (0.0946)  time: 0.1071  data: 0.0177  max mem: 2157\n",
      "Epoch: [0]  [ 90/975]  eta: 0:01:44  lr: 0.000472  loss: 1.2028 (1.8713)  loss_classifier: 0.2982 (0.6606)  loss_box_reg: 0.3336 (0.3645)  loss_mask: 0.3353 (0.6171)  loss_objectness: 0.0962 (0.1385)  loss_rpn_box_reg: 0.0551 (0.0906)  time: 0.1050  data: 0.0171  max mem: 2157\n",
      "Epoch: [0]  [100/975]  eta: 0:01:42  lr: 0.000523  loss: 1.0947 (1.8084)  loss_classifier: 0.2855 (0.6273)  loss_box_reg: 0.3801 (0.3703)  loss_mask: 0.3231 (0.5874)  loss_objectness: 0.0797 (0.1337)  loss_rpn_box_reg: 0.0551 (0.0898)  time: 0.1063  data: 0.0171  max mem: 2157\n",
      "Epoch: [0]  [110/975]  eta: 0:01:40  lr: 0.000574  loss: 1.2501 (1.7610)  loss_classifier: 0.3021 (0.5988)  loss_box_reg: 0.4307 (0.3785)  loss_mask: 0.2994 (0.5616)  loss_objectness: 0.0797 (0.1335)  loss_rpn_box_reg: 0.0739 (0.0887)  time: 0.1070  data: 0.0171  max mem: 2157\n",
      "Epoch: [0]  [120/975]  eta: 0:01:38  lr: 0.000626  loss: 1.1897 (1.7127)  loss_classifier: 0.2796 (0.5726)  loss_box_reg: 0.4088 (0.3816)  loss_mask: 0.2946 (0.5395)  loss_objectness: 0.0783 (0.1312)  loss_rpn_box_reg: 0.0575 (0.0879)  time: 0.1075  data: 0.0175  max mem: 2226\n",
      "Epoch: [0]  [130/975]  eta: 0:01:36  lr: 0.000677  loss: 0.9544 (1.6489)  loss_classifier: 0.2215 (0.5453)  loss_box_reg: 0.3266 (0.3729)  loss_mask: 0.2907 (0.5222)  loss_objectness: 0.0539 (0.1252)  loss_rpn_box_reg: 0.0306 (0.0833)  time: 0.1030  data: 0.0167  max mem: 2226\n",
      "Epoch: [0]  [140/975]  eta: 0:01:35  lr: 0.000728  loss: 0.9176 (1.6121)  loss_classifier: 0.2122 (0.5258)  loss_box_reg: 0.3197 (0.3765)  loss_mask: 0.2885 (0.5055)  loss_objectness: 0.0466 (0.1213)  loss_rpn_box_reg: 0.0285 (0.0830)  time: 0.1039  data: 0.0168  max mem: 2226\n",
      "Epoch: [0]  [150/975]  eta: 0:01:33  lr: 0.000779  loss: 0.9176 (1.5659)  loss_classifier: 0.2011 (0.5045)  loss_box_reg: 0.3299 (0.3701)  loss_mask: 0.2934 (0.4933)  loss_objectness: 0.0495 (0.1164)  loss_rpn_box_reg: 0.0526 (0.0816)  time: 0.1045  data: 0.0172  max mem: 2226\n",
      "Epoch: [0]  [160/975]  eta: 0:01:31  lr: 0.000831  loss: 0.9080 (1.5346)  loss_classifier: 0.2011 (0.4881)  loss_box_reg: 0.3268 (0.3693)  loss_mask: 0.3207 (0.4813)  loss_objectness: 0.0463 (0.1130)  loss_rpn_box_reg: 0.0625 (0.0828)  time: 0.1019  data: 0.0163  max mem: 2226\n",
      "Epoch: [0]  [170/975]  eta: 0:01:29  lr: 0.000882  loss: 0.8865 (1.4923)  loss_classifier: 0.1863 (0.4696)  loss_box_reg: 0.3106 (0.3636)  loss_mask: 0.2944 (0.4710)  loss_objectness: 0.0430 (0.1083)  loss_rpn_box_reg: 0.0524 (0.0799)  time: 0.1024  data: 0.0161  max mem: 2226\n",
      "Epoch: [0]  [180/975]  eta: 0:01:28  lr: 0.000933  loss: 0.9479 (1.4748)  loss_classifier: 0.1798 (0.4568)  loss_box_reg: 0.3310 (0.3656)  loss_mask: 0.3139 (0.4622)  loss_objectness: 0.0325 (0.1065)  loss_rpn_box_reg: 0.0524 (0.0838)  time: 0.1026  data: 0.0168  max mem: 2226\n",
      "Epoch: [0]  [190/975]  eta: 0:01:27  lr: 0.000985  loss: 1.0486 (1.4535)  loss_classifier: 0.2349 (0.4449)  loss_box_reg: 0.3580 (0.3649)  loss_mask: 0.3036 (0.4541)  loss_objectness: 0.0504 (0.1044)  loss_rpn_box_reg: 0.0869 (0.0851)  time: 0.1048  data: 0.0175  max mem: 2226\n",
      "Epoch: [0]  [200/975]  eta: 0:01:25  lr: 0.001036  loss: 0.9757 (1.4320)  loss_classifier: 0.2098 (0.4339)  loss_box_reg: 0.3647 (0.3654)  loss_mask: 0.2961 (0.4459)  loss_objectness: 0.0338 (0.1020)  loss_rpn_box_reg: 0.0481 (0.0848)  time: 0.1054  data: 0.0173  max mem: 2226\n",
      "Epoch: [0]  [210/975]  eta: 0:01:24  lr: 0.001087  loss: 0.9615 (1.4058)  loss_classifier: 0.1908 (0.4220)  loss_box_reg: 0.3684 (0.3624)  loss_mask: 0.2746 (0.4379)  loss_objectness: 0.0272 (0.0987)  loss_rpn_box_reg: 0.0340 (0.0848)  time: 0.1038  data: 0.0172  max mem: 2226\n",
      "Epoch: [0]  [220/975]  eta: 0:01:23  lr: 0.001138  loss: 0.8912 (1.3851)  loss_classifier: 0.1675 (0.4116)  loss_box_reg: 0.2483 (0.3589)  loss_mask: 0.2755 (0.4314)  loss_objectness: 0.0422 (0.0990)  loss_rpn_box_reg: 0.0373 (0.0843)  time: 0.1040  data: 0.0175  max mem: 2226\n",
      "Epoch: [0]  [230/975]  eta: 0:01:21  lr: 0.001190  loss: 0.9481 (1.3652)  loss_classifier: 0.1766 (0.4017)  loss_box_reg: 0.3030 (0.3562)  loss_mask: 0.2939 (0.4258)  loss_objectness: 0.0568 (0.0969)  loss_rpn_box_reg: 0.0493 (0.0846)  time: 0.1045  data: 0.0183  max mem: 2226\n",
      "Epoch: [0]  [240/975]  eta: 0:01:20  lr: 0.001241  loss: 0.9160 (1.3452)  loss_classifier: 0.1700 (0.3912)  loss_box_reg: 0.2927 (0.3521)  loss_mask: 0.2600 (0.4190)  loss_objectness: 0.0459 (0.0971)  loss_rpn_box_reg: 0.0643 (0.0859)  time: 0.1045  data: 0.0184  max mem: 2226\n",
      "Epoch: [0]  [250/975]  eta: 0:01:19  lr: 0.001292  loss: 0.7486 (1.3228)  loss_classifier: 0.1331 (0.3818)  loss_box_reg: 0.2324 (0.3477)  loss_mask: 0.2529 (0.4127)  loss_objectness: 0.0672 (0.0959)  loss_rpn_box_reg: 0.0544 (0.0848)  time: 0.1051  data: 0.0183  max mem: 2226\n",
      "Epoch: [0]  [260/975]  eta: 0:01:17  lr: 0.001343  loss: 0.7414 (1.3011)  loss_classifier: 0.1453 (0.3728)  loss_box_reg: 0.2280 (0.3437)  loss_mask: 0.2529 (0.4064)  loss_objectness: 0.0532 (0.0949)  loss_rpn_box_reg: 0.0416 (0.0833)  time: 0.1012  data: 0.0173  max mem: 2226\n",
      "Epoch: [0]  [270/975]  eta: 0:01:16  lr: 0.001395  loss: 0.7398 (1.2806)  loss_classifier: 0.1453 (0.3649)  loss_box_reg: 0.2280 (0.3393)  loss_mask: 0.2707 (0.4022)  loss_objectness: 0.0413 (0.0927)  loss_rpn_box_reg: 0.0358 (0.0816)  time: 0.0977  data: 0.0164  max mem: 2226\n",
      "Epoch: [0]  [280/975]  eta: 0:01:15  lr: 0.001446  loss: 0.8371 (1.2676)  loss_classifier: 0.1674 (0.3587)  loss_box_reg: 0.2688 (0.3383)  loss_mask: 0.2868 (0.3990)  loss_objectness: 0.0365 (0.0910)  loss_rpn_box_reg: 0.0479 (0.0807)  time: 0.0987  data: 0.0168  max mem: 2226\n",
      "Epoch: [0]  [290/975]  eta: 0:01:13  lr: 0.001497  loss: 0.8371 (1.2493)  loss_classifier: 0.1674 (0.3518)  loss_box_reg: 0.2719 (0.3350)  loss_mask: 0.2595 (0.3944)  loss_objectness: 0.0313 (0.0890)  loss_rpn_box_reg: 0.0468 (0.0791)  time: 0.1006  data: 0.0172  max mem: 2226\n",
      "Epoch: [0]  [300/975]  eta: 0:01:12  lr: 0.001549  loss: 0.7527 (1.2367)  loss_classifier: 0.1335 (0.3457)  loss_box_reg: 0.2564 (0.3325)  loss_mask: 0.2756 (0.3914)  loss_objectness: 0.0262 (0.0881)  loss_rpn_box_reg: 0.0468 (0.0790)  time: 0.1009  data: 0.0172  max mem: 2226\n",
      "Epoch: [0]  [310/975]  eta: 0:01:11  lr: 0.001600  loss: 0.7860 (1.2216)  loss_classifier: 0.1444 (0.3395)  loss_box_reg: 0.2687 (0.3303)  loss_mask: 0.2749 (0.3868)  loss_objectness: 0.0338 (0.0868)  loss_rpn_box_reg: 0.0350 (0.0782)  time: 0.1037  data: 0.0173  max mem: 2226\n",
      "Epoch: [0]  [320/975]  eta: 0:01:10  lr: 0.001651  loss: 0.7384 (1.2069)  loss_classifier: 0.1295 (0.3336)  loss_box_reg: 0.2411 (0.3274)  loss_mask: 0.2533 (0.3834)  loss_objectness: 0.0206 (0.0854)  loss_rpn_box_reg: 0.0324 (0.0771)  time: 0.1047  data: 0.0167  max mem: 2226\n",
      "Epoch: [0]  [330/975]  eta: 0:01:09  lr: 0.001702  loss: 0.7670 (1.1935)  loss_classifier: 0.1497 (0.3284)  loss_box_reg: 0.2577 (0.3257)  loss_mask: 0.2571 (0.3795)  loss_objectness: 0.0257 (0.0839)  loss_rpn_box_reg: 0.0421 (0.0761)  time: 0.1022  data: 0.0163  max mem: 2226\n",
      "Epoch: [0]  [340/975]  eta: 0:01:08  lr: 0.001754  loss: 0.7568 (1.1797)  loss_classifier: 0.1446 (0.3229)  loss_box_reg: 0.2700 (0.3228)  loss_mask: 0.2493 (0.3761)  loss_objectness: 0.0318 (0.0826)  loss_rpn_box_reg: 0.0331 (0.0753)  time: 0.1021  data: 0.0169  max mem: 2226\n",
      "Epoch: [0]  [350/975]  eta: 0:01:06  lr: 0.001805  loss: 0.7977 (1.1723)  loss_classifier: 0.1418 (0.3184)  loss_box_reg: 0.2577 (0.3213)  loss_mask: 0.2735 (0.3736)  loss_objectness: 0.0433 (0.0824)  loss_rpn_box_reg: 0.0431 (0.0767)  time: 0.1028  data: 0.0169  max mem: 2226\n",
      "Epoch: [0]  [360/975]  eta: 0:01:05  lr: 0.001856  loss: 0.6754 (1.1571)  loss_classifier: 0.1084 (0.3128)  loss_box_reg: 0.1824 (0.3177)  loss_mask: 0.2580 (0.3700)  loss_objectness: 0.0364 (0.0811)  loss_rpn_box_reg: 0.0274 (0.0754)  time: 0.1026  data: 0.0164  max mem: 2226\n",
      "Epoch: [0]  [370/975]  eta: 0:01:04  lr: 0.001908  loss: 0.6496 (1.1464)  loss_classifier: 0.1161 (0.3082)  loss_box_reg: 0.1824 (0.3161)  loss_mask: 0.2580 (0.3674)  loss_objectness: 0.0291 (0.0801)  loss_rpn_box_reg: 0.0181 (0.0746)  time: 0.1021  data: 0.0164  max mem: 2226\n",
      "Epoch: [0]  [380/975]  eta: 0:01:03  lr: 0.001959  loss: 0.6948 (1.1362)  loss_classifier: 0.1388 (0.3038)  loss_box_reg: 0.2257 (0.3136)  loss_mask: 0.2658 (0.3650)  loss_objectness: 0.0232 (0.0790)  loss_rpn_box_reg: 0.0380 (0.0748)  time: 0.1040  data: 0.0167  max mem: 2226\n",
      "Epoch: [0]  [390/975]  eta: 0:01:02  lr: 0.002010  loss: 0.6890 (1.1240)  loss_classifier: 0.1169 (0.2989)  loss_box_reg: 0.2257 (0.3115)  loss_mask: 0.2665 (0.3622)  loss_objectness: 0.0245 (0.0775)  loss_rpn_box_reg: 0.0262 (0.0740)  time: 0.1039  data: 0.0168  max mem: 2226\n",
      "Epoch: [0]  [400/975]  eta: 0:01:01  lr: 0.002061  loss: 0.5856 (1.1105)  loss_classifier: 0.0915 (0.2938)  loss_box_reg: 0.2097 (0.3088)  loss_mask: 0.2472 (0.3591)  loss_objectness: 0.0165 (0.0759)  loss_rpn_box_reg: 0.0163 (0.0729)  time: 0.1020  data: 0.0164  max mem: 2226\n",
      "Epoch: [0]  [410/975]  eta: 0:01:00  lr: 0.002113  loss: 0.5821 (1.0996)  loss_classifier: 0.1018 (0.2898)  loss_box_reg: 0.1927 (0.3062)  loss_mask: 0.2226 (0.3560)  loss_objectness: 0.0165 (0.0753)  loss_rpn_box_reg: 0.0187 (0.0722)  time: 0.1035  data: 0.0162  max mem: 2226\n",
      "Epoch: [0]  [420/975]  eta: 0:00:59  lr: 0.002164  loss: 0.6915 (1.0914)  loss_classifier: 0.1402 (0.2867)  loss_box_reg: 0.2285 (0.3052)  loss_mask: 0.2386 (0.3534)  loss_objectness: 0.0222 (0.0744)  loss_rpn_box_reg: 0.0421 (0.0717)  time: 0.1056  data: 0.0169  max mem: 2226\n",
      "Epoch: [0]  [430/975]  eta: 0:00:57  lr: 0.002215  loss: 0.7220 (1.0829)  loss_classifier: 0.1561 (0.2830)  loss_box_reg: 0.2420 (0.3029)  loss_mask: 0.2552 (0.3515)  loss_objectness: 0.0218 (0.0743)  loss_rpn_box_reg: 0.0437 (0.0711)  time: 0.1031  data: 0.0162  max mem: 2226\n",
      "Epoch: [0]  [440/975]  eta: 0:00:56  lr: 0.002267  loss: 0.6447 (1.0727)  loss_classifier: 0.1092 (0.2791)  loss_box_reg: 0.1962 (0.3008)  loss_mask: 0.2505 (0.3490)  loss_objectness: 0.0257 (0.0733)  loss_rpn_box_reg: 0.0357 (0.0705)  time: 0.0996  data: 0.0157  max mem: 2226\n",
      "Epoch: [0]  [450/975]  eta: 0:00:55  lr: 0.002318  loss: 0.6299 (1.0643)  loss_classifier: 0.1021 (0.2758)  loss_box_reg: 0.1975 (0.2986)  loss_mask: 0.2488 (0.3468)  loss_objectness: 0.0265 (0.0730)  loss_rpn_box_reg: 0.0357 (0.0700)  time: 0.1005  data: 0.0164  max mem: 2226\n",
      "Epoch: [0]  [460/975]  eta: 0:00:54  lr: 0.002369  loss: 0.7428 (1.0611)  loss_classifier: 0.1443 (0.2735)  loss_box_reg: 0.2369 (0.2982)  loss_mask: 0.2488 (0.3449)  loss_objectness: 0.0395 (0.0728)  loss_rpn_box_reg: 0.0605 (0.0718)  time: 0.1037  data: 0.0172  max mem: 2226\n",
      "Epoch: [0]  [470/975]  eta: 0:00:53  lr: 0.002420  loss: 0.7909 (1.0517)  loss_classifier: 0.1330 (0.2701)  loss_box_reg: 0.2411 (0.2953)  loss_mask: 0.2557 (0.3432)  loss_objectness: 0.0409 (0.0721)  loss_rpn_box_reg: 0.0605 (0.0710)  time: 0.1038  data: 0.0171  max mem: 2226\n",
      "Epoch: [0]  [480/975]  eta: 0:00:52  lr: 0.002472  loss: 0.6610 (1.0456)  loss_classifier: 0.1231 (0.2677)  loss_box_reg: 0.1988 (0.2941)  loss_mask: 0.2564 (0.3417)  loss_objectness: 0.0263 (0.0714)  loss_rpn_box_reg: 0.0332 (0.0707)  time: 0.1026  data: 0.0165  max mem: 2226\n",
      "Epoch: [0]  [490/975]  eta: 0:00:51  lr: 0.002523  loss: 0.6771 (1.0391)  loss_classifier: 0.1286 (0.2654)  loss_box_reg: 0.2306 (0.2931)  loss_mask: 0.2521 (0.3399)  loss_objectness: 0.0263 (0.0708)  loss_rpn_box_reg: 0.0332 (0.0699)  time: 0.1025  data: 0.0168  max mem: 2226\n",
      "Epoch: [0]  [500/975]  eta: 0:00:50  lr: 0.002574  loss: 0.7014 (1.0345)  loss_classifier: 0.1558 (0.2634)  loss_box_reg: 0.2441 (0.2927)  loss_mask: 0.2363 (0.3380)  loss_objectness: 0.0350 (0.0706)  loss_rpn_box_reg: 0.0363 (0.0698)  time: 0.1053  data: 0.0172  max mem: 2231\n",
      "Epoch: [0]  [510/975]  eta: 0:00:49  lr: 0.002626  loss: 0.7014 (1.0281)  loss_classifier: 0.1297 (0.2610)  loss_box_reg: 0.2441 (0.2919)  loss_mask: 0.2423 (0.3362)  loss_objectness: 0.0345 (0.0698)  loss_rpn_box_reg: 0.0363 (0.0692)  time: 0.1063  data: 0.0168  max mem: 2231\n",
      "Epoch: [0]  [520/975]  eta: 0:00:48  lr: 0.002677  loss: 0.6165 (1.0213)  loss_classifier: 0.1212 (0.2587)  loss_box_reg: 0.1963 (0.2903)  loss_mask: 0.2471 (0.3347)  loss_objectness: 0.0178 (0.0691)  loss_rpn_box_reg: 0.0340 (0.0686)  time: 0.1042  data: 0.0165  max mem: 2231\n",
      "Epoch: [0]  [530/975]  eta: 0:00:47  lr: 0.002728  loss: 0.7669 (1.0181)  loss_classifier: 0.1446 (0.2569)  loss_box_reg: 0.2249 (0.2898)  loss_mask: 0.2541 (0.3332)  loss_objectness: 0.0373 (0.0692)  loss_rpn_box_reg: 0.0544 (0.0691)  time: 0.1062  data: 0.0169  max mem: 2231\n",
      "Epoch: [0]  [540/975]  eta: 0:00:46  lr: 0.002779  loss: 0.7420 (1.0119)  loss_classifier: 0.1369 (0.2545)  loss_box_reg: 0.2298 (0.2884)  loss_mask: 0.2441 (0.3317)  loss_objectness: 0.0401 (0.0687)  loss_rpn_box_reg: 0.0544 (0.0686)  time: 0.1041  data: 0.0167  max mem: 2231\n",
      "Epoch: [0]  [550/975]  eta: 0:00:44  lr: 0.002831  loss: 0.6987 (1.0058)  loss_classifier: 0.1340 (0.2521)  loss_box_reg: 0.2231 (0.2872)  loss_mask: 0.2380 (0.3300)  loss_objectness: 0.0399 (0.0682)  loss_rpn_box_reg: 0.0376 (0.0684)  time: 0.1016  data: 0.0166  max mem: 2231\n",
      "Epoch: [0]  [560/975]  eta: 0:00:43  lr: 0.002882  loss: 0.6552 (1.0002)  loss_classifier: 0.1019 (0.2496)  loss_box_reg: 0.2048 (0.2855)  loss_mask: 0.2384 (0.3291)  loss_objectness: 0.0243 (0.0675)  loss_rpn_box_reg: 0.0409 (0.0685)  time: 0.1015  data: 0.0165  max mem: 2231\n",
      "Epoch: [0]  [570/975]  eta: 0:00:42  lr: 0.002933  loss: 0.6552 (0.9955)  loss_classifier: 0.1127 (0.2473)  loss_box_reg: 0.1743 (0.2842)  loss_mask: 0.2550 (0.3280)  loss_objectness: 0.0221 (0.0674)  loss_rpn_box_reg: 0.0409 (0.0685)  time: 0.0996  data: 0.0159  max mem: 2231\n",
      "Epoch: [0]  [580/975]  eta: 0:00:41  lr: 0.002985  loss: 0.6646 (0.9900)  loss_classifier: 0.1150 (0.2455)  loss_box_reg: 0.2124 (0.2828)  loss_mask: 0.2544 (0.3268)  loss_objectness: 0.0273 (0.0668)  loss_rpn_box_reg: 0.0353 (0.0681)  time: 0.0991  data: 0.0157  max mem: 2231\n",
      "Epoch: [0]  [590/975]  eta: 0:00:40  lr: 0.003036  loss: 0.7000 (0.9854)  loss_classifier: 0.1088 (0.2436)  loss_box_reg: 0.2124 (0.2817)  loss_mask: 0.2503 (0.3259)  loss_objectness: 0.0416 (0.0664)  loss_rpn_box_reg: 0.0348 (0.0678)  time: 0.1005  data: 0.0164  max mem: 2231\n",
      "Epoch: [0]  [600/975]  eta: 0:00:39  lr: 0.003087  loss: 0.7000 (0.9817)  loss_classifier: 0.1088 (0.2418)  loss_box_reg: 0.2129 (0.2809)  loss_mask: 0.2629 (0.3253)  loss_objectness: 0.0327 (0.0659)  loss_rpn_box_reg: 0.0408 (0.0678)  time: 0.1027  data: 0.0174  max mem: 2231\n",
      "Epoch: [0]  [610/975]  eta: 0:00:38  lr: 0.003138  loss: 0.5528 (0.9756)  loss_classifier: 0.1110 (0.2397)  loss_box_reg: 0.1660 (0.2791)  loss_mask: 0.2380 (0.3243)  loss_objectness: 0.0204 (0.0652)  loss_rpn_box_reg: 0.0390 (0.0672)  time: 0.1018  data: 0.0169  max mem: 2231\n",
      "Epoch: [0]  [620/975]  eta: 0:00:37  lr: 0.003190  loss: 0.6295 (0.9709)  loss_classifier: 0.1110 (0.2379)  loss_box_reg: 0.1494 (0.2778)  loss_mask: 0.2367 (0.3235)  loss_objectness: 0.0224 (0.0645)  loss_rpn_box_reg: 0.0243 (0.0670)  time: 0.0990  data: 0.0158  max mem: 2231\n",
      "Epoch: [0]  [630/975]  eta: 0:00:36  lr: 0.003241  loss: 0.6846 (0.9674)  loss_classifier: 0.1050 (0.2365)  loss_box_reg: 0.1774 (0.2771)  loss_mask: 0.2568 (0.3226)  loss_objectness: 0.0271 (0.0644)  loss_rpn_box_reg: 0.0376 (0.0669)  time: 0.0994  data: 0.0161  max mem: 2231\n",
      "Epoch: [0]  [640/975]  eta: 0:00:35  lr: 0.003292  loss: 0.6881 (0.9659)  loss_classifier: 0.1120 (0.2351)  loss_box_reg: 0.2055 (0.2767)  loss_mask: 0.2521 (0.3216)  loss_objectness: 0.0281 (0.0645)  loss_rpn_box_reg: 0.0436 (0.0681)  time: 0.1014  data: 0.0164  max mem: 2231\n",
      "Epoch: [0]  [650/975]  eta: 0:00:34  lr: 0.003344  loss: 0.6790 (0.9608)  loss_classifier: 0.1120 (0.2331)  loss_box_reg: 0.1896 (0.2751)  loss_mask: 0.2458 (0.3206)  loss_objectness: 0.0281 (0.0641)  loss_rpn_box_reg: 0.0396 (0.0680)  time: 0.1017  data: 0.0162  max mem: 2231\n",
      "Epoch: [0]  [660/975]  eta: 0:00:33  lr: 0.003395  loss: 0.6666 (0.9580)  loss_classifier: 0.1184 (0.2319)  loss_box_reg: 0.1892 (0.2745)  loss_mask: 0.2538 (0.3197)  loss_objectness: 0.0409 (0.0639)  loss_rpn_box_reg: 0.0553 (0.0680)  time: 0.0999  data: 0.0159  max mem: 2231\n",
      "Epoch: [0]  [670/975]  eta: 0:00:31  lr: 0.003446  loss: 0.6666 (0.9532)  loss_classifier: 0.1469 (0.2303)  loss_box_reg: 0.1880 (0.2733)  loss_mask: 0.2607 (0.3187)  loss_objectness: 0.0302 (0.0633)  loss_rpn_box_reg: 0.0442 (0.0674)  time: 0.0972  data: 0.0157  max mem: 2231\n",
      "Epoch: [0]  [680/975]  eta: 0:00:30  lr: 0.003497  loss: 0.5404 (0.9482)  loss_classifier: 0.1338 (0.2287)  loss_box_reg: 0.1494 (0.2718)  loss_mask: 0.2597 (0.3179)  loss_objectness: 0.0237 (0.0629)  loss_rpn_box_reg: 0.0177 (0.0670)  time: 0.0961  data: 0.0160  max mem: 2231\n",
      "Epoch: [0]  [690/975]  eta: 0:00:29  lr: 0.003549  loss: 0.5875 (0.9459)  loss_classifier: 0.1189 (0.2274)  loss_box_reg: 0.1643 (0.2704)  loss_mask: 0.2569 (0.3173)  loss_objectness: 0.0266 (0.0635)  loss_rpn_box_reg: 0.0298 (0.0673)  time: 0.1041  data: 0.0224  max mem: 2231\n",
      "Epoch: [0]  [700/975]  eta: 0:00:28  lr: 0.003600  loss: 0.6094 (0.9425)  loss_classifier: 0.1270 (0.2259)  loss_box_reg: 0.1968 (0.2696)  loss_mask: 0.2469 (0.3162)  loss_objectness: 0.0445 (0.0633)  loss_rpn_box_reg: 0.0444 (0.0675)  time: 0.1069  data: 0.0224  max mem: 2231\n",
      "Epoch: [0]  [710/975]  eta: 0:00:27  lr: 0.003651  loss: 0.5958 (0.9386)  loss_classifier: 0.1159 (0.2246)  loss_box_reg: 0.1784 (0.2686)  loss_mask: 0.2266 (0.3151)  loss_objectness: 0.0357 (0.0629)  loss_rpn_box_reg: 0.0263 (0.0673)  time: 0.1022  data: 0.0164  max mem: 2231\n",
      "Epoch: [0]  [720/975]  eta: 0:00:26  lr: 0.003703  loss: 0.5962 (0.9355)  loss_classifier: 0.1151 (0.2233)  loss_box_reg: 0.1820 (0.2682)  loss_mask: 0.2281 (0.3142)  loss_objectness: 0.0255 (0.0626)  loss_rpn_box_reg: 0.0319 (0.0672)  time: 0.1034  data: 0.0172  max mem: 2231\n",
      "Epoch: [0]  [730/975]  eta: 0:00:25  lr: 0.003754  loss: 0.6496 (0.9323)  loss_classifier: 0.1251 (0.2222)  loss_box_reg: 0.1940 (0.2673)  loss_mask: 0.2437 (0.3134)  loss_objectness: 0.0255 (0.0622)  loss_rpn_box_reg: 0.0319 (0.0672)  time: 0.1026  data: 0.0167  max mem: 2231\n",
      "Epoch: [0]  [740/975]  eta: 0:00:24  lr: 0.003805  loss: 0.7424 (0.9297)  loss_classifier: 0.1341 (0.2212)  loss_box_reg: 0.2305 (0.2669)  loss_mask: 0.2480 (0.3127)  loss_objectness: 0.0287 (0.0618)  loss_rpn_box_reg: 0.0414 (0.0670)  time: 0.1006  data: 0.0161  max mem: 2231\n",
      "Epoch: [0]  [750/975]  eta: 0:00:23  lr: 0.003856  loss: 0.7758 (0.9292)  loss_classifier: 0.1521 (0.2204)  loss_box_reg: 0.2305 (0.2665)  loss_mask: 0.2603 (0.3122)  loss_objectness: 0.0377 (0.0628)  loss_rpn_box_reg: 0.0469 (0.0673)  time: 0.1019  data: 0.0165  max mem: 2231\n",
      "Epoch: [0]  [760/975]  eta: 0:00:22  lr: 0.003908  loss: 0.7899 (0.9269)  loss_classifier: 0.1521 (0.2195)  loss_box_reg: 0.1985 (0.2661)  loss_mask: 0.2591 (0.3115)  loss_objectness: 0.0446 (0.0625)  loss_rpn_box_reg: 0.0565 (0.0673)  time: 0.1023  data: 0.0163  max mem: 2231\n",
      "Epoch: [0]  [770/975]  eta: 0:00:21  lr: 0.003959  loss: 0.7228 (0.9245)  loss_classifier: 0.1218 (0.2182)  loss_box_reg: 0.2017 (0.2655)  loss_mask: 0.2266 (0.3104)  loss_objectness: 0.0436 (0.0625)  loss_rpn_box_reg: 0.0601 (0.0679)  time: 0.1022  data: 0.0160  max mem: 2231\n",
      "Epoch: [0]  [780/975]  eta: 0:00:20  lr: 0.004010  loss: 0.6430 (0.9214)  loss_classifier: 0.0962 (0.2168)  loss_box_reg: 0.1865 (0.2642)  loss_mask: 0.2283 (0.3096)  loss_objectness: 0.0369 (0.0625)  loss_rpn_box_reg: 0.0601 (0.0683)  time: 0.1002  data: 0.0158  max mem: 2231\n",
      "Epoch: [0]  [790/975]  eta: 0:00:19  lr: 0.004062  loss: 0.6430 (0.9181)  loss_classifier: 0.1015 (0.2157)  loss_box_reg: 0.1891 (0.2633)  loss_mask: 0.2322 (0.3087)  loss_objectness: 0.0369 (0.0622)  loss_rpn_box_reg: 0.0489 (0.0681)  time: 0.0978  data: 0.0159  max mem: 2231\n",
      "Epoch: [0]  [800/975]  eta: 0:00:18  lr: 0.004113  loss: 0.6619 (0.9164)  loss_classifier: 0.1298 (0.2147)  loss_box_reg: 0.1949 (0.2625)  loss_mask: 0.2374 (0.3089)  loss_objectness: 0.0393 (0.0620)  loss_rpn_box_reg: 0.0454 (0.0683)  time: 0.0984  data: 0.0165  max mem: 2231\n",
      "Epoch: [0]  [810/975]  eta: 0:00:17  lr: 0.004164  loss: 0.6865 (0.9138)  loss_classifier: 0.1140 (0.2137)  loss_box_reg: 0.1888 (0.2618)  loss_mask: 0.2602 (0.3083)  loss_objectness: 0.0362 (0.0618)  loss_rpn_box_reg: 0.0293 (0.0683)  time: 0.0980  data: 0.0162  max mem: 2231\n",
      "Epoch: [0]  [820/975]  eta: 0:00:16  lr: 0.004215  loss: 0.6812 (0.9116)  loss_classifier: 0.1113 (0.2126)  loss_box_reg: 0.1802 (0.2612)  loss_mask: 0.2470 (0.3075)  loss_objectness: 0.0329 (0.0615)  loss_rpn_box_reg: 0.0422 (0.0688)  time: 0.1011  data: 0.0164  max mem: 2231\n",
      "Epoch: [0]  [830/975]  eta: 0:00:15  lr: 0.004267  loss: 0.6286 (0.9085)  loss_classifier: 0.1120 (0.2115)  loss_box_reg: 0.2048 (0.2606)  loss_mask: 0.2359 (0.3065)  loss_objectness: 0.0335 (0.0611)  loss_rpn_box_reg: 0.0796 (0.0687)  time: 0.1025  data: 0.0166  max mem: 2231\n",
      "Epoch: [0]  [840/975]  eta: 0:00:14  lr: 0.004318  loss: 0.6286 (0.9065)  loss_classifier: 0.1120 (0.2104)  loss_box_reg: 0.2048 (0.2600)  loss_mask: 0.2461 (0.3063)  loss_objectness: 0.0270 (0.0609)  loss_rpn_box_reg: 0.0570 (0.0690)  time: 0.1018  data: 0.0165  max mem: 2231\n",
      "Epoch: [0]  [850/975]  eta: 0:00:13  lr: 0.004369  loss: 0.6744 (0.9048)  loss_classifier: 0.1091 (0.2094)  loss_box_reg: 0.2146 (0.2598)  loss_mask: 0.2599 (0.3056)  loss_objectness: 0.0317 (0.0606)  loss_rpn_box_reg: 0.0592 (0.0694)  time: 0.1035  data: 0.0172  max mem: 2231\n",
      "Epoch: [0]  [860/975]  eta: 0:00:11  lr: 0.004420  loss: 0.6909 (0.9021)  loss_classifier: 0.1150 (0.2084)  loss_box_reg: 0.2178 (0.2592)  loss_mask: 0.2441 (0.3049)  loss_objectness: 0.0349 (0.0604)  loss_rpn_box_reg: 0.0537 (0.0693)  time: 0.1031  data: 0.0172  max mem: 2231\n",
      "Epoch: [0]  [870/975]  eta: 0:00:10  lr: 0.004472  loss: 0.6097 (0.8987)  loss_classifier: 0.0924 (0.2072)  loss_box_reg: 0.1957 (0.2581)  loss_mask: 0.2534 (0.3045)  loss_objectness: 0.0143 (0.0599)  loss_rpn_box_reg: 0.0282 (0.0690)  time: 0.1001  data: 0.0165  max mem: 2231\n",
      "Epoch: [0]  [880/975]  eta: 0:00:09  lr: 0.004523  loss: 0.5608 (0.8964)  loss_classifier: 0.1136 (0.2065)  loss_box_reg: 0.1574 (0.2574)  loss_mask: 0.2622 (0.3039)  loss_objectness: 0.0107 (0.0597)  loss_rpn_box_reg: 0.0102 (0.0688)  time: 0.0986  data: 0.0166  max mem: 2231\n",
      "Epoch: [0]  [890/975]  eta: 0:00:08  lr: 0.004574  loss: 0.6870 (0.8940)  loss_classifier: 0.1410 (0.2058)  loss_box_reg: 0.1913 (0.2568)  loss_mask: 0.2409 (0.3035)  loss_objectness: 0.0286 (0.0594)  loss_rpn_box_reg: 0.0284 (0.0685)  time: 0.0996  data: 0.0168  max mem: 2231\n",
      "Epoch: [0]  [900/975]  eta: 0:00:07  lr: 0.004626  loss: 0.6413 (0.8910)  loss_classifier: 0.1207 (0.2048)  loss_box_reg: 0.1913 (0.2561)  loss_mask: 0.2216 (0.3027)  loss_objectness: 0.0246 (0.0591)  loss_rpn_box_reg: 0.0320 (0.0684)  time: 0.1004  data: 0.0170  max mem: 2231\n",
      "Epoch: [0]  [910/975]  eta: 0:00:06  lr: 0.004677  loss: 0.6413 (0.8899)  loss_classifier: 0.1207 (0.2041)  loss_box_reg: 0.2200 (0.2558)  loss_mask: 0.2216 (0.3022)  loss_objectness: 0.0280 (0.0591)  loss_rpn_box_reg: 0.0543 (0.0686)  time: 0.1008  data: 0.0168  max mem: 2231\n",
      "Epoch: [0]  [920/975]  eta: 0:00:05  lr: 0.004728  loss: 0.7143 (0.8882)  loss_classifier: 0.1555 (0.2036)  loss_box_reg: 0.2525 (0.2557)  loss_mask: 0.2438 (0.3017)  loss_objectness: 0.0312 (0.0589)  loss_rpn_box_reg: 0.0444 (0.0684)  time: 0.1007  data: 0.0166  max mem: 2231\n",
      "Epoch: [0]  [930/975]  eta: 0:00:04  lr: 0.004779  loss: 0.6916 (0.8857)  loss_classifier: 0.1254 (0.2025)  loss_box_reg: 0.2126 (0.2548)  loss_mask: 0.2462 (0.3013)  loss_objectness: 0.0312 (0.0587)  loss_rpn_box_reg: 0.0444 (0.0684)  time: 0.0996  data: 0.0173  max mem: 2231\n",
      "Epoch: [0]  [940/975]  eta: 0:00:03  lr: 0.004831  loss: 0.6727 (0.8835)  loss_classifier: 0.1056 (0.2017)  loss_box_reg: 0.1844 (0.2544)  loss_mask: 0.2401 (0.3006)  loss_objectness: 0.0292 (0.0584)  loss_rpn_box_reg: 0.0476 (0.0683)  time: 0.0990  data: 0.0176  max mem: 2231\n",
      "Epoch: [0]  [950/975]  eta: 0:00:02  lr: 0.004882  loss: 0.6724 (0.8806)  loss_classifier: 0.0958 (0.2008)  loss_box_reg: 0.1947 (0.2537)  loss_mask: 0.2173 (0.2999)  loss_objectness: 0.0314 (0.0582)  loss_rpn_box_reg: 0.0405 (0.0681)  time: 0.0993  data: 0.0168  max mem: 2231\n",
      "Epoch: [0]  [960/975]  eta: 0:00:01  lr: 0.004933  loss: 0.5802 (0.8783)  loss_classifier: 0.1064 (0.2000)  loss_box_reg: 0.1889 (0.2532)  loss_mask: 0.2313 (0.2993)  loss_objectness: 0.0267 (0.0580)  loss_rpn_box_reg: 0.0406 (0.0680)  time: 0.0985  data: 0.0162  max mem: 2231\n",
      "Epoch: [0]  [970/975]  eta: 0:00:00  lr: 0.004985  loss: 0.6474 (0.8770)  loss_classifier: 0.1267 (0.1995)  loss_box_reg: 0.2119 (0.2529)  loss_mask: 0.2480 (0.2989)  loss_objectness: 0.0305 (0.0577)  loss_rpn_box_reg: 0.0441 (0.0680)  time: 0.0993  data: 0.0165  max mem: 2231\n",
      "Epoch: [0]  [974/975]  eta: 0:00:00  lr: 0.005000  loss: 0.7558 (0.8769)  loss_classifier: 0.1287 (0.1994)  loss_box_reg: 0.2158 (0.2530)  loss_mask: 0.2369 (0.2988)  loss_objectness: 0.0310 (0.0577)  loss_rpn_box_reg: 0.0441 (0.0679)  time: 0.1002  data: 0.0166  max mem: 2231\n",
      "Epoch: [0] Total time: 0:01:40 (0.1036 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [  0/449]  eta: 0:02:22  model_time: 0.2398 (0.2398)  evaluator_time: 0.0722 (0.0722)  time: 0.3175  data: 0.0045  max mem: 2231\n",
      "Test:  [100/449]  eta: 0:00:30  model_time: 0.0465 (0.0535)  evaluator_time: 0.0260 (0.0282)  time: 0.0773  data: 0.0049  max mem: 2231\n",
      "Test:  [200/449]  eta: 0:00:19  model_time: 0.0270 (0.0459)  evaluator_time: 0.0105 (0.0255)  time: 0.0558  data: 0.0044  max mem: 2231\n",
      "Test:  [300/449]  eta: 0:00:11  model_time: 0.0385 (0.0443)  evaluator_time: 0.0185 (0.0255)  time: 0.0680  data: 0.0047  max mem: 2231\n",
      "Test:  [400/449]  eta: 0:00:03  model_time: 0.0415 (0.0445)  evaluator_time: 0.0230 (0.0259)  time: 0.0745  data: 0.0050  max mem: 2231\n",
      "Test:  [448/449]  eta: 0:00:00  model_time: 0.0426 (0.0444)  evaluator_time: 0.0200 (0.0258)  time: 0.0736  data: 0.0046  max mem: 2231\n",
      "Test: Total time: 0:00:34 (0.0760 s / it)\n",
      "Averaged stats: model_time: 0.0426 (0.0444)  evaluator_time: 0.0200 (0.0258)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.11s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.11s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.518\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.582\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.296\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.560\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.579\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.456\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.621\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.696\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.513\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.566\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.284\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.542\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.610\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.622\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.645\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632\n",
      "Epoch: [1]  [  0/975]  eta: 0:02:07  lr: 0.005000  loss: 0.7223 (0.7223)  loss_classifier: 0.1457 (0.1457)  loss_box_reg: 0.2582 (0.2582)  loss_mask: 0.2535 (0.2535)  loss_objectness: 0.0375 (0.0375)  loss_rpn_box_reg: 0.0273 (0.0273)  time: 0.1307  data: 0.0111  max mem: 2231\n",
      "Epoch: [1]  [ 10/975]  eta: 0:01:35  lr: 0.005000  loss: 0.6377 (0.6535)  loss_classifier: 0.1249 (0.1373)  loss_box_reg: 0.2176 (0.2211)  loss_mask: 0.2090 (0.2320)  loss_objectness: 0.0145 (0.0192)  loss_rpn_box_reg: 0.0273 (0.0438)  time: 0.0987  data: 0.0101  max mem: 2231\n",
      "Epoch: [1]  [ 20/975]  eta: 0:01:30  lr: 0.005000  loss: 0.5957 (0.6177)  loss_classifier: 0.1175 (0.1259)  loss_box_reg: 0.1654 (0.1960)  loss_mask: 0.2114 (0.2339)  loss_objectness: 0.0145 (0.0230)  loss_rpn_box_reg: 0.0237 (0.0388)  time: 0.0929  data: 0.0096  max mem: 2231\n",
      "Epoch: [1]  [ 30/975]  eta: 0:01:29  lr: 0.005000  loss: 0.6374 (0.6594)  loss_classifier: 0.1170 (0.1363)  loss_box_reg: 0.1813 (0.2084)  loss_mask: 0.2438 (0.2412)  loss_objectness: 0.0308 (0.0283)  loss_rpn_box_reg: 0.0371 (0.0452)  time: 0.0927  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [ 40/975]  eta: 0:01:28  lr: 0.005000  loss: 0.6658 (0.6611)  loss_classifier: 0.1141 (0.1304)  loss_box_reg: 0.1813 (0.2016)  loss_mask: 0.2179 (0.2339)  loss_objectness: 0.0283 (0.0286)  loss_rpn_box_reg: 0.0536 (0.0665)  time: 0.0945  data: 0.0099  max mem: 2234\n",
      "Epoch: [1]  [ 50/975]  eta: 0:01:27  lr: 0.005000  loss: 0.5068 (0.6606)  loss_classifier: 0.1284 (0.1328)  loss_box_reg: 0.1595 (0.2035)  loss_mask: 0.2163 (0.2302)  loss_objectness: 0.0250 (0.0297)  loss_rpn_box_reg: 0.0362 (0.0644)  time: 0.0935  data: 0.0098  max mem: 2234\n",
      "Epoch: [1]  [ 60/975]  eta: 0:01:26  lr: 0.005000  loss: 0.6344 (0.6583)  loss_classifier: 0.1303 (0.1306)  loss_box_reg: 0.1913 (0.2052)  loss_mask: 0.2248 (0.2324)  loss_objectness: 0.0244 (0.0287)  loss_rpn_box_reg: 0.0327 (0.0613)  time: 0.0930  data: 0.0094  max mem: 2234\n",
      "Epoch: [1]  [ 70/975]  eta: 0:01:24  lr: 0.005000  loss: 0.6329 (0.6461)  loss_classifier: 0.1068 (0.1258)  loss_box_reg: 0.2021 (0.2011)  loss_mask: 0.2248 (0.2306)  loss_objectness: 0.0166 (0.0275)  loss_rpn_box_reg: 0.0249 (0.0611)  time: 0.0928  data: 0.0093  max mem: 2234\n",
      "Epoch: [1]  [ 80/975]  eta: 0:01:23  lr: 0.005000  loss: 0.5581 (0.6343)  loss_classifier: 0.0972 (0.1233)  loss_box_reg: 0.1655 (0.1957)  loss_mask: 0.2151 (0.2307)  loss_objectness: 0.0154 (0.0264)  loss_rpn_box_reg: 0.0228 (0.0582)  time: 0.0915  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [ 90/975]  eta: 0:01:22  lr: 0.005000  loss: 0.5262 (0.6316)  loss_classifier: 0.0913 (0.1220)  loss_box_reg: 0.1367 (0.1943)  loss_mask: 0.2195 (0.2303)  loss_objectness: 0.0109 (0.0265)  loss_rpn_box_reg: 0.0223 (0.0584)  time: 0.0924  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [100/975]  eta: 0:01:21  lr: 0.005000  loss: 0.4171 (0.6227)  loss_classifier: 0.0689 (0.1197)  loss_box_reg: 0.1524 (0.1914)  loss_mask: 0.2046 (0.2277)  loss_objectness: 0.0109 (0.0265)  loss_rpn_box_reg: 0.0327 (0.0574)  time: 0.0942  data: 0.0099  max mem: 2234\n",
      "Epoch: [1]  [110/975]  eta: 0:01:21  lr: 0.005000  loss: 0.5403 (0.6233)  loss_classifier: 0.0935 (0.1198)  loss_box_reg: 0.1838 (0.1929)  loss_mask: 0.2060 (0.2277)  loss_objectness: 0.0189 (0.0264)  loss_rpn_box_reg: 0.0340 (0.0565)  time: 0.0946  data: 0.0100  max mem: 2234\n",
      "Epoch: [1]  [120/975]  eta: 0:01:20  lr: 0.005000  loss: 0.6406 (0.6305)  loss_classifier: 0.1216 (0.1209)  loss_box_reg: 0.2277 (0.1970)  loss_mask: 0.2317 (0.2290)  loss_objectness: 0.0226 (0.0269)  loss_rpn_box_reg: 0.0340 (0.0567)  time: 0.0958  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [130/975]  eta: 0:01:19  lr: 0.005000  loss: 0.6042 (0.6250)  loss_classifier: 0.1161 (0.1192)  loss_box_reg: 0.1970 (0.1959)  loss_mask: 0.2280 (0.2288)  loss_objectness: 0.0226 (0.0262)  loss_rpn_box_reg: 0.0283 (0.0550)  time: 0.0961  data: 0.0098  max mem: 2234\n",
      "Epoch: [1]  [140/975]  eta: 0:01:18  lr: 0.005000  loss: 0.5153 (0.6200)  loss_classifier: 0.0983 (0.1172)  loss_box_reg: 0.1869 (0.1955)  loss_mask: 0.2194 (0.2278)  loss_objectness: 0.0125 (0.0254)  loss_rpn_box_reg: 0.0271 (0.0541)  time: 0.0953  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [150/975]  eta: 0:01:17  lr: 0.005000  loss: 0.5602 (0.6226)  loss_classifier: 0.1036 (0.1183)  loss_box_reg: 0.1869 (0.1960)  loss_mask: 0.2296 (0.2292)  loss_objectness: 0.0118 (0.0254)  loss_rpn_box_reg: 0.0231 (0.0537)  time: 0.0931  data: 0.0096  max mem: 2234\n",
      "Epoch: [1]  [160/975]  eta: 0:01:16  lr: 0.005000  loss: 0.5822 (0.6221)  loss_classifier: 0.1117 (0.1186)  loss_box_reg: 0.1856 (0.1967)  loss_mask: 0.2341 (0.2289)  loss_objectness: 0.0135 (0.0249)  loss_rpn_box_reg: 0.0287 (0.0529)  time: 0.0914  data: 0.0093  max mem: 2234\n",
      "Epoch: [1]  [170/975]  eta: 0:01:15  lr: 0.005000  loss: 0.5105 (0.6158)  loss_classifier: 0.0878 (0.1168)  loss_box_reg: 0.1799 (0.1959)  loss_mask: 0.1984 (0.2275)  loss_objectness: 0.0135 (0.0243)  loss_rpn_box_reg: 0.0274 (0.0513)  time: 0.0911  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [180/975]  eta: 0:01:14  lr: 0.005000  loss: 0.6405 (0.6249)  loss_classifier: 0.1003 (0.1184)  loss_box_reg: 0.2101 (0.1985)  loss_mask: 0.2091 (0.2289)  loss_objectness: 0.0148 (0.0257)  loss_rpn_box_reg: 0.0274 (0.0534)  time: 0.0930  data: 0.0096  max mem: 2234\n",
      "Epoch: [1]  [190/975]  eta: 0:01:13  lr: 0.005000  loss: 0.6405 (0.6266)  loss_classifier: 0.1090 (0.1185)  loss_box_reg: 0.2224 (0.2002)  loss_mask: 0.2285 (0.2291)  loss_objectness: 0.0205 (0.0256)  loss_rpn_box_reg: 0.0490 (0.0532)  time: 0.0959  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [200/975]  eta: 0:01:12  lr: 0.005000  loss: 0.5649 (0.6246)  loss_classifier: 0.1009 (0.1183)  loss_box_reg: 0.1914 (0.1997)  loss_mask: 0.2166 (0.2285)  loss_objectness: 0.0181 (0.0252)  loss_rpn_box_reg: 0.0333 (0.0530)  time: 0.0947  data: 0.0094  max mem: 2234\n",
      "Epoch: [1]  [210/975]  eta: 0:01:11  lr: 0.005000  loss: 0.5649 (0.6267)  loss_classifier: 0.1243 (0.1192)  loss_box_reg: 0.1820 (0.2002)  loss_mask: 0.2166 (0.2280)  loss_objectness: 0.0213 (0.0265)  loss_rpn_box_reg: 0.0242 (0.0529)  time: 0.0940  data: 0.0094  max mem: 2234\n",
      "Epoch: [1]  [220/975]  eta: 0:01:10  lr: 0.005000  loss: 0.6178 (0.6262)  loss_classifier: 0.1245 (0.1192)  loss_box_reg: 0.1820 (0.1986)  loss_mask: 0.2229 (0.2279)  loss_objectness: 0.0271 (0.0271)  loss_rpn_box_reg: 0.0393 (0.0534)  time: 0.0935  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [230/975]  eta: 0:01:09  lr: 0.005000  loss: 0.5526 (0.6215)  loss_classifier: 0.0985 (0.1180)  loss_box_reg: 0.1768 (0.1973)  loss_mask: 0.2164 (0.2270)  loss_objectness: 0.0213 (0.0267)  loss_rpn_box_reg: 0.0315 (0.0525)  time: 0.0918  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [240/975]  eta: 0:01:08  lr: 0.005000  loss: 0.5526 (0.6196)  loss_classifier: 0.1007 (0.1173)  loss_box_reg: 0.1768 (0.1971)  loss_mask: 0.1945 (0.2258)  loss_objectness: 0.0202 (0.0268)  loss_rpn_box_reg: 0.0283 (0.0526)  time: 0.0933  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [250/975]  eta: 0:01:08  lr: 0.005000  loss: 0.6056 (0.6201)  loss_classifier: 0.1115 (0.1177)  loss_box_reg: 0.2067 (0.1972)  loss_mask: 0.2107 (0.2252)  loss_objectness: 0.0250 (0.0274)  loss_rpn_box_reg: 0.0336 (0.0526)  time: 0.0953  data: 0.0098  max mem: 2234\n",
      "Epoch: [1]  [260/975]  eta: 0:01:06  lr: 0.005000  loss: 0.5800 (0.6156)  loss_classifier: 0.0988 (0.1166)  loss_box_reg: 0.1493 (0.1955)  loss_mask: 0.2135 (0.2247)  loss_objectness: 0.0129 (0.0270)  loss_rpn_box_reg: 0.0230 (0.0518)  time: 0.0931  data: 0.0090  max mem: 2234\n",
      "Epoch: [1]  [270/975]  eta: 0:01:06  lr: 0.005000  loss: 0.5891 (0.6187)  loss_classifier: 0.0932 (0.1171)  loss_box_reg: 0.1648 (0.1963)  loss_mask: 0.2193 (0.2255)  loss_objectness: 0.0107 (0.0272)  loss_rpn_box_reg: 0.0254 (0.0526)  time: 0.0939  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [280/975]  eta: 0:01:05  lr: 0.005000  loss: 0.6144 (0.6171)  loss_classifier: 0.1071 (0.1164)  loss_box_reg: 0.1648 (0.1952)  loss_mask: 0.2270 (0.2250)  loss_objectness: 0.0204 (0.0276)  loss_rpn_box_reg: 0.0335 (0.0529)  time: 0.0941  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [290/975]  eta: 0:01:04  lr: 0.005000  loss: 0.5710 (0.6164)  loss_classifier: 0.1052 (0.1160)  loss_box_reg: 0.1516 (0.1950)  loss_mask: 0.2257 (0.2252)  loss_objectness: 0.0195 (0.0275)  loss_rpn_box_reg: 0.0335 (0.0527)  time: 0.0913  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [300/975]  eta: 0:01:03  lr: 0.005000  loss: 0.6221 (0.6172)  loss_classifier: 0.1049 (0.1160)  loss_box_reg: 0.1928 (0.1954)  loss_mask: 0.2253 (0.2249)  loss_objectness: 0.0271 (0.0283)  loss_rpn_box_reg: 0.0413 (0.0526)  time: 0.0938  data: 0.0094  max mem: 2234\n",
      "Epoch: [1]  [310/975]  eta: 0:01:02  lr: 0.005000  loss: 0.5832 (0.6179)  loss_classifier: 0.1058 (0.1159)  loss_box_reg: 0.2071 (0.1961)  loss_mask: 0.2121 (0.2246)  loss_objectness: 0.0265 (0.0281)  loss_rpn_box_reg: 0.0524 (0.0533)  time: 0.0952  data: 0.0099  max mem: 2234\n",
      "Epoch: [1]  [320/975]  eta: 0:01:01  lr: 0.005000  loss: 0.6219 (0.6199)  loss_classifier: 0.1155 (0.1164)  loss_box_reg: 0.2236 (0.1971)  loss_mask: 0.2110 (0.2246)  loss_objectness: 0.0221 (0.0282)  loss_rpn_box_reg: 0.0530 (0.0536)  time: 0.0955  data: 0.0099  max mem: 2234\n",
      "Epoch: [1]  [330/975]  eta: 0:01:00  lr: 0.005000  loss: 0.6476 (0.6209)  loss_classifier: 0.1155 (0.1162)  loss_box_reg: 0.2230 (0.1969)  loss_mask: 0.2231 (0.2250)  loss_objectness: 0.0221 (0.0284)  loss_rpn_box_reg: 0.0474 (0.0544)  time: 0.0951  data: 0.0098  max mem: 2234\n",
      "Epoch: [1]  [340/975]  eta: 0:00:59  lr: 0.005000  loss: 0.6026 (0.6207)  loss_classifier: 0.0998 (0.1161)  loss_box_reg: 0.2007 (0.1972)  loss_mask: 0.2247 (0.2249)  loss_objectness: 0.0192 (0.0282)  loss_rpn_box_reg: 0.0385 (0.0543)  time: 0.0932  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [350/975]  eta: 0:00:58  lr: 0.005000  loss: 0.6394 (0.6235)  loss_classifier: 0.1105 (0.1171)  loss_box_reg: 0.2177 (0.1983)  loss_mask: 0.2224 (0.2248)  loss_objectness: 0.0210 (0.0285)  loss_rpn_box_reg: 0.0385 (0.0549)  time: 0.0953  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [360/975]  eta: 0:00:57  lr: 0.005000  loss: 0.6757 (0.6245)  loss_classifier: 0.1183 (0.1170)  loss_box_reg: 0.1923 (0.1977)  loss_mask: 0.2224 (0.2257)  loss_objectness: 0.0236 (0.0288)  loss_rpn_box_reg: 0.0439 (0.0553)  time: 0.0972  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [370/975]  eta: 0:00:56  lr: 0.005000  loss: 0.6543 (0.6255)  loss_classifier: 0.0992 (0.1168)  loss_box_reg: 0.1876 (0.1978)  loss_mask: 0.2265 (0.2263)  loss_objectness: 0.0242 (0.0289)  loss_rpn_box_reg: 0.0450 (0.0557)  time: 0.0969  data: 0.0096  max mem: 2234\n",
      "Epoch: [1]  [380/975]  eta: 0:00:55  lr: 0.005000  loss: 0.6233 (0.6231)  loss_classifier: 0.0907 (0.1160)  loss_box_reg: 0.2068 (0.1974)  loss_mask: 0.2263 (0.2262)  loss_objectness: 0.0153 (0.0285)  loss_rpn_box_reg: 0.0291 (0.0549)  time: 0.0946  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [390/975]  eta: 0:00:54  lr: 0.005000  loss: 0.5433 (0.6213)  loss_classifier: 0.0907 (0.1155)  loss_box_reg: 0.1696 (0.1969)  loss_mask: 0.2203 (0.2263)  loss_objectness: 0.0107 (0.0281)  loss_rpn_box_reg: 0.0292 (0.0544)  time: 0.0908  data: 0.0089  max mem: 2234\n",
      "Epoch: [1]  [400/975]  eta: 0:00:53  lr: 0.005000  loss: 0.6018 (0.6214)  loss_classifier: 0.1147 (0.1157)  loss_box_reg: 0.1582 (0.1963)  loss_mask: 0.2305 (0.2274)  loss_objectness: 0.0137 (0.0281)  loss_rpn_box_reg: 0.0318 (0.0540)  time: 0.0900  data: 0.0089  max mem: 2234\n",
      "Epoch: [1]  [410/975]  eta: 0:00:52  lr: 0.005000  loss: 0.5146 (0.6200)  loss_classifier: 0.0958 (0.1152)  loss_box_reg: 0.1565 (0.1959)  loss_mask: 0.2131 (0.2269)  loss_objectness: 0.0200 (0.0279)  loss_rpn_box_reg: 0.0222 (0.0541)  time: 0.0912  data: 0.0090  max mem: 2234\n",
      "Epoch: [1]  [420/975]  eta: 0:00:52  lr: 0.005000  loss: 0.5794 (0.6201)  loss_classifier: 0.0943 (0.1151)  loss_box_reg: 0.1643 (0.1956)  loss_mask: 0.2176 (0.2271)  loss_objectness: 0.0215 (0.0281)  loss_rpn_box_reg: 0.0286 (0.0543)  time: 0.0930  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [430/975]  eta: 0:00:51  lr: 0.005000  loss: 0.5992 (0.6190)  loss_classifier: 0.0914 (0.1146)  loss_box_reg: 0.1877 (0.1954)  loss_mask: 0.2237 (0.2270)  loss_objectness: 0.0156 (0.0281)  loss_rpn_box_reg: 0.0306 (0.0539)  time: 0.0940  data: 0.0091  max mem: 2234\n",
      "Epoch: [1]  [440/975]  eta: 0:00:50  lr: 0.005000  loss: 0.5785 (0.6186)  loss_classifier: 0.0941 (0.1143)  loss_box_reg: 0.1765 (0.1950)  loss_mask: 0.2076 (0.2268)  loss_objectness: 0.0160 (0.0281)  loss_rpn_box_reg: 0.0340 (0.0545)  time: 0.0933  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [450/975]  eta: 0:00:49  lr: 0.005000  loss: 0.5635 (0.6178)  loss_classifier: 0.0920 (0.1142)  loss_box_reg: 0.1758 (0.1946)  loss_mask: 0.2227 (0.2266)  loss_objectness: 0.0184 (0.0281)  loss_rpn_box_reg: 0.0432 (0.0543)  time: 0.0927  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [460/975]  eta: 0:00:48  lr: 0.005000  loss: 0.5832 (0.6174)  loss_classifier: 0.1007 (0.1143)  loss_box_reg: 0.1653 (0.1944)  loss_mask: 0.2259 (0.2267)  loss_objectness: 0.0154 (0.0279)  loss_rpn_box_reg: 0.0283 (0.0540)  time: 0.0921  data: 0.0091  max mem: 2234\n",
      "Epoch: [1]  [470/975]  eta: 0:00:47  lr: 0.005000  loss: 0.5869 (0.6177)  loss_classifier: 0.1089 (0.1143)  loss_box_reg: 0.1714 (0.1946)  loss_mask: 0.2259 (0.2268)  loss_objectness: 0.0194 (0.0280)  loss_rpn_box_reg: 0.0277 (0.0540)  time: 0.0917  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [480/975]  eta: 0:00:46  lr: 0.005000  loss: 0.5345 (0.6155)  loss_classifier: 0.0809 (0.1138)  loss_box_reg: 0.1578 (0.1934)  loss_mask: 0.2246 (0.2271)  loss_objectness: 0.0192 (0.0277)  loss_rpn_box_reg: 0.0147 (0.0534)  time: 0.0894  data: 0.0090  max mem: 2234\n",
      "Epoch: [1]  [490/975]  eta: 0:00:45  lr: 0.005000  loss: 0.5401 (0.6146)  loss_classifier: 0.0803 (0.1136)  loss_box_reg: 0.1243 (0.1928)  loss_mask: 0.2293 (0.2274)  loss_objectness: 0.0174 (0.0278)  loss_rpn_box_reg: 0.0121 (0.0531)  time: 0.0888  data: 0.0087  max mem: 2234\n",
      "Epoch: [1]  [500/975]  eta: 0:00:44  lr: 0.005000  loss: 0.5449 (0.6135)  loss_classifier: 0.0986 (0.1133)  loss_box_reg: 0.1520 (0.1923)  loss_mask: 0.2287 (0.2276)  loss_objectness: 0.0168 (0.0276)  loss_rpn_box_reg: 0.0248 (0.0527)  time: 0.0904  data: 0.0091  max mem: 2234\n",
      "Epoch: [1]  [510/975]  eta: 0:00:43  lr: 0.005000  loss: 0.5061 (0.6106)  loss_classifier: 0.0823 (0.1125)  loss_box_reg: 0.1340 (0.1913)  loss_mask: 0.2146 (0.2271)  loss_objectness: 0.0168 (0.0274)  loss_rpn_box_reg: 0.0219 (0.0522)  time: 0.0888  data: 0.0086  max mem: 2234\n",
      "Epoch: [1]  [520/975]  eta: 0:00:42  lr: 0.005000  loss: 0.5217 (0.6131)  loss_classifier: 0.0881 (0.1131)  loss_box_reg: 0.1590 (0.1917)  loss_mask: 0.2159 (0.2276)  loss_objectness: 0.0225 (0.0279)  loss_rpn_box_reg: 0.0261 (0.0527)  time: 0.0915  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [530/975]  eta: 0:00:41  lr: 0.005000  loss: 0.6216 (0.6139)  loss_classifier: 0.1160 (0.1134)  loss_box_reg: 0.2003 (0.1915)  loss_mask: 0.2312 (0.2275)  loss_objectness: 0.0299 (0.0286)  loss_rpn_box_reg: 0.0398 (0.0528)  time: 0.0935  data: 0.0102  max mem: 2234\n",
      "Epoch: [1]  [540/975]  eta: 0:00:40  lr: 0.005000  loss: 0.6523 (0.6156)  loss_classifier: 0.1252 (0.1137)  loss_box_reg: 0.1810 (0.1913)  loss_mask: 0.2201 (0.2277)  loss_objectness: 0.0299 (0.0293)  loss_rpn_box_reg: 0.0520 (0.0535)  time: 0.0924  data: 0.0105  max mem: 2234\n",
      "Epoch: [1]  [550/975]  eta: 0:00:39  lr: 0.005000  loss: 0.5479 (0.6155)  loss_classifier: 0.0798 (0.1136)  loss_box_reg: 0.1396 (0.1907)  loss_mask: 0.2202 (0.2281)  loss_objectness: 0.0311 (0.0298)  loss_rpn_box_reg: 0.0396 (0.0533)  time: 0.0909  data: 0.0102  max mem: 2234\n",
      "Epoch: [1]  [560/975]  eta: 0:00:38  lr: 0.005000  loss: 0.5479 (0.6148)  loss_classifier: 0.0823 (0.1133)  loss_box_reg: 0.1263 (0.1898)  loss_mask: 0.2334 (0.2284)  loss_objectness: 0.0362 (0.0301)  loss_rpn_box_reg: 0.0304 (0.0531)  time: 0.0890  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [570/975]  eta: 0:00:37  lr: 0.005000  loss: 0.5465 (0.6128)  loss_classifier: 0.0852 (0.1128)  loss_box_reg: 0.1409 (0.1893)  loss_mask: 0.2125 (0.2278)  loss_objectness: 0.0200 (0.0300)  loss_rpn_box_reg: 0.0446 (0.0529)  time: 0.0900  data: 0.0090  max mem: 2234\n",
      "Epoch: [1]  [580/975]  eta: 0:00:36  lr: 0.005000  loss: 0.5452 (0.6125)  loss_classifier: 0.0855 (0.1126)  loss_box_reg: 0.1785 (0.1894)  loss_mask: 0.1871 (0.2275)  loss_objectness: 0.0187 (0.0300)  loss_rpn_box_reg: 0.0513 (0.0531)  time: 0.0923  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [590/975]  eta: 0:00:35  lr: 0.005000  loss: 0.5846 (0.6129)  loss_classifier: 0.1052 (0.1126)  loss_box_reg: 0.1660 (0.1887)  loss_mask: 0.2110 (0.2280)  loss_objectness: 0.0295 (0.0304)  loss_rpn_box_reg: 0.0423 (0.0532)  time: 0.0909  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [600/975]  eta: 0:00:34  lr: 0.005000  loss: 0.5975 (0.6136)  loss_classifier: 0.1089 (0.1124)  loss_box_reg: 0.1484 (0.1885)  loss_mask: 0.2450 (0.2283)  loss_objectness: 0.0427 (0.0308)  loss_rpn_box_reg: 0.0315 (0.0536)  time: 0.0900  data: 0.0093  max mem: 2234\n",
      "Epoch: [1]  [610/975]  eta: 0:00:33  lr: 0.005000  loss: 0.6135 (0.6132)  loss_classifier: 0.1049 (0.1123)  loss_box_reg: 0.1814 (0.1883)  loss_mask: 0.2317 (0.2282)  loss_objectness: 0.0401 (0.0308)  loss_rpn_box_reg: 0.0371 (0.0537)  time: 0.0915  data: 0.0094  max mem: 2234\n",
      "Epoch: [1]  [620/975]  eta: 0:00:33  lr: 0.005000  loss: 0.6152 (0.6144)  loss_classifier: 0.1032 (0.1123)  loss_box_reg: 0.1857 (0.1885)  loss_mask: 0.2145 (0.2283)  loss_objectness: 0.0328 (0.0309)  loss_rpn_box_reg: 0.0469 (0.0544)  time: 0.0927  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [630/975]  eta: 0:00:32  lr: 0.005000  loss: 0.6535 (0.6151)  loss_classifier: 0.1025 (0.1124)  loss_box_reg: 0.2097 (0.1891)  loss_mask: 0.2230 (0.2281)  loss_objectness: 0.0315 (0.0309)  loss_rpn_box_reg: 0.0551 (0.0547)  time: 0.0946  data: 0.0098  max mem: 2234\n",
      "Epoch: [1]  [640/975]  eta: 0:00:31  lr: 0.005000  loss: 0.5883 (0.6157)  loss_classifier: 0.1207 (0.1125)  loss_box_reg: 0.2021 (0.1892)  loss_mask: 0.2218 (0.2281)  loss_objectness: 0.0250 (0.0309)  loss_rpn_box_reg: 0.0386 (0.0550)  time: 0.0941  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [650/975]  eta: 0:00:30  lr: 0.005000  loss: 0.5499 (0.6152)  loss_classifier: 0.1164 (0.1125)  loss_box_reg: 0.1733 (0.1892)  loss_mask: 0.2196 (0.2281)  loss_objectness: 0.0177 (0.0307)  loss_rpn_box_reg: 0.0264 (0.0546)  time: 0.0922  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [660/975]  eta: 0:00:29  lr: 0.005000  loss: 0.5492 (0.6140)  loss_classifier: 0.1003 (0.1123)  loss_box_reg: 0.1351 (0.1884)  loss_mask: 0.2256 (0.2284)  loss_objectness: 0.0166 (0.0306)  loss_rpn_box_reg: 0.0187 (0.0543)  time: 0.0899  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [670/975]  eta: 0:00:28  lr: 0.005000  loss: 0.5467 (0.6137)  loss_classifier: 0.1022 (0.1124)  loss_box_reg: 0.1215 (0.1880)  loss_mask: 0.2273 (0.2282)  loss_objectness: 0.0191 (0.0308)  loss_rpn_box_reg: 0.0197 (0.0543)  time: 0.0895  data: 0.0093  max mem: 2234\n",
      "Epoch: [1]  [680/975]  eta: 0:00:27  lr: 0.005000  loss: 0.5510 (0.6138)  loss_classifier: 0.1042 (0.1124)  loss_box_reg: 0.1913 (0.1883)  loss_mask: 0.2062 (0.2280)  loss_objectness: 0.0259 (0.0308)  loss_rpn_box_reg: 0.0330 (0.0543)  time: 0.0933  data: 0.0102  max mem: 2234\n",
      "Epoch: [1]  [690/975]  eta: 0:00:26  lr: 0.005000  loss: 0.6257 (0.6142)  loss_classifier: 0.1144 (0.1126)  loss_box_reg: 0.1955 (0.1884)  loss_mask: 0.2099 (0.2281)  loss_objectness: 0.0260 (0.0307)  loss_rpn_box_reg: 0.0337 (0.0544)  time: 0.0938  data: 0.0102  max mem: 2234\n",
      "Epoch: [1]  [700/975]  eta: 0:00:25  lr: 0.005000  loss: 0.7044 (0.6170)  loss_classifier: 0.1344 (0.1132)  loss_box_reg: 0.2536 (0.1892)  loss_mask: 0.2268 (0.2283)  loss_objectness: 0.0260 (0.0309)  loss_rpn_box_reg: 0.0494 (0.0554)  time: 0.0941  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [710/975]  eta: 0:00:24  lr: 0.005000  loss: 0.5615 (0.6165)  loss_classifier: 0.1185 (0.1130)  loss_box_reg: 0.1483 (0.1890)  loss_mask: 0.2428 (0.2286)  loss_objectness: 0.0270 (0.0309)  loss_rpn_box_reg: 0.0348 (0.0551)  time: 0.0928  data: 0.0100  max mem: 2234\n",
      "Epoch: [1]  [720/975]  eta: 0:00:23  lr: 0.005000  loss: 0.5615 (0.6166)  loss_classifier: 0.0991 (0.1131)  loss_box_reg: 0.1450 (0.1892)  loss_mask: 0.2358 (0.2286)  loss_objectness: 0.0246 (0.0308)  loss_rpn_box_reg: 0.0202 (0.0550)  time: 0.0922  data: 0.0100  max mem: 2234\n",
      "Epoch: [1]  [730/975]  eta: 0:00:22  lr: 0.005000  loss: 0.6154 (0.6162)  loss_classifier: 0.1079 (0.1129)  loss_box_reg: 0.2031 (0.1895)  loss_mask: 0.2066 (0.2281)  loss_objectness: 0.0229 (0.0308)  loss_rpn_box_reg: 0.0298 (0.0548)  time: 0.0948  data: 0.0103  max mem: 2234\n",
      "Epoch: [1]  [740/975]  eta: 0:00:21  lr: 0.005000  loss: 0.5890 (0.6172)  loss_classifier: 0.1017 (0.1130)  loss_box_reg: 0.2032 (0.1902)  loss_mask: 0.2026 (0.2282)  loss_objectness: 0.0229 (0.0309)  loss_rpn_box_reg: 0.0356 (0.0549)  time: 0.0945  data: 0.0102  max mem: 2234\n",
      "Epoch: [1]  [750/975]  eta: 0:00:20  lr: 0.005000  loss: 0.6519 (0.6175)  loss_classifier: 0.1063 (0.1130)  loss_box_reg: 0.2175 (0.1904)  loss_mask: 0.2074 (0.2280)  loss_objectness: 0.0198 (0.0309)  loss_rpn_box_reg: 0.0400 (0.0551)  time: 0.0930  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [760/975]  eta: 0:00:19  lr: 0.005000  loss: 0.6220 (0.6177)  loss_classifier: 0.1056 (0.1129)  loss_box_reg: 0.2175 (0.1910)  loss_mask: 0.2174 (0.2279)  loss_objectness: 0.0211 (0.0309)  loss_rpn_box_reg: 0.0400 (0.0550)  time: 0.0934  data: 0.0100  max mem: 2234\n",
      "Epoch: [1]  [770/975]  eta: 0:00:19  lr: 0.005000  loss: 0.6262 (0.6191)  loss_classifier: 0.1062 (0.1132)  loss_box_reg: 0.2312 (0.1917)  loss_mask: 0.2239 (0.2279)  loss_objectness: 0.0238 (0.0310)  loss_rpn_box_reg: 0.0422 (0.0553)  time: 0.0960  data: 0.0103  max mem: 2234\n",
      "Epoch: [1]  [780/975]  eta: 0:00:18  lr: 0.005000  loss: 0.6262 (0.6193)  loss_classifier: 0.1312 (0.1133)  loss_box_reg: 0.2195 (0.1920)  loss_mask: 0.2249 (0.2280)  loss_objectness: 0.0190 (0.0309)  loss_rpn_box_reg: 0.0422 (0.0552)  time: 0.0951  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [790/975]  eta: 0:00:17  lr: 0.005000  loss: 0.6584 (0.6197)  loss_classifier: 0.1097 (0.1133)  loss_box_reg: 0.2042 (0.1920)  loss_mask: 0.2160 (0.2282)  loss_objectness: 0.0152 (0.0310)  loss_rpn_box_reg: 0.0386 (0.0552)  time: 0.0933  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [800/975]  eta: 0:00:16  lr: 0.005000  loss: 0.6626 (0.6196)  loss_classifier: 0.1097 (0.1132)  loss_box_reg: 0.1694 (0.1919)  loss_mask: 0.2160 (0.2283)  loss_objectness: 0.0173 (0.0310)  loss_rpn_box_reg: 0.0255 (0.0552)  time: 0.0938  data: 0.0094  max mem: 2234\n",
      "Epoch: [1]  [810/975]  eta: 0:00:15  lr: 0.005000  loss: 0.6938 (0.6210)  loss_classifier: 0.1280 (0.1136)  loss_box_reg: 0.2147 (0.1925)  loss_mask: 0.2357 (0.2285)  loss_objectness: 0.0291 (0.0312)  loss_rpn_box_reg: 0.0419 (0.0553)  time: 0.0960  data: 0.0096  max mem: 2234\n",
      "Epoch: [1]  [820/975]  eta: 0:00:14  lr: 0.005000  loss: 0.6629 (0.6212)  loss_classifier: 0.1314 (0.1137)  loss_box_reg: 0.2147 (0.1929)  loss_mask: 0.2374 (0.2286)  loss_objectness: 0.0250 (0.0310)  loss_rpn_box_reg: 0.0479 (0.0550)  time: 0.0952  data: 0.0099  max mem: 2234\n",
      "Epoch: [1]  [830/975]  eta: 0:00:13  lr: 0.005000  loss: 0.6228 (0.6221)  loss_classifier: 0.1037 (0.1136)  loss_box_reg: 0.1896 (0.1932)  loss_mask: 0.2286 (0.2287)  loss_objectness: 0.0170 (0.0312)  loss_rpn_box_reg: 0.0382 (0.0553)  time: 0.0932  data: 0.0096  max mem: 2234\n",
      "Epoch: [1]  [840/975]  eta: 0:00:12  lr: 0.005000  loss: 0.5537 (0.6209)  loss_classifier: 0.1114 (0.1135)  loss_box_reg: 0.1650 (0.1926)  loss_mask: 0.2268 (0.2286)  loss_objectness: 0.0141 (0.0311)  loss_rpn_box_reg: 0.0369 (0.0551)  time: 0.0911  data: 0.0093  max mem: 2234\n",
      "Epoch: [1]  [850/975]  eta: 0:00:11  lr: 0.005000  loss: 0.5080 (0.6207)  loss_classifier: 0.0810 (0.1134)  loss_box_reg: 0.1608 (0.1926)  loss_mask: 0.2109 (0.2284)  loss_objectness: 0.0155 (0.0311)  loss_rpn_box_reg: 0.0214 (0.0551)  time: 0.0906  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [860/975]  eta: 0:00:10  lr: 0.005000  loss: 0.5171 (0.6205)  loss_classifier: 0.0889 (0.1135)  loss_box_reg: 0.1656 (0.1926)  loss_mask: 0.2015 (0.2283)  loss_objectness: 0.0147 (0.0310)  loss_rpn_box_reg: 0.0284 (0.0550)  time: 0.0918  data: 0.0091  max mem: 2234\n",
      "Epoch: [1]  [870/975]  eta: 0:00:09  lr: 0.005000  loss: 0.5201 (0.6201)  loss_classifier: 0.0944 (0.1134)  loss_box_reg: 0.1603 (0.1923)  loss_mask: 0.2161 (0.2282)  loss_objectness: 0.0150 (0.0310)  loss_rpn_box_reg: 0.0284 (0.0552)  time: 0.0915  data: 0.0091  max mem: 2234\n",
      "Epoch: [1]  [880/975]  eta: 0:00:08  lr: 0.005000  loss: 0.5489 (0.6209)  loss_classifier: 0.0972 (0.1135)  loss_box_reg: 0.1603 (0.1923)  loss_mask: 0.2208 (0.2283)  loss_objectness: 0.0238 (0.0312)  loss_rpn_box_reg: 0.0494 (0.0557)  time: 0.0927  data: 0.0095  max mem: 2234\n",
      "Epoch: [1]  [890/975]  eta: 0:00:07  lr: 0.005000  loss: 0.5669 (0.6206)  loss_classifier: 0.1049 (0.1133)  loss_box_reg: 0.1904 (0.1922)  loss_mask: 0.2207 (0.2280)  loss_objectness: 0.0221 (0.0311)  loss_rpn_box_reg: 0.0494 (0.0560)  time: 0.0930  data: 0.0097  max mem: 2234\n",
      "Epoch: [1]  [900/975]  eta: 0:00:06  lr: 0.005000  loss: 0.5731 (0.6211)  loss_classifier: 0.1049 (0.1134)  loss_box_reg: 0.1801 (0.1924)  loss_mask: 0.2005 (0.2279)  loss_objectness: 0.0309 (0.0312)  loss_rpn_box_reg: 0.0458 (0.0562)  time: 0.0940  data: 0.0101  max mem: 2234\n",
      "Epoch: [1]  [910/975]  eta: 0:00:06  lr: 0.005000  loss: 0.5731 (0.6208)  loss_classifier: 0.0996 (0.1134)  loss_box_reg: 0.1492 (0.1921)  loss_mask: 0.2141 (0.2280)  loss_objectness: 0.0309 (0.0312)  loss_rpn_box_reg: 0.0507 (0.0561)  time: 0.0929  data: 0.0098  max mem: 2234\n",
      "Epoch: [1]  [920/975]  eta: 0:00:05  lr: 0.005000  loss: 0.5449 (0.6200)  loss_classifier: 0.0987 (0.1131)  loss_box_reg: 0.1492 (0.1919)  loss_mask: 0.2141 (0.2279)  loss_objectness: 0.0145 (0.0311)  loss_rpn_box_reg: 0.0328 (0.0559)  time: 0.0908  data: 0.0094  max mem: 2234\n",
      "Epoch: [1]  [930/975]  eta: 0:00:04  lr: 0.005000  loss: 0.5037 (0.6193)  loss_classifier: 0.0859 (0.1130)  loss_box_reg: 0.1631 (0.1918)  loss_mask: 0.2260 (0.2280)  loss_objectness: 0.0123 (0.0309)  loss_rpn_box_reg: 0.0200 (0.0556)  time: 0.0911  data: 0.0090  max mem: 2234\n",
      "Epoch: [1]  [940/975]  eta: 0:00:03  lr: 0.005000  loss: 0.4660 (0.6210)  loss_classifier: 0.0859 (0.1131)  loss_box_reg: 0.1698 (0.1918)  loss_mask: 0.2260 (0.2280)  loss_objectness: 0.0184 (0.0315)  loss_rpn_box_reg: 0.0224 (0.0566)  time: 0.0928  data: 0.0093  max mem: 2234\n",
      "Epoch: [1]  [950/975]  eta: 0:00:02  lr: 0.005000  loss: 0.5112 (0.6205)  loss_classifier: 0.0941 (0.1130)  loss_box_reg: 0.1874 (0.1917)  loss_mask: 0.2224 (0.2280)  loss_objectness: 0.0226 (0.0315)  loss_rpn_box_reg: 0.0273 (0.0564)  time: 0.0922  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [960/975]  eta: 0:00:01  lr: 0.005000  loss: 0.5383 (0.6200)  loss_classifier: 0.0941 (0.1129)  loss_box_reg: 0.1861 (0.1917)  loss_mask: 0.2224 (0.2280)  loss_objectness: 0.0186 (0.0313)  loss_rpn_box_reg: 0.0210 (0.0561)  time: 0.0915  data: 0.0092  max mem: 2234\n",
      "Epoch: [1]  [970/975]  eta: 0:00:00  lr: 0.005000  loss: 0.6031 (0.6196)  loss_classifier: 0.0956 (0.1128)  loss_box_reg: 0.1959 (0.1917)  loss_mask: 0.2105 (0.2279)  loss_objectness: 0.0139 (0.0312)  loss_rpn_box_reg: 0.0257 (0.0559)  time: 0.0924  data: 0.0096  max mem: 2234\n",
      "Epoch: [1]  [974/975]  eta: 0:00:00  lr: 0.005000  loss: 0.5883 (0.6194)  loss_classifier: 0.0956 (0.1128)  loss_box_reg: 0.1772 (0.1917)  loss_mask: 0.2013 (0.2279)  loss_objectness: 0.0129 (0.0311)  loss_rpn_box_reg: 0.0287 (0.0558)  time: 0.0923  data: 0.0093  max mem: 2234\n",
      "Epoch: [1] Total time: 0:01:30 (0.0930 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [  0/449]  eta: 0:00:53  model_time: 0.0871 (0.0871)  evaluator_time: 0.0265 (0.0265)  time: 0.1186  data: 0.0040  max mem: 2234\n",
      "Test:  [100/449]  eta: 0:00:19  model_time: 0.0275 (0.0342)  evaluator_time: 0.0115 (0.0159)  time: 0.0511  data: 0.0047  max mem: 2234\n",
      "Test:  [200/449]  eta: 0:00:12  model_time: 0.0230 (0.0309)  evaluator_time: 0.0065 (0.0143)  time: 0.0404  data: 0.0042  max mem: 2234\n",
      "Test:  [300/449]  eta: 0:00:07  model_time: 0.0245 (0.0302)  evaluator_time: 0.0100 (0.0142)  time: 0.0451  data: 0.0042  max mem: 2234\n",
      "Test:  [400/449]  eta: 0:00:02  model_time: 0.0261 (0.0301)  evaluator_time: 0.0120 (0.0144)  time: 0.0479  data: 0.0045  max mem: 2234\n",
      "Test:  [448/449]  eta: 0:00:00  model_time: 0.0265 (0.0300)  evaluator_time: 0.0130 (0.0144)  time: 0.0487  data: 0.0042  max mem: 2234\n",
      "Test: Total time: 0:00:22 (0.0496 s / it)\n",
      "Averaged stats: model_time: 0.0265 (0.0300)  evaluator_time: 0.0130 (0.0144)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.591\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.873\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.682\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.370\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.626\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.639\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.475\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.659\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.670\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.447\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.705\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.733\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.532\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.865\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.603\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.323\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.615\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.631\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.619\n",
      "Model saved at: results\\models\\model_weights.pth\n",
      "Deleted folder: basketball-and-hoop-11\n",
      "Video not found: results\\data\\Devin Booker Sets Record, Wins Three-Point Contest.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The function `to_tensor(...)` is deprecated and will be removed in a future release. Instead, please use `to_image(...)` followed by `to_dtype(..., dtype=torch.float32, scale=True)`.\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
    "                --workspace basketball-formations \\\n",
    "                --project_name basketball-and-hoop-7xk0h \\\n",
    "                --project_folder_name basketball-and-hoop \\\n",
    "                --version 11 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 2 \\\n",
    "                --threshold 0.6 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=kh7s2tGvswc&t=1s\" \\\n",
    "                --video_name \"Devin Booker Sets Record, Wins Three-Point Contest\" \\\n",
    "                --delete_folder_and_video True \\\n",
    "                --mode train \n",
    "\n",
    "                #--check_intersections True \\\n",
    "                #--classes_to_track Basketball Hoop \\\n",
    "#ex command line usage\n",
    "#python train.py --api_key htpcxp3XQh7SsgMfjJns --workspace basketball-formations --project_name basketball-and-hoop-7xk0h --project_folder_name basketball-and-hoop --version 10 --hidden_layer 256 --lr 0.005 --num_epochs 1 --threshold 0.6 --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" --video_name \"The Best NBA 3 Point Contest Performances\" --mode process_video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"netventure\").project(\"car_segment-yarm8\")\n",
    "#dataset = project.version(1).download(\"coco-segmentation\")\n",
    "\n",
    "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
    "                --workspace netventure \\\n",
    "                --project_name car_segment-yarm8 \\\n",
    "                --project_folder_name car_segment \\\n",
    "                --version 1 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 10 \\\n",
    "                --threshold 0.8 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=boVidZ2K-QI\" \\\n",
    "                --video_name \"Driving for 1 minute\" \\\n",
    "                --mode train \\\n",
    "                --delete_folder_and_video True\n",
    "\n",
    "#ex command line usage\n",
    "#python train.py --api_key htpcxp3XQh7SsgMfjJns --workspace basketball-formations --project_name basketball-and-hoop-7xk0h --project_folder_name basketball-and-hoop --version 10 --hidden_layer 256 --lr 0.005 --num_epochs 1 --threshold 0.6 --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" --video_name \"The Best NBA 3 Point Contest Performances\" --mode process_video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "import torch\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import modular functions (make sure all functions are correctly imported)\n",
    "from going_modular.utils import (get_device, create_directory, get_project,\n",
    "                                 download_files, construct_dataset_paths,\n",
    "                                 download_videos_from_youtube)\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import get_model_instance_segmentation, load_classes_from_json\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.process_video_check import process_video_check\n",
    "from going_modular.transforms import get_transform, transform_image\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Global variables (Consider storing and accessing these more securely and flexibly)\n",
    "MODEL_PATH = Path('results/models/model_weights.pth')\n",
    " \n",
    "\n",
    "# Ensure MODEL_PATH directory exists\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_model(num_classes: int, model_path: Path = MODEL_PATH):\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=256)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the API!\"}\n",
    "\n",
    "@app.post(\"/train\")\n",
    "async def train(api_key: str, workspace: str, project_name: str, project_folder_name: str, version: int, num_epochs: int = 10):\n",
    "    # Set up device\n",
    "    device = get_device()\n",
    "    \n",
    "    # Ensure data and model directories exist\n",
    "    data_path = Path('results/data')\n",
    "    model_path = Path('results/models')\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download and prepare the dataset\n",
    "    dataset = get_project(api_key, workspace, project_name, version)\n",
    "    train_annotation_path, valid_annotation_path, test_annotation_path, train_image_dir, valid_image_dir, test_image_dir = construct_dataset_paths(project_folder_name, version)\n",
    "    \n",
    "    # Load class names and set up the model\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    \n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=256)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = CustomCocoDataset(train_annotation_path, train_image_dir, transforms=get_transform(train=True))\n",
    "    valid_dataset = CustomCocoDataset(valid_annotation_path, valid_image_dir, transforms=get_transform(train=False))\n",
    "    \n",
    "    # Set up data loaders\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_data_loader, valid_data_loader, device, num_epochs)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    return {\"message\": \"Model trained and saved successfully\"}\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(project_folder_name: str, version: int, file: UploadFile = File(...)):\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    model = load_model(num_classes)\n",
    "    \n",
    "    image_bytes = await file.read()\n",
    "    tensor = transform_image(image_bytes)\n",
    "    \n",
    "    # Prediction logic\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        top_prob, top_catid = probabilities.topk(1, dim=1)\n",
    "    \n",
    "    predicted_class = classes[top_catid.item()]\n",
    "    confidence = top_prob.item()\n",
    "    \n",
    "    return JSONResponse(content={\"class\": predicted_class, \"confidence\": confidence})\n",
    "\n",
    "from fastapi import UploadFile, File\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "@app.post(\"/process_video\")\n",
    "async def process_video(project_folder_name: str, version: int, video_file: UploadFile = File(...)):\n",
    "    # Define where to save the video temporarily\n",
    "    temp_video_path = Path(\"temp_videos\") / video_file.filename\n",
    "    temp_video_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save the uploaded video to the temporary path\n",
    "    with temp_video_path.open(\"wb\") as buffer:\n",
    "        shutil.copyfileobj(video_file.file, buffer)\n",
    "    \n",
    "    # Load model and classes for processing\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    model = load_model(num_classes)\n",
    "    \n",
    "    # Assuming process_video_check is adapted to return a meaningful result\n",
    "    # For example, modifying process_video_check to accept a video path and return a dictionary of results\n",
    "    results = process_video_check(str(temp_video_path), model, get_device(), classes, [('Basketball', 'Hoop')], threshold=0.6)\n",
    "\n",
    "    # Optionally, delete the temporary video file after processing\n",
    "    temp_video_path.unlink(missing_ok=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "\n",
    "#!uivcorn app:app --reload\n",
    "\n",
    "#For looking at all the options**\n",
    "#http://127.0.0.1:8000/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
