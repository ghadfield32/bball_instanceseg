{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust workflow\n",
    "*change load classes to the utils\n",
    "* add in the youtube download to utils XX\n",
    "* add changes for above two changes the main.py XX\n",
    "* change main.py to train.py so you can full automate to CLI easiest\n",
    "* add in the classes to the labels visualizations XX\n",
    "* add uses to it \n",
    "* add to hugging face/Snap AR\n",
    "* attempt to use cv2 to see when basketballs intersect with rims XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images outside of youtube nba footage come from: [text](https://stock.adobe.com/search?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bcontent_type%3Aillustration%5D=1&filters%5Bcontent_type%3Azip_vector%5D=1&filters%5Bcontent_type%3Avideo%5D=1&filters%5Bcontent_type%3Atemplate%5D=1&filters%5Bcontent_type%3A3d%5D=1&filters%5Bcontent_type%3Aaudio%5D=0&filters%5Binclude_stock_enterprise%5D=0&filters%5Bis_editorial%5D=0&filters%5Bfree_collection%5D=0&filters%5Bcontent_type%3Aimage%5D=1&k=basketball+hoop&order=relevance&safe_search=1&limit=100&search_page=21&search_type=pagination&get_facets=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "* must have roboflow dataset\n",
    "* must use train/valid/test split  <add in test split in case of other scenarios\n",
    "* must use coco_segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu118\n",
      "CUDA available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "#create going_modular repository\n",
    "create_directory(\"going_modular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required files from torchvision\n",
    "import requests\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "]\n",
    "download_files(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "#!pip install roboflow\n",
    "\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"ai-79z1a\").project(\"basketball_child\")\n",
    "#dataset = project.version(6).download(\"coco-segmentation\")\n",
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "import requests\n",
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "def download_videos_from_youtube(video_urls, output_path):\n",
    "    \"\"\"\n",
    "    Downloads videos from YouTube.\n",
    "\n",
    "    Args:\n",
    "    video_urls (list): List of YouTube video URLs.\n",
    "    output_path (str): Directory where videos will be saved.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of successful and failed downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'best',\n",
    "        'outtmpl': output_path + '/%(title)s.%(ext)s',\n",
    "        'quiet': True\n",
    "    }\n",
    "\n",
    "    failed_downloads = []\n",
    "    successful_downloads = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        for url in video_urls:\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "                print(f\"Successfully downloaded {url}\")\n",
    "                successful_downloads.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "                failed_downloads.append(url)\n",
    "\n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_project(api_key, workspace, project_name, version):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(\"coco-segmentation\")\n",
    "    return dataset\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def construct_dataset_paths(project_name, version):\n",
    "    base_path = f\"{project_name}-{version}\"\n",
    "    train_annotation_path = f\"{base_path}/train/_annotations.coco.json\"\n",
    "    valid_annotation_path = f\"{base_path}/valid/_annotations.coco.json\"\n",
    "    test_annotation_path = f\"{base_path}/test/_annotations.coco.json\"\n",
    "\n",
    "    train_root_dir = f\"{base_path}/train\"\n",
    "    valid_root_dir = f\"{base_path}/valid\"\n",
    "    test_root_dir = f\"{base_path}/test\"\n",
    "\n",
    "    return train_annotation_path, valid_annotation_path, test_annotation_path, train_root_dir, valid_root_dir, test_root_dir\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/coco_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/coco_dataset.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_path, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter out images without annotations\n",
    "        annotated_images = []\n",
    "        for img in self.annotations['images']:\n",
    "            image_id = img['id']\n",
    "            anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "            if len(anns) > 0:\n",
    "                annotated_images.append(img)\n",
    "\n",
    "        self.image_ids = [img['id'] for img in annotated_images]\n",
    "\n",
    "        # Update the self.annotations['images'] to include only annotated images\n",
    "        self.annotations['images'] = annotated_images\n",
    "        \n",
    "        #print(\"Number of images:\", len(self.annotations['images']))\n",
    "        #print(\"Sample image entry:\", self.annotations['images'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.annotations['images'][idx]\n",
    "        image_id = img_info['id']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "        #print(\"Image size (PIL):\", img.size)\n",
    "        #print(\"Image shape (tensor):\", img_tensor.shape)\n",
    "\n",
    "        anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "        #print(\"Number of annotations for this image:\", len(anns))\n",
    "\n",
    "        boxes = [ann['bbox'] for ann in anns]  # bbox format: [x_min, y_min, width, height]\n",
    "        # Convert from XYWH to XYXY format\n",
    "        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = [ann['category_id'] for ann in anns]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        #print(\"Boxes shape:\", boxes.shape)\n",
    "        #print(\"Labels:\", labels)\n",
    "        # Debug print\n",
    "        #print(f\"Boxes shape for image {idx}: {boxes.shape}\")\n",
    "\n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            if 'segmentation' in ann and isinstance(ann['segmentation'], list):\n",
    "                for seg in ann['segmentation']:\n",
    "                    mask_img = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "                    ImageDraw.Draw(mask_img).polygon(seg, outline=1, fill=1)\n",
    "                    mask = np.array(mask_img)\n",
    "                    masks.append(mask)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "        #print(\"Masks shape:\", masks.shape)\n",
    "\n",
    "        areas = [ann['area'] for ann in anns]\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = [ann['iscrowd'] for ann in anns]\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Convert masks to Mask format\n",
    "        masks = tv_tensors.Mask(masks)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id  # Changed to integer\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        #print(\"Target:\", target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/visualization_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/visualization_utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F  # Add this import\n",
    "\n",
    "# New function to visualize transformations\n",
    "def visualize_transformation(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    transformed_img, transformed_target = dataset.transforms(img, target)\n",
    "    original_img = F.to_pil_image(img)\n",
    "    transformed_img = F.to_pil_image(transformed_img)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    for box in target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Original Image - ID: {idx}\")\n",
    "\n",
    "    # Transformed Image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(transformed_img)\n",
    "    for box in transformed_target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='b', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Transformed Image - ID: {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bbox(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    original_img = F.to_pil_image(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(original_img)\n",
    "\n",
    "    for box in target[\"boxes\"]:  # Access the boxes directly\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        # Debug print\n",
    "        print(f\"Visualizing BBox - xmin: {x_min}, ymin: {y_min}, xmax: {x_max}, ymax: {y_max}\")\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.title(f\"Image with Bounding Boxes - ID: {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_utils.py\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extracting classes and their IDs\n",
    "    classes = {category['id']: category['name'] for category in data['categories']}\n",
    "    return classes\n",
    "\n",
    "# Usage example:\n",
    "#classes = load_classes_from_json('basketball_child-6/test/_annotations.coco.json')\n",
    "#print(classes)\n",
    "\n",
    "# model_utils.py\n",
    "def get_model_instance_segmentation(num_classes, hidden_layer=256):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/transforms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/transforms.py\n",
    "import torch  # Add this import statement\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, ConvertImageDtype\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    #if train:\n",
    "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def transform_image(image_bytes):\n",
    "    \"\"\"\n",
    "    Transforms image bytes into a tensor with the correct format for the model.\n",
    "    \n",
    "    Args:\n",
    "    image_bytes (bytes): The image in bytes format, as uploaded by the user.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The transformed image as a tensor.\n",
    "    \"\"\"\n",
    "    # Define the transformations\n",
    "    my_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to the size required by your model\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard normalization for pre-trained models\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load the image from bytes and apply transformations\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    return my_transforms(image).unsqueeze(0)  # Add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "# train.py\n",
    "import torch\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "\n",
    "def train_model(model, data_loader, data_loader_valid, device, num_epochs,\n",
    "                lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_valid, device=device)\n",
    "\n",
    "    #torch.save(model.state_dict(), 'results/models/model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/process_video_check.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/process_video_check.py\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "def intersects(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
    "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
    "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
    "\n",
    "def process_video_check(video_path, model, device, classes, classes_to_track, threshold=0.5):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    score = 0  # Initialize the score counter\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_tensor = T.ToTensor()(frame).unsqueeze_(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)[0]\n",
    "\n",
    "        pred_scores = prediction['scores']\n",
    "        pred_boxes = prediction['boxes']\n",
    "        pred_labels = prediction['labels']\n",
    "        pred_masks = prediction['masks']\n",
    "\n",
    "        keep = pred_scores > threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        pred_masks = pred_masks[keep]\n",
    "\n",
    "        print(f\"Original Frame Size: {frame.shape}\")\n",
    "        print(f\"Pred Boxes before drawing: {pred_boxes}\")\n",
    "\n",
    "        if not keep.any():\n",
    "            continue  # Skip this frame if no detections are kept\n",
    "\n",
    "        # Convert numeric labels to class names\n",
    "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
    "\n",
    "        # Iterate through each pair of classes to track\n",
    "        for class_pair in classes_to_track:\n",
    "            class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
    "            class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
    "\n",
    "            # Check for intersections and update score\n",
    "            for box1 in class1_boxes:\n",
    "                for box2 in class2_boxes:\n",
    "                    if intersects(box1, box2):\n",
    "                        score += 1\n",
    "                        print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score)\n",
    "\n",
    "        # Frame Tensor Conversion for Drawing\n",
    "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
    "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
    "\n",
    "        # Draw bounding boxes and segmentation masks\n",
    "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
    "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
    "\n",
    "        # Convert output image for displaying\n",
    "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        output_image = np.clip(output_image, 0, 255)  # Ensure values are within 0-255\n",
    "\n",
    "        # Draw score text\n",
    "        cv2.putText(output_image, f'Score: {score}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Frame', output_image)\n",
    "        \n",
    "        # Just before cv2.imshow\n",
    "        print(f\"Output Image Size: {output_image.shape}\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# process_video_check(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last todos:\n",
    "* add in more descriptive args like roboflow_api and youtube_video_url\n",
    "* add in args.parse for the classes to check for intersection\n",
    "* add in huggingface save and reload option so we don't save the model here\n",
    "* add streamlit for this so it can be visualized by anyone\n",
    "* update google collab version the same^ but also so that it can work as they walk into it, so just the clones and then train.py\n",
    "*move to different repo and name instanceseg_track_roboflow or something more rounded than bball_instanceseg (so people don't see your api in previous versions)\n",
    "*^clean up to only the bare essentials\n",
    "*add readme and docstrings to all code to make easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from going_modular.utils import get_device, create_directory, get_project, download_videos_from_youtube\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import get_model_instance_segmentation\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.transforms import get_transform\n",
    "from going_modular.process_video_check import process_video_check\n",
    "import utils\n",
    "\n",
    "def load_classes_from_json(annotation_path):\n",
    "    with open(annotation_path) as f:\n",
    "        data = json.load(f)\n",
    "    categories = data['categories']\n",
    "    classes = {category['id']: category['name'] for category in categories}\n",
    "    return classes\n",
    "\n",
    "def split_dataset(dataset, split_ratio=0.8):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * split_ratio)\n",
    "    valid_size = total_size - train_size\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(42))\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def upload_to_huggingface(model_directory, model_id):\n",
    "    \"\"\"Upload the model to Hugging Face Hub.\"\"\"\n",
    "    hf_api = HfApi()\n",
    "    username = hf_api.whoami()['name']\n",
    "    repo_name = f\"{username}/{model_id}\"\n",
    "    repo_url = hf_api.create_repo(repo_name, exist_ok=True, private=False)\n",
    "\n",
    "    repo = Repository(local_dir=model_directory, clone_from=repo_url, use_auth_token=True)\n",
    "    repo.lfs_track([\"*.bin\", \"*.pth\", \"*.ckpt\"])  # Track large model files with Git LFS\n",
    "    repo.git_add()\n",
    "    repo.git_commit(\"Initial commit of the model\")\n",
    "    try:\n",
    "        repo.git_push()\n",
    "        print(f\"Model successfully uploaded to: {repo_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload model to Hugging Face: {e}\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Create model path directory\n",
    "    model_path = Path(args.model_path)\n",
    "    create_directory(model_path)\n",
    "\n",
    "    # Check if dataset is already downloaded\n",
    "    project_folder = Path.cwd() / f'{args.project_folder_name}-{args.version}'\n",
    "    print(f\"Checking dataset at: {project_folder.absolute()}\")\n",
    "    \n",
    "    if args.mode == 'train':\n",
    "        if not project_folder.exists():\n",
    "            print(\"Downloading dataset...\")\n",
    "            dataset = get_project(args.api_key, args.workspace, args.project_name, args.version)\n",
    "        else:\n",
    "            print(\"Project data already downloaded. Skipping download...\")\n",
    "\n",
    "        # Load classes from annotation file\n",
    "        classes_path = project_folder / 'train' / '_annotations.coco.json'\n",
    "        if not classes_path.exists():\n",
    "            print(f\"Error: Annotation file for the train dataset not found at {classes_path.absolute()}.\")\n",
    "            return\n",
    "        classes = load_classes_from_json(classes_path)\n",
    "        print(\"Classes loaded:\", classes)\n",
    "\n",
    "        # Initialize model\n",
    "        num_classes = len(classes) + 1\n",
    "        device = get_device()\n",
    "        model = get_model_instance_segmentation(num_classes, hidden_layer=args.hidden_layer)\n",
    "        model.to(device)\n",
    "\n",
    "        # Load datasets\n",
    "        datasets = {}\n",
    "        data_loaders = {}\n",
    "        for dtype in ['train', 'valid', 'test']:\n",
    "            ann_path = project_folder / dtype / '_annotations.coco.json'\n",
    "            img_dir = project_folder / dtype\n",
    "            if ann_path.exists() and img_dir.exists():\n",
    "                datasets[dtype] = CustomCocoDataset(str(ann_path), str(img_dir), transforms=get_transform(train=dtype=='train'))\n",
    "                batch_size = 2 if dtype == 'train' else 1\n",
    "                data_loaders[dtype] = DataLoader(datasets[dtype], batch_size=batch_size, shuffle=dtype=='train', num_workers=0, collate_fn=utils.collate_fn)\n",
    "                print(f\"{dtype.capitalize()} dataset loaded.\")\n",
    "            else:\n",
    "                print(f\"{dtype.capitalize()} dataset not found or incomplete. Skipping.\")\n",
    "\n",
    "        if 'train' in datasets and 'valid' not in datasets:\n",
    "            print(\"Splitting dataset into train and valid...\")\n",
    "            train_dataset, valid_dataset = split_dataset(datasets['train'])\n",
    "            data_loaders['train'] = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "            data_loaders['valid'] = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "            \n",
    "        if 'train' in data_loaders and 'valid' in data_loaders:\n",
    "            print(\"Starting training process...\")\n",
    "            train_model(model, data_loaders['train'], data_loaders.get('valid'), device, args.num_epochs, lr=args.lr)\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(f\"Model saved at: {model_file_path}\")\n",
    "        else:\n",
    "            print(\"Training failed. No valid data loaders available.\")\n",
    "\n",
    "    # Process video if mode is 'process_video'\n",
    "    if args.mode == 'process_video':\n",
    "        video_filename = args.video_name if args.video_name.endswith('.mp4') else f\"{args.video_name}.mp4\"\n",
    "        video_path = Path(args.data_path) / video_filename\n",
    "        if not video_path.exists():\n",
    "            print(\"Downloading video...\")\n",
    "            download_videos_from_youtube([args.video_url], str(Path(args.data_path)))\n",
    "        \n",
    "        if video_path.exists():\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            if model_file_path.exists():\n",
    "                model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
    "                process_video_check(video_path, model, device, classes, [('Basketball', 'Hoop')], threshold=args.threshold)\n",
    "            else:\n",
    "                print(\"Model weights file not found. Please train the model first.\")\n",
    "\n",
    "    if args.upload_to_hf:\n",
    "        hf_login()  # Ensure user is logged in\n",
    "\n",
    "        # Automatically determine model directory (or you can still ask the user)\n",
    "        model_directory = args.model_path  # Assuming this is where your model is saved\n",
    "        model_id = input(\"Enter a name for your model on Hugging Face (e.g., my-cool-model): \")\n",
    "\n",
    "        try:\n",
    "            upload_to_huggingface(model_directory, model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during model upload: {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a model for object detection or process video\")\n",
    "    parser.add_argument('--api_key', type=str, default=\"your_roboflow_api_key\", help='API key for Roboflow')\n",
    "    parser.add_argument('--workspace', type=str, default=\"your_roboflow_workspace\", help='Workspace name in Roboflow')\n",
    "    parser.add_argument('--project_name', type=str, default=\"your_roboflow_project\", help='Project name in Roboflow')\n",
    "    parser.add_argument('--project_folder_name', type=str, default=\"your_roboflow_project_folder_name\", help='Project folder name in Roboflow')\n",
    "    parser.add_argument('--version', type=int, default=1, help='Version of the dataset in Roboflow')\n",
    "    parser.add_argument('--hidden_layer', type=int, default=256, help='Hidden layer size for the MaskRCNN predictor')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs to train the model')\n",
    "    parser.add_argument('--video_url', type=str, default=\"https://www.youtube.com/watch?v=example_video_id\", help='URL of the video to process')\n",
    "    parser.add_argument('--video_name', type=str, default=\"your_youtube_video_name\", help='Name of the video file (with extension) to process')\n",
    "    parser.add_argument('--threshold', type=float, default=0.6, help='Detection threshold for process_video')\n",
    "    parser.add_argument('--display_video', type=bool, default=True, help='Whether to display the video during processing')\n",
    "    parser.add_argument('--data_path', type=str, default='results/data', help='Path to save downloaded data')\n",
    "    parser.add_argument('--model_path', type=str, default='results/models', help='Path to save model weights')\n",
    "    parser.add_argument('--mode', type=str, choices=['train', 'process_video'], default='train', help='Mode of operation: train or process_video')\n",
    "    parser.add_argument('--upload_to_hf', action='store_true', help='Whether to upload the model to Hugging Face')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add in a delete for the existing image dataset and youtube video's\n",
    "add in hugging face upload if they like the model and name of model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset at: c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\basketball-and-hoop-11\n",
      "Downloading video...\n",
      "\n",
      "[download]   0.0% of    9.01MiB at  Unknown B/s ETA Unknown\n",
      "[download]   0.0% of    9.01MiB at    1.96MiB/s ETA 00:04  \n",
      "[download]   0.1% of    9.01MiB at    3.42MiB/s ETA 00:02\n",
      "[download]   0.2% of    9.01MiB at    4.88MiB/s ETA 00:01\n",
      "[download]   0.3% of    9.01MiB at    6.06MiB/s ETA 00:01\n",
      "[download]   0.7% of    9.01MiB at    2.92MiB/s ETA 00:03\n",
      "[download]   1.4% of    9.01MiB at    1.15MiB/s ETA 00:07\n",
      "[download]   2.8% of    9.01MiB at    1.86MiB/s ETA 00:04\n",
      "[download]   5.5% of    9.01MiB at    2.92MiB/s ETA 00:02\n",
      "[download]  11.1% of    9.01MiB at    4.73MiB/s ETA 00:01\n",
      "[download]  22.2% of    9.01MiB at    8.08MiB/s ETA 00:00\n",
      "[download]  44.4% of    9.01MiB at   12.92MiB/s ETA 00:00\n",
      "[download]  88.8% of    9.01MiB at   17.87MiB/s ETA 00:00\n",
      "[download] 100.0% of    9.01MiB at   19.21MiB/s ETA 00:00\n",
      "[download] 100% of    9.01MiB in 00:00:00 at 10.72MiB/s  \n",
      "                                                       \n",
      "Successfully downloaded https://www.youtube.com/watch?v=kh7s2tGvswc&t=1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\train.py\", line 122, in <module>\n",
      "    main(args)\n",
      "  File \"c:\\Users\\ghadf\\vscode_projects\\venv_projects\\Pytorch\\bball_instanceseg\\train.py\", line 98, in main\n",
      "    model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
      "    ^^^^^\n",
      "UnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
    "                --workspace basketball-formations \\\n",
    "                --project_name basketball-and-hoop-7xk0h \\\n",
    "                --project_folder_name basketball-and-hoop \\\n",
    "                --version 11 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 1 \\\n",
    "                --threshold 0.6 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=kh7s2tGvswc&t=1s\" \\\n",
    "                --video_name \"Devin Booker Sets Record, Wins Three-Point Contest\" \\\n",
    "                --mode process_video\n",
    "\n",
    "#ex command line usage\n",
    "#python train.py --api_key htpcxp3XQh7SsgMfjJns --workspace basketball-formations --project_name basketball-and-hoop-7xk0h --project_folder_name basketball-and-hoop --version 10 --hidden_layer 256 --lr 0.005 --num_epochs 1 --threshold 0.6 --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" --video_name \"The Best NBA 3 Point Contest Performances\" --mode process_video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"netventure\").project(\"car_segment-yarm8\")\n",
    "#dataset = project.version(1).download(\"coco-segmentation\")\n",
    "\n",
    "!python train.py --api_key htpcxp3XQh7SsgMfjJns \\\n",
    "                --workspace netventure \\\n",
    "                --project_name car_segment-yarm8 \\\n",
    "                --project_folder_name car_segment \\\n",
    "                --version 1 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 10 \\\n",
    "                --threshold 0.8 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=D5rD5Z6uCRI\" \\\n",
    "                --video_name \"Driving for 1 minute\" \\\n",
    "                --mode train\n",
    "\n",
    "#ex command line usage\n",
    "#python train.py --api_key htpcxp3XQh7SsgMfjJns --workspace basketball-formations --project_name basketball-and-hoop-7xk0h --project_folder_name basketball-and-hoop --version 10 --hidden_layer 256 --lr 0.005 --num_epochs 1 --threshold 0.6 --video_url \"https://www.youtube.com/watch?v=y8i6fsAXDZE\" --video_name \"The Best NBA 3 Point Contest Performances\" --mode process_video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "import torch\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import modular functions (make sure all functions are correctly imported)\n",
    "from going_modular.utils import (get_device, create_directory, get_project,\n",
    "                                 download_files, construct_dataset_paths,\n",
    "                                 download_videos_from_youtube)\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import get_model_instance_segmentation, load_classes_from_json\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.process_video_check import process_video_check\n",
    "from going_modular.transforms import get_transform, transform_image\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Global variables (Consider storing and accessing these more securely and flexibly)\n",
    "MODEL_PATH = Path('results/models/model_weights.pth')\n",
    " \n",
    "\n",
    "# Ensure MODEL_PATH directory exists\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_model(num_classes: int, model_path: Path = MODEL_PATH):\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=256)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the API!\"}\n",
    "\n",
    "@app.post(\"/train\")\n",
    "async def train(api_key: str, workspace: str, project_name: str, project_folder_name: str, version: int, num_epochs: int = 10):\n",
    "    # Set up device\n",
    "    device = get_device()\n",
    "    \n",
    "    # Ensure data and model directories exist\n",
    "    data_path = Path('results/data')\n",
    "    model_path = Path('results/models')\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download and prepare the dataset\n",
    "    dataset = get_project(api_key, workspace, project_name, version)\n",
    "    train_annotation_path, valid_annotation_path, test_annotation_path, train_image_dir, valid_image_dir, test_image_dir = construct_dataset_paths(project_folder_name, version)\n",
    "    \n",
    "    # Load class names and set up the model\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    \n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=256)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = CustomCocoDataset(train_annotation_path, train_image_dir, transforms=get_transform(train=True))\n",
    "    valid_dataset = CustomCocoDataset(valid_annotation_path, valid_image_dir, transforms=get_transform(train=False))\n",
    "    \n",
    "    # Set up data loaders\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_data_loader, valid_data_loader, device, num_epochs)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    return {\"message\": \"Model trained and saved successfully\"}\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(project_folder_name: str, version: int, file: UploadFile = File(...)):\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    model = load_model(num_classes)\n",
    "    \n",
    "    image_bytes = await file.read()\n",
    "    tensor = transform_image(image_bytes)\n",
    "    \n",
    "    # Prediction logic\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        top_prob, top_catid = probabilities.topk(1, dim=1)\n",
    "    \n",
    "    predicted_class = classes[top_catid.item()]\n",
    "    confidence = top_prob.item()\n",
    "    \n",
    "    return JSONResponse(content={\"class\": predicted_class, \"confidence\": confidence})\n",
    "\n",
    "from fastapi import UploadFile, File\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "@app.post(\"/process_video\")\n",
    "async def process_video(project_folder_name: str, version: int, video_file: UploadFile = File(...)):\n",
    "    # Define where to save the video temporarily\n",
    "    temp_video_path = Path(\"temp_videos\") / video_file.filename\n",
    "    temp_video_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save the uploaded video to the temporary path\n",
    "    with temp_video_path.open(\"wb\") as buffer:\n",
    "        shutil.copyfileobj(video_file.file, buffer)\n",
    "    \n",
    "    # Load model and classes for processing\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    model = load_model(num_classes)\n",
    "    \n",
    "    # Assuming process_video_check is adapted to return a meaningful result\n",
    "    # For example, modifying process_video_check to accept a video path and return a dictionary of results\n",
    "    results = process_video_check(str(temp_video_path), model, get_device(), classes, [('Basketball', 'Hoop')], threshold=0.6)\n",
    "\n",
    "    # Optionally, delete the temporary video file after processing\n",
    "    temp_video_path.unlink(missing_ok=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "\n",
    "#!uivcorn app:app --reload\n",
    "\n",
    "#For looking at all the options**\n",
    "#http://127.0.0.1:8000/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
